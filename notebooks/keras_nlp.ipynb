{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_nlp.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/assaflehr/language-style-transfer/blob/master/notebooks/keras_nlp.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "vR_wGTR6szDb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Keras accuracy/performace limitations\n",
        "\n",
        "## Pretraining autoencoder or LM will surely help.\n",
        "\n",
        "see late review here: https://thegradient.pub/author/sebastian\n",
        "and autoencoder (which is less recommended than LM here: \"Semi-supervised Sequence Learning\" 2015. They use one RNN for both encoder and decoder)\n",
        "\n",
        "\n",
        "### CuDNNLSTM vs LSTM \n",
        "The former trains very fast on GPU, but does not support the attributes: dropout,recurrent_dropout,which are the STOA regulaizers. This is cuda problem, and even native TF does not support it\n",
        "\n",
        "### Tip to self:\n",
        "* manually check loss value on one sample (predict vs gt). From doing this, I saw <s> was not given one-hot-value"
      ]
    },
    {
      "metadata": {
        "id": "OrKzdh71sBKm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "UbJwZjyhsDvP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# adaptation of: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
        "# first Dataset class to load bible-data\n",
        "# then copy of the model, but working with words instead of chars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wwc6_EHFD_Ir",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ]
    },
    {
      "metadata": {
        "id": "PcPSCDUrx3B8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "outputId": "87a16e76-f336-4d56-9a53-f6d1bb0709d7"
      },
      "cell_type": "code",
      "source": [
        "## NLP preprocessing for text\n",
        "# has few parts:\n",
        "# 1. load zip files and then use glob to filter part of them (data/*/*.txt)\n",
        "# 2. parse each row into (x,y) by passing a parser method. it can be simple as lambda line:line:x, or if you use tab delimited lambda line: line.split(',')[4]\n",
        "# 3. tokenize - split by spaces, but also by ., and be smart about it.  ('...' should be one token , \"ai'nt\" one token. then; should be two 'token' and ';')\n",
        "#    you should also build vocabulary, keep X words and throw away rare ones, they will be replaced by <oov> flag.\n",
        "# 4. transform text to sequences for the result. for words there are usually two different types: ['s>','hello', 'world'] -> [0,5,6] but there is also \n",
        "#     a one-hot-econding version where 5 is actaully a vector of size voc-length full of zeros, with 5th index==1.\n",
        "#    The one-hot ecoding is used as output for text-generation and has a HUGE MEMORY requirement.  100K sentences of size 20 words need 2M floats = 8MB\n",
        "#    But for the one-hot-encoding multiply this by vocab-size. for char-encoding it's ~30 , for good vocab of 10K words, we need 80GB(!)\n",
        "#    The simple, and only , way to solve this , is to never keep one-hot-encoding in memory, just use a generator to make it one-hot in runtime\n",
        "\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import csv, json\n",
        "from collections import namedtuple\n",
        "from zipfile import ZipFile\n",
        "from os.path import expanduser, exists\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "TVT = namedtuple('TVT',['train','val','test'])\n",
        "\n",
        "class Dataset:\n",
        "  #Dataset for COPY encoder-decoder\n",
        "  # to David: generator x,y where x is batch,max-words of type integer. (            <s>   hello word)\n",
        "  #                               y is batch,max-words,one-hot-encoding (offset one: hello world <end>)\n",
        "  # tokenizer is currently very bad. replace it\n",
        "  # vocabulary (training-only) , don't use 666 , use large number (10K?)\n",
        "  \n",
        "  # Dataset2 for style, x is sentence + STYLE(one-hot-encoding of integer) , y is one hot encoding \n",
        "  \n",
        "  # Dataset3 for classifier: x is sentence + STYLE(one-hot-encoding of integer),  output: STYLE(one-hot-encoding of integer)\n",
        "  \n",
        "  \n",
        "  \n",
        "  def __init__(self,unique_name,url,extract,cache_dir,pattern,skip_first,row_parser,validation_pattern=0.1,test_pattern=0.1):\n",
        "    '''\n",
        "    unique_name will be used for the dataset source(or zip) file. \n",
        "    pattern need to include path inside zip (including zip root)\n",
        "    extract - is it zipped/tarred or not\n",
        "    cache_dir - under which the files be downloaded <cache_dir>/datasets/<unique_name>\n",
        "    pattern - glob will be done to choose only those files ,for example data*.txt. This should incldude both train and test\n",
        "    validation - subset glob pattern to use. If it's a float like 0.1, use it as split of one file\n",
        "    test - see above\n",
        "    '''\n",
        "    if not extract and pattern:\n",
        "      raise ValueError('pattern must be empty if extract=False chooses a subset of the files (data/*.txt). but you downloaded only one file')\n",
        "\n",
        "    if not os.path.exists(cache_dir):\n",
        "       os.makedirs(cache_dir)\n",
        "\n",
        "    fpath=get_file(unique_name, url,extract=True, cache_dir=cache_dir)\n",
        "    print ('fpath',fpath)\n",
        "    files = [fpath] if not pattern else glob.glob(f'{cache_dir}/datasets/{pattern}')\n",
        "\n",
        "    train,val,test=[],[],[]\n",
        "    for f in files:\n",
        "      lines = [row_parser(f,line.rstrip()) for line in open(f,encoding=\"latin-1\").readlines()] \n",
        "      lines = lines[1 if skip_first else 0:][:10000]\n",
        "      print ('HARDCODED MAX LINES = 10000')\n",
        "      \n",
        "      print (files,'#lines',len(lines),'first 3 lines')\n",
        "      print (lines[0],'\\n',lines[1],'\\n',lines[2])\n",
        "      \n",
        "      if isinstance(validation_pattern,float) and isinstance(test_pattern,float):\n",
        "        test_count = int(len(lines)*(1-validation_pattern))\n",
        "        val_count =  int(len(lines)*(1-test_pattern-validation_pattern))\n",
        "        print ('(val_count,test_count)',val_count,test_count)\n",
        "        train+= lines[:val_count]\n",
        "        val+=   lines[val_count:test_count]\n",
        "        test+=  lines[test_count:]\n",
        "        \n",
        "    self.tvt_lines = TVT(train, val, test)\n",
        "    print ('train:',len(self.tvt_lines.train),'val',len(self.tvt_lines.val),'test',len(self.tvt_lines.test))\n",
        "    self.parsed= self.tvt_lines\n",
        "  \n",
        "  \n",
        "  def fit(self):\n",
        "    \"\"\" the current implementation is quite bad, hello world! will be 2 tokens world! is the second. \n",
        "    \"\"\"\n",
        "    print ('limiting num_words in Tokenizer due to MEMORY BOUNDS')  #num_words =100*1000\n",
        "\n",
        "    \n",
        "    # I use here tokenizer only to count freq. of words, then manually choose most freq. and manually split\n",
        "    # this is bad. There are better ways to do it (probably library/code which do it in one line)\n",
        "    print ('\\nREPLACE ME . BAD TOKENIZER!!!')\n",
        "    self.tokenizer = Tokenizer(num_words=100000, filters='', lower=False, split=' ', char_level=False, oov_token='<oov>')\n",
        "    \n",
        "    # self.parsed.train is a list , each value is tuple text_string,file_name\n",
        "    self.tokenizer.fit_on_texts([x for x,style in self.parsed.train])\n",
        "    self.styles = set([style for x,style in self.parsed.train])\n",
        "    print ('styles',self.styles)\n",
        "    self.style2index = {style:i for i,style in enumerate(self.styles)}\n",
        "    self.index2style = {index:style for style,index in self.style2index.items() }\n",
        "    print (self.style2index,self.index2style)\n",
        "    \n",
        "    print ('\\n word_index',len(self.tokenizer.word_index),'<oov>',self.tokenizer.word_index['<oov>'])\n",
        "    print ('common',list(self.tokenizer.word_index.items())[:15])\n",
        "    print ('uncommon',list(self.tokenizer.word_index.items())[-15:])\n",
        "  \n",
        "    num_words= 2000\n",
        "    print ('keeping only ',num_words,'of',len(self.tokenizer.word_index))\n",
        "           \n",
        "           \n",
        "    word2index = dict(list(self.tokenizer.word_index.items())[:num_words-len(self.styles)-2])\n",
        "    word2index['<s>']=0  #keras tokenizer keeps 0 unused\n",
        "    for i,style in enumerate(self.styles):\n",
        "      word2index[style]=num_words-1-len(self.styles)+i  #if num_words=100 . [96,97,98] \n",
        "    word2index['<oov>']=num_words-1                   #<oov> is [99]\n",
        "    print ('word2index',len(word2index))\n",
        "    \n",
        "    #FOR NOW the start and end are both ZERO. maybe not good???\n",
        "    \n",
        "    num_encoder_tokens = num_decoder_tokens= num_words # len(self.tokenizer.word_index)\n",
        "    self.word2index = word2index\n",
        "    self.index2word = {index:word for (word,index) in self.word2index.items()}\n",
        "    self.MAX_SEQUENCE_LENGTH=10\n",
        "    \n",
        "    verbose=5\n",
        "    result = []\n",
        "    for rows in self.parsed:\n",
        "\n",
        "      encoder_input_data  = np.zeros( (len(rows), self.MAX_SEQUENCE_LENGTH),    dtype='float32')\n",
        "      decoder_input_data  = np.zeros( (len(rows), self.MAX_SEQUENCE_LENGTH),    dtype='float32') # shifted by 1\n",
        "      decoder_target_data = np.zeros((len(rows),  self.MAX_SEQUENCE_LENGTH, num_decoder_tokens),    dtype='float32')\n",
        "      style_data          = np.zeros((len(rows),  len(self.styles)),    dtype='float32') #one-hot\n",
        "      \n",
        "      #input to decoder   <s> hello world\n",
        "      #target of decoder: hello world <s>\n",
        "      \n",
        "      for i, (input_text,style) in enumerate(rows):\n",
        "        input_text = input_text.split(' ') #BUG: we need to use tokenizer here!!!!\n",
        "        #pad with end token  hello world <end> <end> <end>\n",
        "        end_token='<s>'\n",
        "        input_text += [end_token for _ in range(self.MAX_SEQUENCE_LENGTH - len(input_text)+1)]\n",
        "        if verbose:\n",
        "          print ('input_text',input_text)\n",
        "          verbose-=1\n",
        "        # out : hello  world  <end>  (MAX_SEQUENCE_LENGTH=2)  <-encoder_input+ decoder_output(but one-hot)\n",
        "        #\n",
        "        # in: : <s>   hello   world  <- decoder-input\n",
        "         \n",
        "        for t, word in enumerate(([style]+input_text)[:self.MAX_SEQUENCE_LENGTH]):\n",
        "            one_hot = word2index['<oov>'] if word not in word2index else word2index[word]\n",
        "            decoder_input_data[i, t ] = one_hot\n",
        "            \n",
        "        for t,word in enumerate(input_text[:self.MAX_SEQUENCE_LENGTH]):  #last must be <end>=<s> token\n",
        "            one_hot = word2index['<oov>'] if word not in word2index else word2index[word]\n",
        "            encoder_input_data[i,t]=one_hot\n",
        "            decoder_target_data[i, t, one_hot] = 1. \n",
        "            \n",
        "        style_data[i,self.style2index[style]]= 1\n",
        "        \n",
        "      print (decoder_target_data.sum(),len(rows)*self.MAX_SEQUENCE_LENGTH)\n",
        "      assert int(decoder_target_data.sum())==len(rows)*self.MAX_SEQUENCE_LENGTH #one-hot-encoding must always include one\n",
        "      \n",
        "      result.append( (encoder_input_data,decoder_input_data,decoder_target_data,style_data))\n",
        "\n",
        "    self.result= TVT(*result)\n",
        "\n",
        "  def one_x_as_text(self,x):\n",
        "    \"\"\" 1x20 or 20 input\"\"\"\n",
        "    if len(x.shape)==2: \n",
        "      assert x.shape[0] ==1  #can only work on batch of 1\n",
        "      x= x[0]\n",
        "    return ' '.join([self.index2word[index] for index in x])\n",
        "\n",
        "  def one_y_as_text(self,y):\n",
        "    \"\"\" 1x20x2000 or 20x2000 input, in case of first will work on y[0]\"\"\"\n",
        "    if len(y.shape)==3: \n",
        "      assert y.shape[0] ==1  #can only work on batch of 1\n",
        "      y=y[0]\n",
        "      \n",
        "    best_token = np.argmax(y,1)\n",
        "    return ' '.join([self.index2word[index] for index in best_token])\n",
        "\n",
        "  \n",
        "cache_dir='cache' \n",
        "#dataset('quora_dups','http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv',False,cache_dir) \n",
        "#dataset('bible4','https://codeload.github.com/keithecarlson/Zero-Shot-Style-Transfer/zip/master',extract=True,cache_dir=cache_dir\n",
        "#       pattern=('Zero-Shot-Style-Transfer-master/Data/Bibles/ASV/*/*.txt','Zero-Shot-Style-Transfer-master/Data/Bibles/BBE/*/*.txt')\n",
        "\n",
        "row_parser= lambda file_name,line: (line.split(',')[4],file_name.split('/')[-1]) #map x to x,style_file\n",
        "dataset = Dataset('bible_csv','https://codeload.github.com/ashual/style-transfer/zip/master',extract=True,cache_dir=cache_dir,\n",
        "                  #,t_bbe,BBE,english,Bible in Basic English,,http://en.wikipedia.org/wiki/Bible_in_Basic_English,,Public Domain,\n",
        "                  #,t_dby,DARBY,english,Darby English Bible,,http://en.wikipedia.org/wiki/Darby_Bible,,Public Domain,\n",
        "                  #,t_kjv,KJV,english,King James Version,,http://en.wikipedia.org/wiki/King_James_Version,,Public Domain,\n",
        "                  pattern='style-transfer-master/datasets/bible-corpus/t_[kb]*.csv',skip_first=True,row_parser=row_parser)    #kbd\n",
        "dataset.fit()        \n",
        "x_train, x_train_d, y_train,style_train = dataset.result.train\n",
        "x_val, x_val_d,y_val ,style_val= dataset.result.val\n",
        "x_test,x_test_d,y_test ,style_test= dataset.result.test\n",
        "\n",
        "print ('train',x_train.shape,x_train_d.shape,y_train.shape,style_train.shape)\n",
        "print('val',x_val.shape,y_val.shape)\n",
        "print ('train in MB x,y',x_train.nbytes/1e6,y_train.nbytes/1e6)\n"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fpath cache/datasets/bible_csv\n",
            "HARDCODED MAX LINES = 10000\n",
            "['cache/datasets/style-transfer-master/datasets/bible-corpus/t_bbe.csv', 'cache/datasets/style-transfer-master/datasets/bible-corpus/t_kjv.csv'] #lines 10000 first 3 lines\n",
            "('At the first God made the heaven and the earth.', 't_bbe.csv') \n",
            " ('And the earth was waste and without form; and it was dark on the face of the deep: and the Spirit of God was moving on the face of the waters.', 't_bbe.csv') \n",
            " ('\"And God said', 't_bbe.csv')\n",
            "(val_count,test_count) 8000 9000\n",
            "HARDCODED MAX LINES = 10000\n",
            "['cache/datasets/style-transfer-master/datasets/bible-corpus/t_bbe.csv', 'cache/datasets/style-transfer-master/datasets/bible-corpus/t_kjv.csv'] #lines 10000 first 3 lines\n",
            "('In the beginning God created the heaven and the earth.', 't_kjv.csv') \n",
            " ('\"And the earth was without form', 't_kjv.csv') \n",
            " ('\"And God said', 't_kjv.csv')\n",
            "(val_count,test_count) 8000 9000\n",
            "train: 16000 val 2000 test 2000\n",
            "limiting num_words in Tokenizer due to MEMORY BOUNDS\n",
            "\n",
            "REPLACE ME . BAD TOKENIZER!!!\n",
            "styles {'t_bbe.csv', 't_kjv.csv'}\n",
            "{'t_bbe.csv': 0, 't_kjv.csv': 1} {0: 't_bbe.csv', 1: 't_kjv.csv'}\n",
            "\n",
            " word_index 7328 <oov> 7328\n",
            "common [('the', 1), ('of', 2), ('\"And', 3), ('to', 4), ('and', 5), ('in', 6), ('said', 7), ('he', 8), ('his', 9), ('a', 10), ('for', 11), ('unto', 12), ('be', 13), ('shall', 14), ('that', 15)]\n",
            "uncommon [('Jezreel;', 7314), ('anointed:', 7315), ('cruse', 7316), ('bolster;', 7317), ('faithfulness;', 7318), ('Gath:', 7319), ('disguised', 7320), ('obeyedst', 7321), ('straightway', 7322), ('distressed;', 7323), ('invasion', 7324), ('twilight', 7325), ('recovered', 7326), ('rescued', 7327), ('<oov>', 7328)]\n",
            "keeping only  2000 of 7328\n",
            "word2index 2000\n",
            "input_text ['At', 'the', 'first', 'God', 'made', 'the', 'heaven', 'and', 'the', 'earth.', '<s>']\n",
            "input_text ['And', 'the', 'earth', 'was', 'waste', 'and', 'without', 'form;', 'and', 'it', 'was', 'dark', 'on', 'the', 'face', 'of', 'the', 'deep:', 'and', 'the', 'Spirit', 'of', 'God', 'was', 'moving', 'on', 'the', 'face', 'of', 'the', 'waters.']\n",
            "input_text ['\"And', 'God', 'said', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
            "input_text ['\"And', 'God', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
            "input_text ['\"Naming', 'the', 'light', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
            "160000.0 160000\n",
            "20000.0 20000\n",
            "20000.0 20000\n",
            "train (16000, 10) (16000, 10) (16000, 10, 2000) (16000, 2)\n",
            "val (2000, 10) (2000, 10, 2000)\n",
            "train in MB x,y 0.64 1280.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9GT06VNk7vSN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!head -50 cache/datasets/style-transfer-master/datasets/bible-corpus/bible_version_key.csv\n",
        "#!ls -lh cache/datasets/style-transfer-master/datasets/bible-corpus\n",
        "#!head -5 cache/datasets/style-transfer-master/datasets/bible-corpus/t_kjv.csv\n",
        "#!head -5 cache/datasets/style-transfer-master/datasets/bible-corpus/t_bbe.csv\n",
        "#!head -5 cache/datasets/style-transfer-master/datasets/bible-corpus/t_dby.csv\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S_v9vKqIcTn6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "c31f9d23-3859-46b8-cf62-71fcbe0dbd1e"
      },
      "cell_type": "code",
      "source": [
        "#show a sample of x_train\n",
        "for i in range(1150,1152):\n",
        "  print ('\\ntokens  :' , x_train_d[i])\n",
        "  print ('as words:',[dataset.index2word[index] for index in x_train_d[i] ])\n",
        "  print ('original:',dataset.parsed.train[i][0].split(' '))\n",
        "  print (dataset.one_x_as_text(x_train_d[i]))\n",
        "  print (dataset.one_y_as_text(y_train[i]))"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "tokens  : [1997.   80.  116.   20.  176.  121.    4.  759.    5. 1999.]\n",
            "as words: ['t_bbe.csv', '\"Now', 'Joseph', 'was', 'taken', 'down', 'to', 'Egypt;', 'and', '<oov>']\n",
            "original: ['\"Now', 'Joseph', 'was', 'taken', 'down', 'to', 'Egypt;', 'and', 'Potiphar', 'the', 'Egyptian']\n",
            "t_bbe.csv \"Now Joseph was taken down to Egypt; and <oov>\n",
            "\"Now Joseph was taken down to Egypt; and <oov> the\n",
            "\n",
            "tokens  : [1.997e+03 3.000e+00 1.000e+00 2.400e+01 2.000e+01 2.800e+01 1.160e+02\n",
            " 0.000e+00 0.000e+00 0.000e+00]\n",
            "as words: ['t_bbe.csv', '\"And', 'the', 'Lord', 'was', 'with', 'Joseph', '<s>', '<s>', '<s>']\n",
            "original: ['\"And', 'the', 'Lord', 'was', 'with', 'Joseph']\n",
            "t_bbe.csv \"And the Lord was with Joseph <s> <s> <s>\n",
            "\"And the Lord was with Joseph <s> <s> <s> <s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BWVEcaeF6DBR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model defintion\n"
      ]
    },
    {
      "metadata": {
        "id": "8ceIddxbqWdD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "kqkLdFGl0FMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "682ae052-3bfe-4709-e045-3071f813917c"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding,CuDNNLSTM,Bidirectional,Concatenate,Dropout\n",
        "\n",
        "# size of tokenizer indexes\n",
        "\n",
        "num_decoder_tokens = num_encoder_tokens = len(dataset.word2index) \n",
        "print (num_decoder_tokens)\n",
        "\n",
        "embedding_dim=300\n",
        "latent_dim = 256\n",
        "batch_size=64\n",
        "epochs=30\n",
        "\n",
        "bidi_encoder=False\n",
        "cuddlstm=True  #on bidi , diff in time is 20s vs 32 se\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None,),name='encoder_inputs')\n",
        "\n",
        "shared_embedding = Embedding(num_encoder_tokens, \n",
        "                     embedding_dim, \n",
        "                     #weights=[word_embedding_matrix], if there is one\n",
        "                     #trainable=False,                            \n",
        "                     #input_length=MAX_SEQUENCE_LENGTH, if there is one\n",
        "                     )\n",
        "#see dropout disucssion: https://github.com/keras-team/keras/issues/7290. iliaschalkidis \n",
        "#Dropout(noise_shape=(batch_size, 1, features))\n",
        "x = shared_embedding(encoder_inputs) \n",
        "if (cuddlstm):\n",
        "  encoder_lstm=CuDNNLSTM(latent_dim, return_state=True)\n",
        "else:\n",
        "  print ('using LSTM with dropout!')\n",
        "  #need to tune the dropout values (maybe fast.ai tips) , just invented those value\n",
        "  encoder_lstm=LSTM(latent_dim, return_state=True,dropout=0.3,recurrent_dropout=0.3)\n",
        "if (bidi_encoder):\n",
        "  encoder_lstm=Bidirectional(encoder_lstm,merge_mode='concat')\n",
        "  x, forward_h, forward_c, backward_h, backward_c = encoder_lstm(x) #output,h1,c1,h2,c2\n",
        "  state_h = Concatenate()([forward_h, backward_h])\n",
        "  state_c = Concatenate()([forward_c, backward_c])\n",
        "else:  \n",
        "  \n",
        "  x, state_h, state_c = encoder_lstm(x)\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,),name='decoder_inputs')\n",
        "\n",
        "decoder_latent_dim = latent_dim*2 if bidi_encoder else latent_dim #bi-di pass merge of h1+h2, c1+c2\n",
        "if (cuddlstm):\n",
        "  decoder_lstm = CuDNNLSTM(decoder_latent_dim, return_sequences=True,return_state=True) #returned state used in inference\n",
        "else:\n",
        "  decoder_lstm = LSTM(decoder_latent_dim, return_sequences=True,return_state=True)\n",
        "decoder_outputs, _, _  = decoder_lstm(shared_embedding(decoder_inputs), initial_state=encoder_states)\n",
        "decoder_dense  = Dense(num_decoder_tokens, activation='softmax',name='decoder_softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile & run training\n",
        "print (model.summary())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# now for the INFER models (re-arrangement of the prev one)\n",
        "# Remember that the training model varaibles were:\n",
        "#                                        decoder_outputs\n",
        "#encoder   --->    encoder_states  -->   decoder_lstm  \n",
        "#shared_embedding                        shared_embeddings  \n",
        "#encoder_inputs                          decdoer_inputs                  \n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_states_inputs = [Input(shape=(decoder_latent_dim,)), Input(shape=(decoder_latent_dim,))]\n",
        "\n",
        "decoder_outputs2, state_h, state_c = decoder_lstm(shared_embedding(decoder_inputs), initial_state=decoder_states_inputs)\n",
        "\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states)\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "decoder_inputs (InputLayer)     (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_inputs (InputLayer)     (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_10 (Embedding)        (None, None, 300)    600000      encoder_inputs[0][0]             \n",
            "                                                                 decoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_19 (CuDNNLSTM)       [(None, 256), (None, 571392      embedding_10[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_20 (CuDNNLSTM)       [(None, None, 256),  571392      embedding_10[1][0]               \n",
            "                                                                 cu_dnnlstm_19[0][1]              \n",
            "                                                                 cu_dnnlstm_19[0][2]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder_softmax (Dense)         (None, None, 2000)   514000      cu_dnnlstm_20[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 2,256,784\n",
            "Trainable params: 2,256,784\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S983Fs2WYshb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VTobPYftYvQC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  classifier\n",
        "\n",
        "**Intro**\n",
        "\n",
        "(1) We start with regukar seq2seq is encoder->embedding->decoder and trained with reconstruction-loss\n",
        "\n",
        "(2)We can build style-discriminator where the target is to classify author-style from the sentence-embedding.\n",
        "When training it you need to freeze the encoder and decoder parts of the model, then:\n",
        "input1: sentence --freezed encoder--> embedding    (no need to run decoder)\n",
        "input2: style (one-hot)\n",
        "output: style (one-hot)  \n",
        "The discriminator can be a simple classifier (dense-based) with simple minimize cross-entropy target.\n",
        "\n",
        "(3) The smart-part: We want to train the encoder to create an embedding which will fool the discriminator.\n",
        "We will freeze the discriminator weights, and train the encoder-decoder similiarly to (1) with extra objective.\n",
        "That the loss from the discriminator will be Maximized. \n"
      ]
    },
    {
      "metadata": {
        "id": "2BIl7v9DnM5j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#### Classifier model #############################\n",
        "# works on the embedding itself\n",
        "style_concat= Concatenate()(encoder_states)\n",
        "\n",
        "a = Dropout(0.1)(style_concat)                          \n",
        "# if your inputs have shape  (batch_size, timesteps, features) and you want the dropout mask to be the same for all timesteps, you can use noise_shape=(batch_size, 1, features).\n",
        "a = Dense(100,activation='relu',name='d_dense1')(a)\n",
        "a = Dropout(0.1)(a)\n",
        "style_outputs = Dense(len(dataset.style2index),activation='softmax',name='d_dense_softmax')(a)\n",
        "\n",
        "style_classifier_model = Model(encoder_inputs,style_outputs)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9TBO_5dXv40K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## adverserial model"
      ]
    },
    {
      "metadata": {
        "id": "owHJkiERHsG1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "7ffe1ba4-8bcc-4349-fccc-1960952e81ef"
      },
      "cell_type": "code",
      "source": [
        "############################# Adv model\n",
        "# Note that it does not have new layers, just combining all of them with new loss\n",
        "def inverse_categorical_crossentropy(y_true, y_pred):\n",
        "  #need to implement it better , sum(1/categorical_crossentropy_per_sample)\n",
        "  return 1/(K.categorical_crossentropy(y_true, y_pred)+0.0001)\n",
        "#style_outputs\n",
        "adv_model = Model([encoder_inputs, decoder_inputs],[decoder_outputs,style_outputs])\n",
        "\n",
        "print (adv_model.summary())\n"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "decoder_inputs (InputLayer)     (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_inputs (InputLayer)     (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_10 (Embedding)        (None, None, 300)    600000      encoder_inputs[0][0]             \n",
            "                                                                 decoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_19 (CuDNNLSTM)       [(None, 256), (None, 571392      embedding_10[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 512)          0           cu_dnnlstm_19[0][1]              \n",
            "                                                                 cu_dnnlstm_19[0][2]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 512)          0           concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "d_dense1 (Dense)                (None, 100)          51300       dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_20 (CuDNNLSTM)       [(None, None, 256),  571392      embedding_10[1][0]               \n",
            "                                                                 cu_dnnlstm_19[0][1]              \n",
            "                                                                 cu_dnnlstm_19[0][2]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 100)          0           d_dense1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "decoder_softmax (Dense)         (None, None, 2000)   514000      cu_dnnlstm_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "d_dense_softmax (Dense)         (None, 2)            202         dropout_28[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 2,308,286\n",
            "Trainable params: 2,308,286\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YjI-3lfJv62T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "d6dd90f4-c447-4425-e8b0-8581593bbdf5"
      },
      "cell_type": "code",
      "source": [
        "# compile it all\n",
        "def print_trainable(model):\n",
        "  for layer in model.layers:\n",
        "    print (layer.name,layer,layer.trainable)\n",
        "    \n",
        "\n",
        "def set_trainable(model,trainable) :\n",
        "  \"\"\" set all layers of the model (ignores Input) to trainable True/False\"\"\"\n",
        "  for layer in model.layers:\n",
        "    if type(layer)==keras.engine.topology.InputLayer:\n",
        "      pass\n",
        "    else:\n",
        "      layer.trainable = trainable \n",
        "\n",
        "######################################3\n",
        "#compile all, set trainable parts (as keras hold it before compilation)\n",
        "set_trainable(d,model)    \n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')  #30peocs loss: 0.2836 - val_loss: 0.4428\n",
        "print ('\\nmodel compiled with:')\n",
        "print_trainable(model)\n",
        "\n",
        "\n",
        "# when training d, the encoder should not change.\n",
        "# impl. detail: instead of choose layers one by one, we first set all d True then override part with False\n",
        "set_trainable(d,True)    \n",
        "set_trainable(encoder_model,False) #setting it back(it's parts of d)\n",
        "d.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "print ('\\nd compiled with:')\n",
        "print_trainable(d)\n",
        "\n",
        "# when training adv_model, the decoder should not change\n",
        "set_trainable(d,False)\n",
        "set_trainable(model,True)\n",
        "adv_model.compile(optimizer='adam', \n",
        "                  loss=['categorical_crossentropy',inverse_categorical_crossentropy],\n",
        "                  loss_weights=[1, 10])\n",
        "print ('\\nadv compiled with:')\n",
        "print_trainable(adv_model)\n"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "model compiled with:\n",
            "decoder_inputs <keras.engine.topology.InputLayer object at 0x7f2e2afeda20> False\n",
            "encoder_inputs <keras.engine.topology.InputLayer object at 0x7f2e2afedfd0> False\n",
            "embedding_10 <keras.layers.embeddings.Embedding object at 0x7f2e2afede10> <keras.engine.training.Model object at 0x7f2ec7322be0>\n",
            "cu_dnnlstm_19 <keras.layers.cudnn_recurrent.CuDNNLSTM object at 0x7f2e2afeda90> <keras.engine.training.Model object at 0x7f2ec7322be0>\n",
            "cu_dnnlstm_20 <keras.layers.cudnn_recurrent.CuDNNLSTM object at 0x7f2e2b1fb7b8> True\n",
            "decoder_softmax <keras.layers.core.Dense object at 0x7f2e2b14ff98> True\n",
            "\n",
            "d compiled with:\n",
            "encoder_inputs <keras.engine.topology.InputLayer object at 0x7f2e2afedfd0> False\n",
            "embedding_10 <keras.layers.embeddings.Embedding object at 0x7f2e2afede10> False\n",
            "cu_dnnlstm_19 <keras.layers.cudnn_recurrent.CuDNNLSTM object at 0x7f2e2afeda90> False\n",
            "concatenate_16 <keras.layers.merge.Concatenate object at 0x7f2ec809c8d0> True\n",
            "dropout_25 <keras.layers.core.Dropout object at 0x7f2ec82a8080> True\n",
            "d_dense1 <keras.layers.core.Dense object at 0x7f2e2ae9c080> True\n",
            "dropout_26 <keras.layers.core.Dropout object at 0x7f2e2b2d7048> True\n",
            "d_dense_softmax <keras.layers.core.Dense object at 0x7f2ec809c3c8> True\n",
            "\n",
            "adv compiled with:\n",
            "decoder_inputs <keras.engine.topology.InputLayer object at 0x7f2e2afeda20> False\n",
            "encoder_inputs <keras.engine.topology.InputLayer object at 0x7f2e2afedfd0> False\n",
            "embedding_10 <keras.layers.embeddings.Embedding object at 0x7f2e2afede10> True\n",
            "cu_dnnlstm_19 <keras.layers.cudnn_recurrent.CuDNNLSTM object at 0x7f2e2afeda90> True\n",
            "concatenate_17 <keras.layers.merge.Concatenate object at 0x7f2e2b1fb2b0> True\n",
            "dropout_27 <keras.layers.core.Dropout object at 0x7f2e2b1fb278> True\n",
            "d_dense1 <keras.layers.core.Dense object at 0x7f2e2b1fb400> True\n",
            "cu_dnnlstm_20 <keras.layers.cudnn_recurrent.CuDNNLSTM object at 0x7f2e2b1fb7b8> True\n",
            "dropout_28 <keras.layers.core.Dropout object at 0x7f2e2b1fb320> True\n",
            "decoder_softmax <keras.layers.core.Dense object at 0x7f2e2b14ff98> True\n",
            "d_dense_softmax <keras.layers.core.Dense object at 0x7f2ec7a3ee80> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_ZC94QpK1bcw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_decoder_seq_length=dataset.MAX_SEQUENCE_LENGTH\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq,style,verbose=False):\n",
        "    assert input_seq.shape == (1, max_decoder_seq_length )\n",
        "    if verbose: print ('input_seq',input_seq.shape)\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    if verbose: print ('encoder result states_value','h',states_value[0].shape,states_value[0].mean(),'c',states_value[1].shape,states_value[1].mean())\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    #                      batch,word-number value is token (0 /122)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = dataset.word2index[style]\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = []\n",
        "    while not stop_condition:\n",
        "        #start with encoder-state then change to self state\n",
        "        output_tokens, h, c = decoder_model.predict( [target_seq] + states_value) \n",
        "        if verbose: print ('output_tokens',output_tokens.shape,output_tokens.mean(),'h',h.shape,'c',c.shape)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens)# [0, -1, :])\n",
        "        if verbose: print ('sampled_token_index',sampled_token_index.shape,output_tokens.max(),sampled_token_index)\n",
        "        decoded_sentence.append(sampled_token_index)\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_token_index == dataset.word2index['<s>'] or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "          stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    res= np.array(decoded_sentence) #[dataset.index2word[index] for index in decoded_sentence]\n",
        "    return res\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9s0-8EJi6AG1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TRAINING"
      ]
    },
    {
      "metadata": {
        "id": "0_0G8_Dr2bMM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "      self.losses = {'loss':[],'val_loss':[]}\n",
        "      \n",
        "    #def on_train_begin(self, logs={}):\n",
        "    #  pass  \n",
        "    \n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "      for loss in ['loss','val_loss']:\n",
        "        self.losses[loss].append(logs.get(loss))\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "loss_history = LossHistory()\n",
        "loss_history_d = LossHistory()\n",
        "loss_history_adv = LossHistory()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sm05AY862pem",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "9c908d46-beab-4a63-9122-09e3a28b0e55"
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def train_g(epochs):\n",
        "    model.fit([x_train, x_train_d], y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            #verbose=2,\n",
        "            validation_data=([x_val, x_val_d], y_val),callbacks=[loss_history,\n",
        "                                                                #ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=3)\n",
        "                                                                ])\n",
        "def train_d(epochs):\n",
        "  # check wieghts\n",
        "  d.trainable = True  #only for warning\n",
        "  d.fit(x_train, style_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            #verbose=2,\n",
        "            validation_data=(x_val,style_val),\n",
        "            callbacks=[loss_history_d,\n",
        "            #ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=3)\n",
        "              ])\n",
        "  d.trainable = False  #only for warning\n",
        "\n",
        "  \n",
        "def train_adv(epochs):\n",
        "  adv_model.fit([x_train, x_train_d], [y_train,style_train],\n",
        "            batch_size=batch_size,\n",
        "            #verbose=2,\n",
        "            epochs=epochs,\n",
        "            validation_data=([x_val, x_val_d], [y_val,style_val]),callbacks=[loss_history_adv])\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "def print_d_mean():\n",
        "  print(d.get_layer('d_dense_softmax').get_weights()[0].mean(),'b',d.get_layer('d_dense_softmax').get_weights()[1].mean())\n",
        "\n",
        "'''\n",
        "train_g(3)\n",
        "train_d(2)\n",
        "train_g(20)\n",
        "train_adv(10)\n",
        "train_d(10)\n",
        "'''  \n",
        "train_adv(5)\n",
        "\n"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 16000 samples, validate on 2000 samples\n",
            "Epoch 1/5\n",
            "16000/16000 [==============================] - 8s 492us/step - loss: 5.4290 - decoder_softmax_loss: 0.2763 - d_dense_softmax_loss: 0.5153 - val_loss: 97.6461 - val_decoder_softmax_loss: 0.8268 - val_d_dense_softmax_loss: 9.6819\n",
            "Epoch 2/5\n",
            "16000/16000 [==============================] - 8s 480us/step - loss: 5.1784 - decoder_softmax_loss: 0.2173 - d_dense_softmax_loss: 0.4961 - val_loss: 9.9092 - val_decoder_softmax_loss: 0.9473 - val_d_dense_softmax_loss: 0.8962\n",
            "Epoch 3/5\n",
            " 4032/16000 [======>.......................] - ETA: 5s - loss: 5.8089 - decoder_softmax_loss: 0.2759 - d_dense_softmax_loss: 0.5533"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16000/16000 [==============================] - 8s 487us/step - loss: 5.4386 - decoder_softmax_loss: 0.2505 - d_dense_softmax_loss: 0.5188 - val_loss: 75.4238 - val_decoder_softmax_loss: 0.8276 - val_d_dense_softmax_loss: 7.4596\n",
            "Epoch 4/5\n",
            "16000/16000 [==============================] - 8s 481us/step - loss: 4.8399 - decoder_softmax_loss: 0.1871 - d_dense_softmax_loss: 0.4653 - val_loss: 116.6845 - val_decoder_softmax_loss: 0.8139 - val_d_dense_softmax_loss: 11.5871\n",
            "Epoch 5/5\n",
            " 8512/16000 [==============>...............] - ETA: 3s - loss: 4.3149 - decoder_softmax_loss: 0.1510 - d_dense_softmax_loss: 0.4164"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "16000/16000 [==============================] - 8s 477us/step - loss: 4.6668 - decoder_softmax_loss: 0.1709 - d_dense_softmax_loss: 0.4496 - val_loss: 17.8018 - val_decoder_softmax_loss: 0.9164 - val_d_dense_softmax_loss: 1.6885\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R_OBmX182djj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "8382ca64-912e-4888-a8f1-dfd9dfc9554c"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "# summarize history for loss\n",
        "def plt_losses(loss_history,title):\n",
        "  plt.plot(loss_history.losses['loss'][:])\n",
        "  plt.plot(loss_history.losses['val_loss'][:])\n",
        "  plt.title(title)\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper right')\n",
        "  \n",
        "\n",
        "plt.figure(figsize=(14,4))\n",
        "plt.subplot(131) #numrows, numcols, fignum\n",
        "plt_losses(loss_history,'g loss')  \n",
        "plt.subplot(132)\n",
        "plt_losses(loss_history_d,'d loss')  \n",
        "plt.subplot(133)\n",
        "plt_losses(loss_history_adv,'adv loss') \n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0QAAAEVCAYAAADAVCVVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRERExLb+H92JMPCDzOuEAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f2e278b6f28>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "HWiYXoTTX02f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        },
        "outputId": "0fa23830-7806-4778-b767-eb3c190be9de"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "\n",
            "gold  x:  ['At', 'the', 'first', 'God', 'made', 'the', 'heaven', 'and', 'the', 'earth.']\n",
            "gold_styl: ['t_bbe.csv', 'At', 'the', 'first', 'God', 'made', 'the', 'heaven', 'and', 'the']\n",
            "gold y:  ['At', 'the', 'first', 'God', 'made', 'the', 'heaven', 'and', 'the', 'earth.']\n",
            "actual y: ['At', 'the', 'first', 'God', 'made', 'the', 'heaven', 'and', 'the', 'earth.', 'cry']\n",
            "\n",
            "gold  x:  ['In', 'the', '<oov>', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth.']\n",
            "gold_styl: ['t_kjv.csv', 'In', 'the', '<oov>', 'God', 'created', 'the', 'heaven', 'and', 'the']\n",
            "gold y:  ['In', 'the', '<oov>', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth.']\n",
            "actual y: ['In', 'the', '<oov>', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth.', 'relation']\n",
            "\n",
            "gold  x:  ['And', 'the', 'earth', 'was', 'waste', 'and', 'without', '<oov>', 'and', 'it']\n",
            "gold_styl: ['t_bbe.csv', 'And', 'the', 'earth', 'was', 'waste', 'and', 'without', '<oov>', 'and']\n",
            "gold y:  ['And', 'the', 'earth', 'was', 'waste', 'and', 'without', '<oov>', 'and', 'it']\n",
            "actual y: ['And', 'the', 'earth', 'was', 'waste', 'and', 'without', '<oov>', 'and', 'it', '<oov>']\n",
            "\n",
            "gold  x:  ['\"And', 'the', 'earth', 'was', 'without', '<oov>', '<s>', '<s>', '<s>', '<s>']\n",
            "gold_styl: ['t_kjv.csv', '\"And', 'the', 'earth', 'was', 'without', '<oov>', '<s>', '<s>', '<s>']\n",
            "gold y:  ['\"And', 'the', 'earth', 'was', 'without', '<oov>', '<s>', '<s>', '<s>', '<s>']\n",
            "actual y: ['\"And', 'the', 'earth', 'was', 'without', '<oov>', '<s>']\n",
            "\n",
            "gold  x:  ['\"Then', 'Hamor', 'and', 'Shechem', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
            "gold_styl: ['t_bbe.csv', '\"Then', 'Hamor', 'and', 'Shechem', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
            "gold y:  ['\"Then', 'Hamor', 'and', 'Shechem', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
            "actual y: ['\"Then', 'Hamor', 'and', 'Shechem', '<s>']\n",
            "\n",
            "gold  x:  ['\"And', 'Hamor', 'and', 'Shechem', 'his', 'son', 'came', 'unto', 'the', 'gate']\n",
            "gold_styl: ['t_kjv.csv', '\"And', 'Hamor', 'and', 'Shechem', 'his', 'son', 'came', 'unto', 'the']\n",
            "gold y:  ['\"And', 'Hamor', 'and', 'Shechem', 'his', 'son', 'came', 'unto', 'the', 'gate']\n",
            "actual y: ['\"And', 'Hamor', 'and', 'Shechem', 'his', 'son', 'came', 'unto', 'the', 'gate', 'of']\n",
            "\n",
            "gold  x:  ['\"It', 'is', 'the', 'desire', 'of', 'these', 'men', 'to', 'be', 'at']\n",
            "gold_styl: ['t_bbe.csv', '\"It', 'is', 'the', 'desire', 'of', 'these', 'men', 'to', 'be']\n",
            "gold y:  ['\"It', 'is', 'the', 'desire', 'of', 'these', 'men', 'to', 'be', 'at']\n",
            "actual y: ['\"It', 'is', 'the', 'desire', 'of', 'these', 'men', 'to', 'be', 'at', 'the']\n",
            "\n",
            "gold  x:  ['\"These', 'men', 'are', '<oov>', 'with', 'us;', 'therefore', 'let', 'them', 'dwell']\n",
            "gold_styl: ['t_kjv.csv', '\"These', 'men', 'are', '<oov>', 'with', 'us;', 'therefore', 'let', 'them']\n",
            "gold y:  ['\"These', 'men', 'are', '<oov>', 'with', 'us;', 'therefore', 'let', 'them', 'dwell']\n",
            "actual y: ['\"These', 'men', 'are', '<oov>', 'with', 'us;', 'therefore', 'let', 'them', 'dwell', '<s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I2NGxSQb2dxD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "c9fcbba8-8e48-459e-d563-934d3e977955"
      },
      "cell_type": "code",
      "source": [
        "#print ('eval on train:',model.evaluate([x_train, x_train_d], y_train))\n",
        "#print ('eval on val:',model.evaluate([x_val, x_val_d], y_val))\n",
        "\n",
        "s=20\n",
        "e=s+1\n",
        "print ('score',model.evaluate([x_train[s:e], x_train_d[s:e]], y_train[s:e],batch_size=batch_size,verbose=0))\n",
        "\n",
        "show_sample('train',False,s) \n",
        "\n",
        "\n",
        "print ('\\n COMPARE TO VAL:\\n')\n",
        "print('score',model.evaluate([x_val[s:e], x_val_d[s:e]], y_val[s:e],batch_size=batch_size,verbose=0))\n",
        "show_sample('val',False,s) \n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "score 0.006540380418300629\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-67451f46e0d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mshow_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-7470d20ad84d>\u001b[0m in \u001b[0;36mshow_sample\u001b[0;34m(data_type, teacher_forcing, sample_ids, replace_style)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mteacher_forcing\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mwe\u001b[0m \u001b[0msimulate\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m       \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "XvIkFOWy55ov",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Error analysis"
      ]
    },
    {
      "metadata": {
        "id": "Ob6kDbRUBJki",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "outputId": "58270786-998c-4c4d-d961-2d9c772c93c6"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def show_sample(data_type='val',teacher_forcing=False,sample_ids=[1000],replace_style=None):\n",
        "  \"\"\" \n",
        "    teacher_forcing : should we simulate training and feed the input. default False, as for test\n",
        "  \"\"\"\n",
        "  for i in sample_ids:\n",
        "      data={'train': dataset.result.train,'val':dataset.result.val , 'test':dataset.result.test}\n",
        "      \n",
        "      print ('\\n')\n",
        "      one_x= data[data_type][0][i:i+1]\n",
        "\n",
        "      one_x_d= data[data_type][1][i:i+1]\n",
        "      if replace_style:\n",
        "        if dataset.index2word[one_x_d[0,0]]==replace_style:\n",
        "           print ('No need to replace style, already in',replace_style)\n",
        "        style_as_text = replace_style\n",
        "        one_x_d= np.copy(one_x_d)\n",
        "        one_x_d[0,0]=dataset.word2index[replace_style]\n",
        "      else:\n",
        "        style_as_text = dataset.index2word[one_x_d[0][0]]\n",
        "        \n",
        "      one_y= data[data_type][2][i:i+1]\n",
        "      if one_x.shape[0]==0:\n",
        "        print ('sample out of range',data[data_type][0].shape)\n",
        "      print ('gold  x: ',dataset.one_x_as_text(one_x))\n",
        "      #internal debug print ('gold_styl:',dataset.one_x_as_text(one_x_d))\n",
        "      #internal debug print ('gold y: ',dataset.one_y_as_text(one_y))\n",
        "      \n",
        "      \n",
        "      if teacher_forcing:\n",
        "        p = model.predict([one_x,one_x_d])\n",
        "        print ('actual y:',dataset.one_y_as_text(p))  \n",
        "        print ('used teacher-forcing:',one_x_d[0][0])\n",
        "      else:\n",
        "        p = decode_sequence(one_x,style_as_text)\n",
        "        print ('actual y:',dataset.one_x_as_text(p))   \n",
        "        print ('used sample:',style_as_text)\n",
        "\n",
        "\n",
        "#show_sample(data_type='train',sample_ids=[0,8000,16000])#,replace_style='t_bbe.csv')    \n",
        "#show_sample(data_type='train',sample_ids=[1,8001,16001])#,replace_style='t_bbe.csv')  \n",
        "\n",
        "\n",
        "for i in [0,1,1000,1001]:\n",
        "  print ('#'*30,'verb',i,'#'*30)\n",
        "  show_sample('train',sample_ids=[i,8000+i],teacher_forcing=True,replace_style='t_bbe.csv') "
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "############################## verb 0 ##############################\n",
            "\n",
            "\n",
            "No need to replace style, already in t_bbe.csv\n",
            "gold  x:  At the first God made the heaven and the earth.\n",
            "actual y: May the first God made the people and the head\n",
            "used teacher-forcing: 1997.0\n",
            "\n",
            "\n",
            "gold  x:  In the <oov> God created the heaven and the earth.\n",
            "actual y: In the <oov> God the the earth and the Gadites\n",
            "used teacher-forcing: 1997.0\n",
            "############################## verb 1 ##############################\n",
            "\n",
            "\n",
            "No need to replace style, already in t_bbe.csv\n",
            "gold  x:  And the earth was waste and without <oov> and it\n",
            "actual y: And the earth was on and without <oov> and <oov>\n",
            "used teacher-forcing: 1997.0\n",
            "\n",
            "\n",
            "gold  x:  \"And the earth was without <oov> <s> <s> <s> <s>\n",
            "actual y: \"And the earth was like <oov> <s> <s> <s> <s>\n",
            "used teacher-forcing: 1997.0\n",
            "############################## verb 1000 ##############################\n",
            "\n",
            "\n",
            "No need to replace style, already in t_bbe.csv\n",
            "gold  x:  \"Then Hamor and Shechem <s> <s> <s> <s> <s> <s>\n",
            "actual y: \"Then Hamor and Hamor <s> <s> <s> <s> <s> <s>\n",
            "used teacher-forcing: 1997.0\n",
            "\n",
            "\n",
            "gold  x:  \"And Hamor and Shechem his son came unto the gate\n",
            "actual y: \"And Hamor and Shechem his son came unto the wilderness\n",
            "used teacher-forcing: 1997.0\n",
            "############################## verb 1001 ##############################\n",
            "\n",
            "\n",
            "No need to replace style, already in t_bbe.csv\n",
            "gold  x:  \"It is the desire of these men to be at\n",
            "actual y: \"It is the man of these men to be at\n",
            "used teacher-forcing: 1997.0\n",
            "\n",
            "\n",
            "gold  x:  \"These men are <oov> with us; therefore let them dwell\n",
            "actual y: \"These sons are <oov> with us; even let them forth\n",
            "used teacher-forcing: 1997.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TkFXxinr0fNo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "1750af53-9416-4f6a-cfd6-efe4ac984f08"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#a= model.predict([x_train[s:e], x_train_d[s:e]])\n",
        "#for i in range(dataset.MAX_SEQUENCE_LENGTH):\n",
        "#  best=np.argmax(a[0,i])\n",
        "#  print (i,best,dataset.index2word[best],a[0,i,best],a[0,i,0])\n",
        "\n",
        "from keras.losses import categorical_crossentropy\n",
        "p=model.predict([x_val,x_val_d])\n",
        "scores=K.eval(K.sum(categorical_crossentropy(K.constant(p), K.constant(y_val) ),axis=1))\n",
        "worse_10 = scores.argsort()[::-1][:10]\n",
        "\n",
        "for i in range(len(worse_10)):\n",
        "  bad=worse_10[i]\n",
        "  print (i,'arg',bad,'score',scores[bad],show_sample('val',False,bad))\n",
        "  \n"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-1e4d70dec522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworse_10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mbad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworse_10\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'arg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-7470d20ad84d>\u001b[0m in \u001b[0;36mshow_sample\u001b[0;34m(data_type, teacher_forcing, sample_ids, replace_style)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mteacher_forcing\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mwe\u001b[0m \u001b[0msimulate\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m       \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.int64' object is not iterable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8QaOXXjUAwVH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Error of style disc."
      ]
    },
    {
      "metadata": {
        "id": "t_YnBS4fAr1M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Error of style discriminator\n",
        "\n",
        "#print (style_train[:10])\n",
        "#style_classifier_model.predict(x_train[:10])#, style_train,\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
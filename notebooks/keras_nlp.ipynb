{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_nlp.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/assaflehr/language-style-transfer/blob/master/notebooks/keras_nlp.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "XAObLLyiv6Jx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# adaptation of: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
        "# first Dataset class to load bible-data\n",
        "# then copy of the model, but working with words instead of chars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PcPSCDUrx3B8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "626852d0-f2ca-4d8e-ba33-a3a5da591c53"
      },
      "cell_type": "code",
      "source": [
        "## NLP preprocessing for text\n",
        "# has few parts:\n",
        "# 1. load zip files and then use glob to filter part of them (data/*/*.txt)\n",
        "# 2. parse each row into (x,y) by passing a parser method. it can be simple as lambda line:line:x, or if you use tab delimited lambda line: line.split(',')[4]\n",
        "# 3. tokenize - split by spaces, but also by ., and be smart about it.  ('...' should be one token , \"ai'nt\" one token. then; should be two 'token' and ';')\n",
        "#    you should also build vocabulary, keep X words and throw away rare ones, they will be replaced by <oov> flag.\n",
        "# 4. transform text to sequences for the result. for words there are usually two different types: ['s>','hello', 'world'] -> [0,5,6] but there is also \n",
        "#     a one-hot-econding version where 5 is actaully a vector of size voc-length full of zeros, with 5th index==1.\n",
        "#    The one-hot ecoding is used as output for text-generation and has a HUGE MEMORY requirement.  100K sentences of size 20 words need 2M floats = 8MB\n",
        "#    But for the one-hot-encoding multiply this by vocab-size. for char-encoding it's ~30 , for good vocab of 10K words, we need 80GB(!)\n",
        "#    The simple, and only , way to solve this , is to never keep one-hot-encoding in memory, just use a generator to make it one-hot in runtime\n",
        "\n",
        "\n",
        "\n",
        "# output:\n",
        "# LM as classification: x is first N tokens . y is only one tokens N+1\n",
        "#    as seq2seq: x is N tokens.  y is N tokens, with are advanced by 1.\n",
        "# can be both char level or word level\n",
        "\n",
        "# Pairs classification\n",
        "# x is 2 sentences (x1,x2) , y is label (duplicate/not.  entitelitmennt/neutral/...)\n",
        "\n",
        "# translation\n",
        "# x is sentence of size N, y is sentence of different size M\n",
        "\n",
        "#see: torchtext http://anie.me/On-Torchtext/\n",
        "\n",
        "\n",
        "#TODO : \n",
        "# support more types\n",
        "# one file with 2 sentences each row, tab-delimited.  (parallel-data)\n",
        "# 2 parallel folders, multiple files inside internal folder in each 1 sentence each row. (parallel-data)\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import csv, json\n",
        "from collections import namedtuple\n",
        "from zipfile import ZipFile\n",
        "from os.path import expanduser, exists\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "TVT = namedtuple('TVT',['train','val','test'])\n",
        "\n",
        "class Dataset:\n",
        "\n",
        "  \n",
        "  def __init__(self,unique_name,url,extract,cache_dir,pattern=None,validation=0.1,test=0.1):\n",
        "    '''\n",
        "    unique_name will be used for the dataset source(or zip) file. \n",
        "    pattern need to include path inside zip (including zip root)\n",
        "    extract - is it zipped/tarred or not\n",
        "    cache_dir - under which the files be downloaded <cache_dir>/datasets/<unique_name>\n",
        "    pattern - glob will be done to choose only those files ,for example data*.txt. This should incldude both train and test\n",
        "    validation - subset glob pattern to use. If it's a float like 0.1, use it as split of one file\n",
        "    test - see above\n",
        "    '''\n",
        "    if not extract and pattern:\n",
        "      raise ValueError('pattern must be empty if extract=False chooses a subset of the files (data/*.txt). but you downloaded only one file')\n",
        "\n",
        "    if not os.path.exists(cache_dir):\n",
        "       os.makedirs(cache_dir)\n",
        "\n",
        "    fpath=get_file(unique_name, url,extract=True, cache_dir=cache_dir)\n",
        "    print ('fpath',fpath)\n",
        "    files = [fpath] if not pattern else glob.glob(f'{cache_dir}/datasets/{pattern}')\n",
        "\n",
        "    lines = [line.rstrip() for f in files for line in open(f).readlines()] \n",
        "    print (files,'#lines',len(lines),'first 3 lines')\n",
        "    print (lines[0],'\\n',lines[1],'\\n',lines[2])\n",
        "    \n",
        "    if isinstance(validation,float) and isinstance(test,float):\n",
        "      test_count = int(len(lines)*(1-test))\n",
        "      val_count =  int(len(lines)*(1-test-validation))\n",
        "      self.tvt_lines = TVT(lines[:val_count],lines[val_count:test_count],lines[test_count:])\n",
        "    \n",
        "    print ('train:',len(self.tvt_lines.train),'val',len(self.tvt_lines.val),'test',len(self.tvt_lines.test))\n",
        "  \n",
        "  def parse(self,row_parser,skip_first=False):\n",
        "    self.parsed=[]\n",
        "    for i,lines in enumerate(self.tvt_lines):\n",
        "      self.parsed.append([row_parser(line) for line in lines[1 if skip_first else 0:]])\n",
        "    self.parsed= TVT(*self.parsed)\n",
        "    print ('\\nrow_parser train:',self.parsed.train[0],'test:',self.parsed.test[0])\n",
        "    \n",
        "  def tokenize(self):\n",
        "    \"\"\" the current implementation is quite bad, hello world! will be 2 tokens world! is the second. \n",
        "    \"\"\"\n",
        "    print ('limiting num_words in Tokenizer due to MEMORY BOUNDS')  #num_words =100*1000\n",
        "\n",
        "    self.tokenizer = Tokenizer(num_words=100000, filters='', lower=False, split=' ', char_level=False, oov_token='<oov>')\n",
        "    print ('\\nonly tokenizing x[0], not y!!! using all words, pretty bad tokenizer!!!')\n",
        "    print (type(self.parsed.train),self.parsed.train[0][0])\n",
        "    self.tokenizer.fit_on_texts([x for x,y in self.parsed.train])\n",
        "    \n",
        "    print ('\\n word_index',len(self.tokenizer.word_index),'<oov>',self.tokenizer.word_index['<oov>'])\n",
        "    print ('common',list(self.tokenizer.word_index.items())[:15])\n",
        "    print ('uncommon',list(self.tokenizer.word_index.items())[-15:])\n",
        "  \n",
        "    num_words= 666\n",
        "    print ('keeping only ',num_words,'of',len(self.tokenizer.word_index))\n",
        "           \n",
        "           \n",
        "    word2index = dict(list(self.tokenizer.word_index.items())[:num_words-1])\n",
        "    word2index['<s>']=0  #keras tokenizer keeps 0 unused\n",
        "    word2index['<oov>']=num_words-1\n",
        "    #FOR NOW the start and end are both ZERO. maybe not good???\n",
        "    \n",
        "    num_encoder_tokens = num_decoder_tokens= num_words # len(self.tokenizer.word_index)\n",
        "    \n",
        "    MAX_SEQUENCE_LENGTH=20\n",
        "    \n",
        "    \n",
        "    result = []\n",
        "    for rows in self.parsed:\n",
        "      input_texts = [x for x,y in rows]\n",
        "      encoder_input_data  = np.zeros( (len(input_texts), MAX_SEQUENCE_LENGTH),    dtype='float32')\n",
        "      decoder_target_data = np.zeros((len(input_texts),  MAX_SEQUENCE_LENGTH, num_decoder_tokens),    dtype='float32')\n",
        "      \n",
        "      #input to decoder   <s> hello world\n",
        "      #target of decoder: hello world <s>\n",
        "      \n",
        "      for i, input_text in enumerate(input_texts):\n",
        "        \n",
        "        # out : hello  world  <end>  (MAX_SEQUENCE_LENGTH=2)\n",
        "        #\n",
        "        # in: : <s>   hello   world\n",
        "          \n",
        "        for t, word in enumerate(('<s>'+input_text)[:MAX_SEQUENCE_LENGTH]):\n",
        "            one_hot = word2index['<oov>'] if word not in word2index else word2index[word]\n",
        "            encoder_input_data[i, t ] = one_hot\n",
        "            \n",
        "        for t,word in enumerate(input_text[:MAX_SEQUENCE_LENGTH-1]):  #last must be <end>=<s> token\n",
        "            decoder_target_data[i, t, one_hot] = 1. \n",
        "      #x = pad_sequences(self.tokenizer.texts_to_sequences([x for x,y in rows]), maxlen=MAX_SEQUENCE_LENGTH)\n",
        "      #y = pad_sequences(self.tokenizer.texts_to_sequences([y for x,y in rows]), maxlen=MAX_SEQUENCE_LENGTH)\n",
        "      #y = to_categorical(y, num_classes=num_encoder_tokens)\n",
        " \n",
        "      result.append( (encoder_input_data,decoder_target_data))\n",
        "\n",
        "    self.result= TVT(*result)\n",
        "\n",
        " \n",
        "\n",
        "cache_dir='cache' \n",
        "#dataset('quora_dups','http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv',False,cache_dir) \n",
        "#dataset('bible4','https://codeload.github.com/keithecarlson/Zero-Shot-Style-Transfer/zip/master',extract=True,cache_dir=cache_dir\n",
        "#       pattern=('Zero-Shot-Style-Transfer-master/Data/Bibles/ASV/*/*.txt','Zero-Shot-Style-Transfer-master/Data/Bibles/BBE/*/*.txt')\n",
        "\n",
        "dataset = Dataset('bible_csv','https://codeload.github.com/ashual/style-transfer/zip/master',extract=True,cache_dir=cache_dir,pattern='style-transfer-master/datasets/bible-corpus/t_a*.csv')        \n",
        "row_parser= lambda line: (line.split(',')[4],line.split(',')[4]) #map x to x\n",
        "dataset.parse(row_parser,skip_first=True)\n",
        "dataset.tokenize()        \n",
        "x_train, y_train = dataset.result.train\n",
        "x_val,y_val = dataset.result.val\n",
        "x_test,y_test = dataset.result.test\n",
        "\n",
        "print ('train',x_train.shape,y_train.shape)\n",
        "print('val',x_val.shape,y_val.shape)\n",
        "print ('train in MB x,y',x_train.nbytes/1e6,y_train.nbytes/1e6)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fpath cache/datasets/bible_csv\n",
            "['cache/datasets/style-transfer-master/datasets/bible-corpus/t_asv.csv'] #lines 31104 first 3 lines\n",
            "id,b,c,v,t \n",
            " 1001001,1,1,1,In the beginning God created the heavens and the earth. \n",
            " 1001002,1,1,2,And the earth was waste and void; and darkness was upon the face of the deep: and the Spirit of God moved upon the face of the waters.\n",
            "train: 24883 val 3110 test 3111\n",
            "\n",
            "row_parser train: ('In the beginning God created the heavens and the earth.', 'In the beginning God created the heavens and the earth.') test: ('\"Much every way: first of all', '\"Much every way: first of all')\n",
            "limiting num_words in Tokenizer due to MEMORY BOUNDS\n",
            "\n",
            "only tokenizing x[0], not y!!! using all words, pretty bad tokenizer!!!\n",
            "<class 'list'> In the beginning God created the heavens and the earth.\n",
            "\n",
            " word_index 13075 <oov> 13075\n",
            "common [('the', 1), ('of', 2), ('\"And', 3), ('and', 4), ('to', 5), ('unto', 6), ('in', 7), ('shall', 8), ('that', 9), ('he', 10), ('a', 11), ('his', 12), ('Jehovah', 13), ('they', 14), ('I', 15)]\n",
            "uncommon [('marvelled.', 13061), (\"do'\", 13062), ('purple', 13063), ('myrrh:', 13064), ('superscription', 13065), ('robbers;', 13066), ('scripture', 13067), ('bottom.', 13068), ('afar:', 13069), ('Arimathaea', 13070), ('already', 13071), ('dead:', 13072), (\"`mother'\", 13073), ('Joses', 13074), ('<oov>', 13075)]\n",
            "keeping only  666 of 13075\n",
            "train (24882, 20) (24882, 20, 666)\n",
            "val (3109, 20) (3109, 20, 666)\n",
            "train in MB x,y 1.99056 1325.71296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZYagFZih8C5m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "rOWiRe-VuXZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "04c9c613-f42d-470b-858b-98cc55ef80ae"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(24882, 20) (3109, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iYh4oeQlABcR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "outputId": "4ddca3fe-033d-4ff4-8c18-ca649a175c4c"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding\n",
        "\n",
        "# size of tokenizer indexes\n",
        "num_decoder_tokens = num_encoder_tokens = 666 #hard-coded for now\n",
        "\n",
        "embedding_dim=300\n",
        "latent_dim = 256\n",
        "batch_size=64\n",
        "epochs=1\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "\n",
        "shared_embedding = Embedding(num_encoder_tokens, \n",
        "                     embedding_dim, \n",
        "                     #weights=[word_embedding_matrix], if there is one\n",
        "                     #trainable=False,                            \n",
        "                     #input_length=MAX_SEQUENCE_LENGTH, if there is one\n",
        "                     )\n",
        "x = shared_embedding(encoder_inputs) \n",
        "encoder_lstm=LSTM(latent_dim, return_state=True)\n",
        "x, state_h, state_c = encoder_lstm(x)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "x = shared_embedding(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True)\n",
        "x = decoder_lstm(x, initial_state=encoder_states)\n",
        "decoder_outputs = Dense(num_decoder_tokens, activation='softmax')(x)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile & run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "print (model.summary())\n",
        "model.fit([x_train, x_train], y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=([x_train, x_train], y_train))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_14 (InputLayer)           (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_13 (InputLayer)           (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, None, 300)    199800      input_13[0][0]                   \n",
            "                                                                 input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_9 (LSTM)                   [(None, 256), (None, 570368      embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_10 (LSTM)                  (None, None, 256)    570368      embedding_5[1][0]                \n",
            "                                                                 lstm_9[0][1]                     \n",
            "                                                                 lstm_9[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, None, 666)    171162      lstm_10[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,511,698\n",
            "Trainable params: 1,511,698\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Train on 24882 samples, validate on 24882 samples\n",
            "Epoch 1/1\n",
            " 2880/24882 [==>...........................] - ETA: 2:57 - loss: 0.5807"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "24882/24882 [==============================] - 231s 9ms/step - loss: 0.1324 - val_loss: 0.0145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbe17936f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "C2cufjsW_9Zw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "_ZC94QpK1bcw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_0G8_Dr2bMM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R_OBmX182djj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I2NGxSQb2dxD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TkFXxinr0fNo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "ff01515c-fb9f-4e3e-e7a0-431e37ec3519"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datadir /content/.keras/datasets\n",
            "archive.extractall(path) <zipfile.ZipFile filename='/content/.keras/datasets/bible' mode='r'> /content/.keras/datasets\n",
            "zip done\n",
            "total 12M\n",
            "-rw-r--r-- 1 root root 12M Jul  5 10:06 bible\n",
            "total 4.0K\n",
            "drwxr-xr-x 3 root root 4.0K Jul  2 16:56 datalab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "beDWv50D8MYj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
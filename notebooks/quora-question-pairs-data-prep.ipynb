{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quora-question-pairs-data-prep.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-5wzZxtNpvYr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Quora question pairs: data preparation"
      ]
    },
    {
      "metadata": {
        "id": "kWZDg8XnwARX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "initial code. looks clean and tidy. \n",
        "accuracy results low.\n",
        "\n",
        "imporevents:\n",
        "*dense layer and not LSTM!!!!\n",
        "* encoder not shared (two different )\n",
        "* classification head'\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "NYYm9ELXpvYs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import packages"
      ]
    },
    {
      "metadata": {
        "id": "1VPo7wSHpvYt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8ea51420-11ef-4158-9a99-b7cecb6064e2"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import csv, json\n",
        "from zipfile import ZipFile\n",
        "from os.path import expanduser, exists\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.data_utils import get_file\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-gP3t_dZpvYy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initialize global variables"
      ]
    },
    {
      "metadata": {
        "id": "Ex9Du58spvYz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ee7901c0-8d58-4e40-f10d-cee06a401d82"
      },
      "cell_type": "code",
      "source": [
        "KERAS_DATASETS_DIR = expanduser('~/.keras/datasets/')\n",
        "QUESTION_PAIRS_FILE_URL = 'http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv'\n",
        "QUESTION_PAIRS_FILE = 'quora_duplicate_questions.tsv'\n",
        "GLOVE_ZIP_FILE_URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
        "GLOVE_ZIP_FILE = 'glove.840B.300d.zip'\n",
        "GLOVE_FILE = 'glove.840B.300d.txt'\n",
        "Q1_TRAINING_DATA_FILE = 'q1_train.npy'\n",
        "Q2_TRAINING_DATA_FILE = 'q2_train.npy'\n",
        "LABEL_TRAINING_DATA_FILE = 'label_train.npy'\n",
        "WORD_EMBEDDING_MATRIX_FILE = 'word_embedding_matrix.npy'\n",
        "NB_WORDS_DATA_FILE = 'nb_words.json'\n",
        "MAX_NB_WORDS = 200000\n",
        "MAX_SEQUENCE_LENGTH = 25\n",
        "EMBEDDING_DIM = 300"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PNvqCclEpvY3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download and extract questions pairs data"
      ]
    },
    {
      "metadata": {
        "id": "cv_kfmuCpvY4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "23bfe88b-cc44-4438-8477-2d70c777fd64"
      },
      "cell_type": "code",
      "source": [
        "if not exists(KERAS_DATASETS_DIR + QUESTION_PAIRS_FILE):\n",
        "    get_file(QUESTION_PAIRS_FILE, QUESTION_PAIRS_FILE_URL)\n",
        "\n",
        "print(\"Processing\", QUESTION_PAIRS_FILE)\n",
        "\n",
        "question1 = []\n",
        "question2 = []\n",
        "is_duplicate = []\n",
        "with open(KERAS_DATASETS_DIR + QUESTION_PAIRS_FILE, encoding='utf-8') as csvfile:\n",
        "    reader = csv.DictReader(csvfile, delimiter='\\t')\n",
        "    for row in reader:\n",
        "        question1.append(row['question1'])\n",
        "        question2.append(row['question2'])\n",
        "        is_duplicate.append(row['is_duplicate'])\n",
        "\n",
        "print('Question pairs: %d' % len(question1))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv\n",
            "58179584/58176133 [==============================] - 0s 0us/step\n",
            "Processing quora_duplicate_questions.tsv\n",
            "Question pairs: 404290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "25rcv1dSpvY8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build tokenized word index"
      ]
    },
    {
      "metadata": {
        "id": "7Y2M2_1fpvY9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "def493e9-deda-4a63-cb9e-666c97d6fa13"
      },
      "cell_type": "code",
      "source": [
        "questions = question1 + question2\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(questions)\n",
        "question1_word_sequences = tokenizer.texts_to_sequences(question1)\n",
        "question2_word_sequences = tokenizer.texts_to_sequences(question2)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print(\"Words in index: %d\" % len(word_index))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words in index: 95596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G3Pe0BqlpvZA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download and process GloVe embeddings"
      ]
    },
    {
      "metadata": {
        "id": "UcJkzvpGpvZB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "0a59fa6c-5100-4ec2-d44a-93d87e7ce8f9"
      },
      "cell_type": "code",
      "source": [
        "if not exists(KERAS_DATASETS_DIR + GLOVE_ZIP_FILE):\n",
        "    zipfile = ZipFile(get_file(GLOVE_ZIP_FILE, GLOVE_ZIP_FILE_URL))\n",
        "    zipfile.extract(GLOVE_FILE, path=KERAS_DATASETS_DIR)\n",
        "    \n",
        "print(\"Processing\", GLOVE_FILE)\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(KERAS_DATASETS_DIR + GLOVE_FILE, encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings: %d' % len(embeddings_index))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "1059217408/2176768927 [=============>................] - ETA: 59s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2176770048/2176768927 [==============================] - 115s 0us/step\n",
            "Processing glove.840B.300d.txt\n",
            "Word embeddings: 2196016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xsdZkTd_pvZI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare word embedding matrix"
      ]
    },
    {
      "metadata": {
        "id": "AgsYUai0pvZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9fbed4f6-e94b-4fd0-c135-7a7e61935886"
      },
      "cell_type": "code",
      "source": [
        "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
        "word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NB_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        word_embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Null word embeddings: 29276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0eSMu4x2pvZO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare training data tensors"
      ]
    },
    {
      "metadata": {
        "id": "VZRLTCoGpvZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d62cbb8f-ae4f-43da-8836-ed48f5ef4acc"
      },
      "cell_type": "code",
      "source": [
        "q1_data = pad_sequences(question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "q2_data = pad_sequences(question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels = np.array(is_duplicate, dtype=int)\n",
        "print('Shape of question1 data tensor:', q1_data.shape)\n",
        "print('Shape of question2 data tensor:', q2_data.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of question1 data tensor: (404290, 25)\n",
            "Shape of question2 data tensor: (404290, 25)\n",
            "Shape of label tensor: (404290,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lBs_tgivpvZU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Persist training and configuration data to files"
      ]
    },
    {
      "metadata": {
        "id": "P7IRDRq2pvZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6a2a49d4-f742-4598-ddaa-2910921724cf"
      },
      "cell_type": "code",
      "source": [
        "np.save(open(Q1_TRAINING_DATA_FILE, 'wb'), q1_data)\n",
        "np.save(open(Q2_TRAINING_DATA_FILE, 'wb'), q2_data)\n",
        "np.save(open(LABEL_TRAINING_DATA_FILE, 'wb'), labels)\n",
        "np.save(open(WORD_EMBEDDING_MATRIX_FILE, 'wb'), word_embedding_matrix)\n",
        "with open(NB_WORDS_DATA_FILE, 'w') as f:\n",
        "    json.dump({'nb_words': nb_words}, f)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L5dRefMYsASI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "8cfc8420-c361-44c6-a889-343fa56880c1"
      },
      "cell_type": "code",
      "source": [
        "!ls -lh"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 300M\r\n",
            "drwxr-xr-x 3 root root 4.0K Jul  2 16:56 datalab\r\n",
            "-rw-r--r-- 1 root root 3.1M Jul  4 09:44 label_train.npy\r\n",
            "-rw-r--r-- 1 root root   19 Jul  4 09:44 nb_words.json\r\n",
            "-rw-r--r-- 1 root root  39M Jul  4 09:44 q1_train.npy\r\n",
            "-rw-r--r-- 1 root root  39M Jul  4 09:44 q2_train.npy\r\n",
            "-rw-r--r-- 1 root root 219M Jul  4 09:44 word_embedding_matrix.npy\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gezIPbGssNLu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Emf3a2rsNOk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7bz2GGppsNR0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4KfPjhrRp1WK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "96fb0042-5d94-45a9-fb56-18347f73cb1a"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime, time, json\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, TimeDistributed, Dense, Lambda, concatenate, Dropout, BatchNormalization\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UA6no2Rxp1WP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initialize global variables"
      ]
    },
    {
      "metadata": {
        "id": "ozz2ixJ4p1WQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d5b50c35-a85d-4221-8a02-273835449543"
      },
      "cell_type": "code",
      "source": [
        "Q1_TRAINING_DATA_FILE = 'q1_train.npy'\n",
        "Q2_TRAINING_DATA_FILE = 'q2_train.npy'\n",
        "LABEL_TRAINING_DATA_FILE = 'label_train.npy'\n",
        "WORD_EMBEDDING_MATRIX_FILE = 'word_embedding_matrix.npy'\n",
        "NB_WORDS_DATA_FILE = 'nb_words.json'\n",
        "MODEL_WEIGHTS_FILE = 'question_pairs_weights.h5'\n",
        "MAX_SEQUENCE_LENGTH = 25\n",
        "EMBEDDING_DIM = 300\n",
        "VALIDATION_SPLIT = 0.1\n",
        "TEST_SPLIT = 0.1\n",
        "RNG_SEED = 13371447\n",
        "NB_EPOCHS = 25\n",
        "DROPOUT = 0.1\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tbaQimg4p1WU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the dataset, embedding matrix and word count"
      ]
    },
    {
      "metadata": {
        "id": "IHTzrkxyp1WV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "355b4f09-6a99-49c2-c3ce-7d4d111156a8"
      },
      "cell_type": "code",
      "source": [
        "q1_data = np.load(open(Q1_TRAINING_DATA_FILE, 'rb'))\n",
        "q2_data = np.load(open(Q2_TRAINING_DATA_FILE, 'rb'))\n",
        "labels = np.load(open(LABEL_TRAINING_DATA_FILE, 'rb'))\n",
        "word_embedding_matrix = np.load(open(WORD_EMBEDDING_MATRIX_FILE, 'rb'))\n",
        "with open(NB_WORDS_DATA_FILE, 'r') as f:\n",
        "    nb_words = json.load(f)['nb_words']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Owg8BnpPp1WZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Partition the dataset into train and test sets"
      ]
    },
    {
      "metadata": {
        "id": "K41KLHlDp1WZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b7bd1384-b9d2-4a61-c7a0-cba40dc48fd4"
      },
      "cell_type": "code",
      "source": [
        "X = np.stack((q1_data, q2_data), axis=1)\n",
        "y = labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
        "Q1_train = X_train[:,0]\n",
        "Q2_train = X_train[:,1]\n",
        "Q1_test = X_test[:,0]\n",
        "Q2_test = X_test[:,1]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ciGIzdbBp1Wd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the model"
      ]
    },
    {
      "metadata": {
        "id": "UZlsbLb_p1Wd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "60624bdd-a5eb-4ecd-d7af-f0e64d6bcbac"
      },
      "cell_type": "code",
      "source": [
        "def dense_model():\n",
        "  question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "  question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "\n",
        "  q1 = Embedding(nb_words + 1, \n",
        "                   EMBEDDING_DIM, \n",
        "                   weights=[word_embedding_matrix], \n",
        "                   input_length=MAX_SEQUENCE_LENGTH, \n",
        "                   trainable=False)(question1)\n",
        "  q1 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q1)\n",
        "  q1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q1)\n",
        "\n",
        "  q2 = Embedding(nb_words + 1, \n",
        "                   EMBEDDING_DIM, \n",
        "                   weights=[word_embedding_matrix], \n",
        "                   input_length=MAX_SEQUENCE_LENGTH, \n",
        "                   trainable=False)(question2)\n",
        "  q2 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q2)\n",
        "  q2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q2)\n",
        "\n",
        "  merged = concatenate([q1,q2])\n",
        "  merged = Dense(200, activation='relu')(merged)\n",
        "  merged = Dropout(DROPOUT)(merged)\n",
        "  is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
        "\n",
        "  model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "model = dense_model()\n",
        "model.summary()\n",
        "\n",
        "  "
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_25 (InputLayer)           (None, 25)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_26 (InputLayer)           (None, 25)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_21 (Embedding)        (None, 25, 300)      28679100    input_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_22 (Embedding)        (None, 25, 300)      28679100    input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_15 (TimeDistri (None, 25, 300)      90300       embedding_21[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_16 (TimeDistri (None, 25, 300)      90300       embedding_22[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_15 (Lambda)              (None, 300)          0           time_distributed_15[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "lambda_16 (Lambda)              (None, 300)          0           time_distributed_16[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 600)          0           lambda_15[0][0]                  \n",
            "                                                                 lambda_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_37 (Dense)                (None, 200)          120200      concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 200)          0           dense_37[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_38 (Dense)                (None, 1)            201         dropout_14[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 57,659,201\n",
            "Trainable params: 301,001\n",
            "Non-trainable params: 57,358,200\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cbd7tW23IkyZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "11f66501-4432-477f-9fb0-c54047338c17"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, LSTM, Merge,merge,CuDNNLSTM\n",
        "\n",
        "#see: https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07\n",
        "def siamise_model(pretrained_emb=True):\n",
        "  question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "  question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "\n",
        "  if pretrained_emb:\n",
        "    shared_embedding = Embedding(nb_words + 1, \n",
        "                     EMBEDDING_DIM, \n",
        "                     weights=[word_embedding_matrix], \n",
        "                     input_length=MAX_SEQUENCE_LENGTH, \n",
        "                     trainable=False)\n",
        "  else:\n",
        "    shared_embedding = Embedding(nb_words + 1, \n",
        "                     EMBEDDING_DIM, \n",
        "                     #weights=[word_embedding_matrix], \n",
        "                     input_length=MAX_SEQUENCE_LENGTH, \n",
        "                     trainable=True)\n",
        "      \n",
        "  \n",
        "  \n",
        "  \n",
        "  shared_lstm = CuDNNLSTM(200) #LSTM(200)\n",
        "\n",
        "  left_output =  shared_lstm(shared_embedding(question1))\n",
        "  right_output = shared_lstm(shared_embedding(question2))\n",
        "\n",
        "  def exponent_neg_manhattan_distance(left, right):\n",
        "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "  \n",
        "  # Calculates the distance as defined by the MaLSTM model\n",
        "  malstm_distance = Merge(mode=lambda x: exponent_neg_manhattan_distance(x[0], x[1]), output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "\n",
        "  # Pack it all up into a model\n",
        "  malstm = Model([question1, question2], [malstm_distance])\n",
        "  malstm.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "  return malstm\n",
        "\n",
        "model = siamise_model(pretrained_emb=False)\n",
        "#model = siamise_model(pretrained_emb=True) #0.82 val\n",
        "model.summary()  "
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_43 (InputLayer)           (None, 25)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_44 (InputLayer)           (None, 25)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_33 (Embedding)        (None, 25, 300)      28679100    input_43[0][0]                   \n",
            "                                                                 input_44[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_5 (CuDNNLSTM)        (None, 200)          401600      embedding_33[0][0]               \n",
            "                                                                 embedding_33[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "merge_12 (Merge)                (None, 1)            0           cu_dnnlstm_5[0][0]               \n",
            "                                                                 cu_dnnlstm_5[1][0]               \n",
            "==================================================================================================\n",
            "Total params: 29,080,700\n",
            "Trainable params: 29,080,700\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "DsJ_jgbap1Wl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model, checkpointing weights with best validation accuracy"
      ]
    },
    {
      "metadata": {
        "id": "OMfzV9tsp1Wn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "9c1b0739-ad6a-4927-97cb-77640f26b82a"
      },
      "cell_type": "code",
      "source": [
        "print (K.eval(model.optimizer.lr))\n",
        "#model.optimizer.lr = model.optimizer.lr/10\n",
        "#print ('model.optimizer.lr',model.optimizer.lr)\n",
        "\n",
        "print(\"Starting training at\", datetime.datetime.now())\n",
        "t0 = time.time()\n",
        "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_acc', save_best_only=True)]\n",
        "S= int(len(Q1_train)/10)\n",
        "print (f'training on a subset of size {S}')\n",
        "history = model.fit([Q1_train[:S], Q2_train[:S]],\n",
        "                    y_train[:S],\n",
        "                    epochs=NB_EPOCHS,\n",
        "                    validation_split=VALIDATION_SPLIT,\n",
        "                    verbose=2,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    callbacks=callbacks)\n",
        "t1 = time.time()\n",
        "print(\"Training ended at\", datetime.datetime.now())\n",
        "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-64b2fb4f06cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#model.optimizer.lr = model.optimizer.lr/10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print ('model.optimizer.lr',model.optimizer.lr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training at\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'K' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "qr1tcf64p1Wt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Plot training and validation accuracy"
      ]
    },
    {
      "metadata": {
        "id": "XQ0wMapYp1Wt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1282
        },
        "outputId": "60d162a3-1308-4294-c004-502b92be856d"
      },
      "cell_type": "code",
      "source": [
        "acc = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n",
        "                    'training': history.history['acc'],\n",
        "                    'validation': history.history['val_acc']})\n",
        "ax = acc.iloc[:,:].plot(x='epoch', figsize={5,8}, grid=True)\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "ax.set_ylim([0.0,1.0]);"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-9440295b9b0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0;34m'training'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     'validation': history.history['val_acc']})\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[1;32m   2675\u001b[0m                           \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolormap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                           \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecondary_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2677\u001b[0;31m                           sort_columns=sort_columns, **kwds)\n\u001b[0m\u001b[1;32m   2678\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36mplot_frame\u001b[0;34m(data, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[1;32m   1900\u001b[0m                  \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m                  \u001b[0msecondary_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m                  **kwds)\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_plot\u001b[0;34m(data, x, y, subplots, ax, kind, **kwds)\u001b[0m\n\u001b[1;32m   1727\u001b[0m         \u001b[0mplot_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1729\u001b[0;31m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1730\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_plot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_subplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_setup_subplots\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0max\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mfigure\u001b[0;34m(num, figsize, dpi, facecolor, edgecolor, frameon, FigureClass, clear, **kwargs)\u001b[0m\n\u001b[1;32m    537\u001b[0m                                         \u001b[0mframeon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m                                         \u001b[0mFigureClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFigureClass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m                                         **kwargs)\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfigLabel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mnew_figure_manager\u001b[0;34m(cls, num, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mfig_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FigureClass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_figure_manager_given_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, figsize, dpi, facecolor, edgecolor, linewidth, frameon, subplotpars, tight_layout)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mframeon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.frameon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             raise ValueError('figure size must be finite not '\n\u001b[1;32m    337\u001b[0m                              '{}'.format(figsize))\n",
            "\u001b[0;31mTypeError\u001b[0m: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "A5oXNQaYp1Wx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Print best validation accuracy and epoch"
      ]
    },
    {
      "metadata": {
        "id": "28HpgHa-p1Wy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "50c471eb-46bc-4846-f5fd-fb4f8f961af6"
      },
      "cell_type": "code",
      "source": [
        "max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(history.history['val_acc']))\n",
        "print('Maximum accuracy at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(max_val_acc))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum accuracy at epoch 16 = 0.8150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aqBkTkNTp1W3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model with best validation accuracy on the test partition"
      ]
    },
    {
      "metadata": {
        "id": "-1kp_Mztp1W4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fbe627cc-9c3e-4f92-e4b3-b1d8e5af44bc"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights(MODEL_WEIGHTS_FILE)\n",
        "loss, accuracy = model.evaluate([Q1_test, Q2_test], y_test, verbose=0)\n",
        "print('loss = {0:.4f}, accuracy = {1:.4f}'.format(loss, accuracy))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss = 0.5435, accuracy = 0.8137\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
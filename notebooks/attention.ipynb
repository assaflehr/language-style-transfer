{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/assaflehr/language-style-transfer/blob/master/notebooks/attention.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "F7OHsnTjaMma",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Attention for humans and TF/Keras\n",
        "Just another notebook on attention, hopefully simplified one.\n",
        "\n",
        "\n",
        "We will go into details of attention but use a toy problem with invented numbers to first get the intuition. Then we will get into code and then, if you really want, you can look back to the equations.\n",
        "\n",
        "I do assume you know about seq2seq encoder-decoder models (if not, please read about it now)\n",
        "\n",
        "## \"The toy problem: number to text \n",
        "input: a string representing a number , like '42.5' , '-12001''\n",
        "output: a text description like 'forty-two point five' , 'minus twelve thousand and one'\n",
        "\n",
        "### dataset\n",
        "we will generate one using num2words python library.\n",
        "\n",
        "### preprocessing \n",
        "we assume a one-hot-encoding input embedding, for the each **character** 0..9,-,''.'' \n",
        "\n",
        "we will use a one-hot-encoding output embedding, for each **word** '\"one\",\"two\",...\"thosand\",\"minus\",...\n",
        "\n",
        "## architecture\n",
        "Encoder-Decoder with attention(ofcourse).\n",
        "\n",
        "\n",
        "Keep this image open and then we will show example:\n",
        "First open this great figure from [distill.pub blog](https://distill.pub/2016/augmented-rnns/) \n",
        "![image-from-distill](https://distill.pub/2016/augmented-rnns/assets/rnn_attentional_02.svg) from distill.pub blog. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "syK1wLGuf1xz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To make our life simpler, let's only support 4 characters  1,2,.,<e> in the embeddings and use one-hot-encoding.\n",
        "Imagine we just finished running the encoding step on \"12.1<e>...<e>\" (padding with end-sequence token) All RNN encoders have two possible inputs: output (o-dimentation) and hidden-state(h-dimension) after each seen characters.\n",
        "\n",
        "* In attention-less architecture,  we ignore all but the last hidden-state\n",
        "\n",
        "* With attention, we use all the per-char outputs and the last hidden-state and ignore other hidden-states.\n",
        "\n",
        "We configured encoder output dimention to be 10d. Their value after training correspand nicely to: 0..3 values are the input embedding, and values 4..9 correspond to the digit number, again to make it simple for us, the human viewers we use one hot-encoding. actual values will surely be different and much more dense. so for '12.1<e>...' we get:\n",
        "\n",
        "* '1'= 1,0,0,0,**0,0,0,0,0,1**\n",
        "* '2'= 0,1,0,0,**0,0,0,0,1,0**\n",
        "* '. ' =  0,0,1,0,**0,0,0,1,0,0**\n",
        "* '1'= 1,0,0,0,**0,0,1,0,0,0** (the left part is thes same as the1)\n",
        "* 'e' = 0,0,0,1,**0,1,0,0,0,0**\n",
        "* 'e' = 0,0,0,1,**1,0,0,0,0,0**\n",
        "  \n",
        "The hidden-state is typically of different dimention (not 4), let's say 5 and should include (intuitevly):\n",
        "* info about the total number of digits, which will help the decoder to know how to process the first digit: 11 elevan, VS 112 one-hundred and two.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "lQixymxhsiBn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Attention\n",
        "We will talk about few variants of attentions. We will start with toy attention, and then move to real usefull ones.\n",
        "\n",
        "The decoder size start with the encoder last hidden-state. This value is used as a query into all the encoder outputs to choose which 1/few to look at. \n",
        "\n",
        "#### (1) toy-attention, with few problems\n",
        "\n",
        "query 1: look with 100% on the first encoder-output.  Then do some logic either write right away \"twenty\"/\"thirty\" or if it was '1', remember that, don't output anything and wait for the next char.\n",
        "query 2: look with 100% on the second encoder-output... \n",
        "\n",
        "How can we achieve that? with a 1st query like '0,0,0,0,**0,0,0,0,0,1** which will only return non-zero for the first character. \n",
        "\n",
        "Actual code will do dot-product of outputs(6x10)*query(10x1)=(6x1) weights which will be zero except the last.\n",
        "\n",
        "Problem: Most translation systems do not accept \"holes\" or \"spaces\" in translation. In our case, we sometimes skip the first word (case of starting with 1).\n",
        "\n",
        "**Solution: **\n",
        "\n",
        "query 1 will look at the first two characters and decide whether to decode a one word for both (\"12\"-> \"twelve\") or one for the first only (\"2x\"-> \"twenty\"). The hidden-state will remember which chars were already fully processed.\n",
        "\n",
        "query: if 1st char=='1' pass 1st and 2nd. Else pass only 1st.\n",
        "This requires more than just vector dot-product.\n",
        "can be achieved with query x W_matrix x outputs. \n",
        "\n",
        "\n",
        "### ** real solutions**\n",
        "In the real attention mechansims, there are few steps. note that there are a dozen of flavors of attention: \"monotonic_attention\", \"BahdanauMonotonicAttention\", \"LuongAttention\" etc etc. They change the equations a bit.\n",
        "* read context-vector from memory of the encoder-outputs:\n",
        "  * find attention weights for each encoder-output.  can be done like we did in the toy, where it is 0 or 1,  or in two other popular ways:  query x W x outputs and *Bahdanau attention*:  FC(tanh(FC(ExO) + FC(H))) . In both we also apply \n",
        "  * then apply softmax , to normlize the result into prob.\n",
        "  * then do weighted-sum of the memory vecotrs.\n",
        "  * now we have a context-vector of the size of the output.\n",
        "* (optional) build attention-vector as combination of \"context vector\" with the current target hidden state. (a = tanh(Wxconcat(c,h), we added tanh and W for some learning). \n",
        "\n",
        "* Create big-decoder-input as concatination of the context-vector and the regular input-vector (embedding applied on X).(start with no-input in first decoding, but then input is the previous output). It can be combined \n",
        "\n",
        "* run the RNN (GRU) unit on this big-decoder-input, to get a list of hidden-states and outputs. Here we don't need the hidden-states. To get the actual output, we apply a dense layer on each RNN output\n",
        "  \n",
        "* should attention query is of h_t or h_t-1\n",
        "* ???? how to move from attnetion reuslt to output\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PqwHnNjoaQbI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "90a63d4a-2a50-47df-ea0a-ee10f094cd69"
      },
      "cell_type": "code",
      "source": [
        "!pip install num2words\n",
        "from num2words import num2words\n",
        "for n in [11.2,42.5, -12001]:\n",
        "  print (n,num2words(n))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting num2words\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/6e/6d026d15d1b0fd37a9dd42ecf559f36871cee67158aff5ba652d3130e8b9/num2words-0.5.6-py2.py3-none-any.whl (64kB)\n",
            "\u001b[K    100% |████████████████████████████████| 71kB 1.8MB/s \n",
            "\u001b[?25hInstalling collected packages: num2words\n",
            "Successfully installed num2words-0.5.6\n",
            "11.2 eleven point two\n",
            "42.5 forty-two point five\n",
            "-12001 minus twelve thousand and one\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R6he3nCBv0cQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "86da5718-3a3a-4f75-df63-242d5b912ecf"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "in_to_out=[('1', [1,0,0,0,0,0,0,0,0,1]),\n",
        "('2', [0,1,0,0,0,0,0,0,1,0]),\n",
        "('. ',[0,0,1,0,0,0,0,1,0,0]),\n",
        "('1', [1,0,0,0,0,0,1,0,0,0]), #(the left part is thes same as the1)\n",
        "('e', [0,0,0,1,0,1,0,0,0,0]),\n",
        "('e', [0,0,0,1,1,0,0,0,0,0])]\n",
        "outputs=np.array(list(zip(*in_to_out))[1]).T\n",
        "print ('toy attention 1 - looking only at first character')\n",
        "print ('outputs',outputs.shape)\n",
        "\n",
        "query= np.array([[0,0,0,0,0,0,0,0,1,1]])\n",
        "print ('query1',query.shape)\n",
        "\n",
        "result= query@outputs\n",
        "print (f'query @ outputs  {query.shape} x {outputs.shape} =',result.shape,result)\n",
        "\n",
        "#print ('toy attention 2 - looking into two first')\n",
        "#query= np.array([[0,0,0,0,0,0,0,0,1,1]])\n",
        "#W = np.eye(10)\n",
        "#W[:,:]= 4 \n",
        "#result = query @ W @ outputs\n",
        "#print ('query @ W @ outputs',result.shape, result)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "toy attention 1 - looking only at first character\n",
            "outputs (10, 6)\n",
            "query1 (1, 10)\n",
            "query @ outputs  (1, 10) x (10, 6) = (1, 6) [[1 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5gLjEK3tXAQv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3k_1wxeKYepQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Code\n",
        "see keras/eager-tf in : \n",
        "https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\n",
        "\n",
        "see older and more detailed version in:\n",
        "https://www.tensorflow.org/tutorials/seq2seq#background_on_the_attention_mechanism\n"
      ]
    }
  ]
}
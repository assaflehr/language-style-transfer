{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ST_EXP1_sentence_autoendcoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/assaflehr/language-style-transfer/blob/master/notebooks/ST_EXP1_sentence_autoendcoder.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "wAzuDhY3rgGS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Exp1 report:\n",
        "\n",
        "First check it's working at all (use 160 examples) with short-sentences(<12 words).  Took few hours to find initial working model. encoder-decoder based on simple (one-directional) CudaGRU. a lot of code taken from tf eager [tutorial](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb#scrollTo=cnxC7q-j3jFD)\n",
        "num_examples = 160 , crop_sentence_max_words=12 , BATCH_SIZE = 64 , embedding_dim = 256 , units = 1024 \n",
        "\n",
        "\n",
        "Then increase to 16000 examples (all file) and let it run\n"
      ]
    },
    {
      "metadata": {
        "id": "qmpQCAFADO_b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 16000 #16000  #max = 14716+3679 in current file\n",
        "crop_sentence_max_words=12\n",
        "BATCH_SIZE = 64\n",
        "embedding_dim = 256\n",
        "units = 1024  #hidden size in encoder and decoder (as one pass to the other)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-y-OZBfa3bh8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exp 1 \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "oPkV5zdB55vT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "9bcb0423-7514-4021-c2fc-97bcb8a210a4"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# Import TensorFlow >= 1.9 and enable eager execution\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0e17280f9b52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution\u001b[0;34m(config, device_policy, execution_mode)\u001b[0m\n\u001b[1;32m   5488\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     raise ValueError(\n\u001b[0;32m-> 5490\u001b[0;31m         \"tf.enable_eager_execution must be called at program startup.\")\n\u001b[0m\u001b[1;32m   5491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5492\u001b[0m   \u001b[0;31m# Monkey patch to get rid of an unnecessary conditional since the context is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: tf.enable_eager_execution must be called at program startup."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "wwY54rxG15dg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "c75a8ea9-da69-4c42-a6fc-4994e8d596e9"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(tf.__version__)\n",
        "if  tf.test.gpu_device_name() != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "\n",
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w,crop_sentence_max_words):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w= ' '.join(w.split(' ')[:crop_sentence_max_words])\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    \n",
        "    return w\n",
        " \n",
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return sentence list\n",
        "def create_dataset(path, num_examples,crop_sentence_max_words,verbose=True):\n",
        "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    sentences = [preprocess_sentence(s,crop_sentence_max_words) for s  in lines[:num_examples]]\n",
        "    return sentences\n",
        "  \n",
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "class LanguageIndex():\n",
        "  def __init__(self, lang):\n",
        "    self.lang = lang\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    for phrase in self.lang:\n",
        "      self.vocab.update(phrase.split(' '))\n",
        "    \n",
        "    self.vocab = sorted(self.vocab)\n",
        "    \n",
        "    self.word2idx['<pad>'] = 0\n",
        "    for index, word in enumerate(self.vocab):\n",
        "      self.word2idx[word] = index + 1\n",
        "    \n",
        "    for word, index in self.word2idx.items():\n",
        "      self.idx2word[index] = word\n",
        "      \n",
        "      \n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "def load_dataset(path, num_examples,crop_sentence_max_words=9999,verbose=True):\n",
        "    # creating cleaned input, output pairs\n",
        "    sentences = create_dataset(path, num_examples,crop_sentence_max_words)\n",
        "    if verbose:\n",
        "      print (sentences[:2])\n",
        "\n",
        "    # index language using the class defined above    \n",
        "    targ_lang = LanguageIndex(sentences)\n",
        "    \n",
        "    # Vectorize the input and target languages\n",
        "    \n",
        "    target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en in sentences]\n",
        "    \n",
        "    # Calculate max_length of input and output tensor\n",
        "    # Here, we'll set those to the longest sentence in the dataset\n",
        "    max_length_tar = max_length(target_tensor)\n",
        "      \n",
        "    # Padding the input and output tensor to the maximum length\n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
        "                                                                  maxlen=max_length_tar, \n",
        "                                                                  padding='post')\n",
        "    \n",
        "    return  target_tensor,  targ_lang,  max_length_tar\n",
        "  \n",
        "  # Download the file\n",
        "path_to_file = tf.keras.utils.get_file(\n",
        "    'train.modern.nltktok', origin='https://raw.githubusercontent.com/harsh19/Shakespearizing-Modern-English/master/data/train.modern.nltktok',\n",
        "    extract=False)\n",
        "print (path_to_file)\n",
        "\n",
        "\n",
        "target_tensor,  targ_lang,  max_length_targ = load_dataset(path_to_file, num_examples,crop_sentence_max_words=crop_sentence_max_words)\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(target_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "  \n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val),'max_length_targ',max_length_targ\n",
        "  \n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0-rc1\n",
            "Downloading data from https://raw.githubusercontent.com/harsh19/Shakespearizing-Modern-English/master/data/train.modern.nltktok\n",
            "925696/918924 [==============================] - 0s 0us/step\n",
            "/content/.keras/datasets/train.modern.nltktok\n",
            "['<start> i have half a mind to hit you before you speak again <end>', '<start> but if antony is alive , healthy , friendly with caesar , <end>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12800, 12800, 3200, 3200, 'max_length_targ', 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "jjWiaQeP8Yki",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### model"
      ]
    },
    {
      "metadata": {
        "id": "9xbqO7Iie9bb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "49f98193-8b0d-4260-954f-b9f3c04dbe5e"
      },
      "cell_type": "code",
      "source": [
        "def gru(units):\n",
        "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
        "  # the code automatically does that.\n",
        "  if tf.test.is_gpu_available():\n",
        "    print ('using CuDNNGRU')\n",
        "    return tf.keras.layers.CuDNNGRU(units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "  else:\n",
        "    return tf.keras.layers.GRU(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='sigmoid', \n",
        "                               recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  \n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = gru(self.enc_units)\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        x = self.embedding(x)\n",
        "        outputs, states = self.gru(x, initial_state = hidden)        \n",
        "        return outputs, states\n",
        "    \n",
        "    def get_initial_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))  \n",
        "\n",
        "      \n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = gru(self.dec_units)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, new_hidden = self.gru(x, initial_state = hidden)\n",
        "        \n",
        "        # output shape == (batch_size * max_length, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        # output shape == (batch_size * max_length, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x,new_hidden\n",
        "        \n",
        "    #def get_initial_hidden_state(self):\n",
        "    #    return tf.zeros((self.batch_sz, self.dec_units)) \n",
        "\n",
        "\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "\n",
        "vocab_tar_size = len(targ_lang.word2idx)\n",
        "vocab_inp_size= vocab_tar_size \n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))\n",
        "\n",
        "      \n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)      "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using CuDNNGRU\n",
            "using CuDNNGRU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RqZ5Ey6Su6QG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## EVAL SINGLE"
      ]
    },
    {
      "metadata": {
        "id": "oe9yjmD9u9ph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "78566557-fce9-48a8-b26e-827df3eacf45"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def evaluate_single(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "    #attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    \n",
        "    sentence = preprocess_sentence(sentence,crop_sentence_max_words)\n",
        "\n",
        "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    vis_tensor ('inputs',inputs)\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    vis_tensor ('enc_hidden',enc_hidden)\n",
        "    vis_tensor ('enc_out',enc_out)\n",
        "    \n",
        "    \n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "        \n",
        "        predicted_id = tf.multinomial(tf.exp(predictions), num_samples=1)[0][0].numpy()\n",
        "\n",
        "        result += targ_lang.idx2word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "        \n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence\n",
        "  \n",
        "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
        "    result, sentence = evaluate_single(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
        "        \n",
        "    print('Input:      {}'.format(sentence))\n",
        "    print('Predicted : {}'.format(result))\n",
        "print (vocab_inp_size)   \n",
        "\n",
        "\n",
        "translate(\"I haven't done anything to you .\", encoder, decoder, targ_lang, targ_lang, max_length_targ, max_length_targ)\n",
        "print ('\\n\\n\\n')\n",
        "#translate(\"Good madam , restrain yourself .\", encoder, decoder, targ_lang, targ_lang, max_length_targ, max_length_targ)\n",
        "#translate(\"life is good .\", encoder, decoder, targ_lang, targ_lang, max_length_targ, max_length_targ)\n",
        "translate(\"Good madam , restrain yourself\", encoder, decoder, targ_lang, targ_lang, max_length_targ, max_length_targ)\n",
        "\n",
        "translate(\"Good madam , I haven't done anything to you\", encoder, decoder, targ_lang, targ_lang, max_length_targ, max_length_targ)\n",
        "\n",
        "translate(\"Ho my god, what happened.\", encoder, decoder, targ_lang, targ_lang, max_length_targ, max_length_targ)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6928\n",
            "Input:      <start> i haven t done anything to you . <end>\n",
            "Predicted : i haven t done anything to you . <end> \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Input:      <start> good madam , restrain yourself <end>\n",
            "Predicted : good madam , restrain yourself ! <end> \n",
            "Input:      <start> good madam , i haven t done anything to you <end>\n",
            "Predicted : good gentlemen , i won t do this you deserve <end> \n",
            "Input:      <start> ho my god , what happened . <end>\n",
            "Predicted : as my children , be patient . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hpObfY22IddU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
        "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5. Use *teacher forcing* to decide the next input to the decoder.\n",
        "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "metadata": {
        "id": "ddefjBMa3jF0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5791
        },
        "outputId": "ad692c92-b5d6-45b9-eec2-ac4802690788"
      },
      "cell_type": "code",
      "source": [
        "verbose=True\n",
        "\n",
        "def vis_tensor(name,t):\n",
        "  if verbose:\n",
        "    cnt=5\n",
        "    content = (t[0][:cnt].numpy() if len(t.shape.dims)<=2 else t[0][0][:cnt].numpy())\n",
        "    print (name, t.shape, content),#tf.reduce_max(t).numpy(),tf.reduce_mean(t).numpy(),tf.reduce_min(t).numpy())\n",
        "    print \n",
        "    #print ('\\n')\n",
        "\n",
        "    \n",
        "#print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = 1 - np.equal(real, 0)\n",
        "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "\n",
        "EPOCHS = 200\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.get_initial_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        verbose = False if batch==10 else False\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            vis_tensor('inp',inp)\n",
        "            vis_tensor('hidden',hidden)\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            vis_tensor('enc_output',enc_output)\n",
        "            vis_tensor('enc_hidden',enc_hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            \n",
        "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                vis_tensor('dec_input',dec_input)\n",
        "                vis_tensor('dec_hidden',dec_hidden)\n",
        "                predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "                vis_tensor('predictions',predictions)\n",
        "                vis_tensor('dec_hidden',dec_hidden)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing, TODO: change it to only sometimes run\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "                \n",
        "           \n",
        "        total_loss += (loss / int(targ.shape[1]))\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "      \n",
        "        optimizer.apply_gradients(zip(gradients, variables), tf.train.get_or_create_global_step())\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         loss.numpy() / int(targ.shape[1])))\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss/len(target_tensor)))\n",
        "    translate(\"I haven't done anything to you .\", encoder, decoder, targ_lang, targ_lang, max_length_targ, max_length_targ)\n",
        "    translate(\"We try to see how you are doing\", encoder, decoder, targ_lang, targ_lang, max_length_targ, max_length_targ)\n",
        "\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.0000\n",
            "Epoch 1 Batch 10 Loss 0.0004\n",
            "Epoch 1 Batch 20 Loss 0.0224\n",
            "Epoch 1 Batch 30 Loss 0.0148\n",
            "Epoch 1 Batch 40 Loss 0.0082\n",
            "Epoch 1 Batch 50 Loss 0.0197\n",
            "Epoch 1 Batch 60 Loss 0.0179\n",
            "Epoch 1 Batch 70 Loss 0.0345\n",
            "Epoch 1 Batch 80 Loss 0.0169\n",
            "Epoch 1 Batch 90 Loss 0.0228\n",
            "Epoch 1 Batch 100 Loss 0.0352\n",
            "Epoch 1 Batch 110 Loss 0.0439\n",
            "Epoch 1 Batch 120 Loss 0.0370\n",
            "Epoch 1 Batch 130 Loss 0.0375\n",
            "Epoch 1 Batch 140 Loss 0.0372\n",
            "Epoch 1 Batch 150 Loss 0.0324\n",
            "Epoch 1 Batch 160 Loss 0.0575\n",
            "Epoch 1 Batch 170 Loss 0.0313\n",
            "Epoch 1 Batch 180 Loss 0.0500\n",
            "Epoch 1 Batch 190 Loss 0.0405\n",
            "Epoch 1 Loss 0.0003\n",
            "Input:      <start> i haven t done anything to you . <end>\n",
            "Predicted : i haven t done anything to you . <end> \n",
            "Input:      <start> we try to see how you are doing <end>\n",
            "Predicted : we thought you ought to let us die <end> \n",
            "Time taken for 1 epoch 44.424299001693726 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.0371\n",
            "Epoch 2 Batch 10 Loss 0.0514\n",
            "Epoch 2 Batch 20 Loss 0.0419\n",
            "Epoch 2 Batch 30 Loss 0.0274\n",
            "Epoch 2 Batch 40 Loss 0.0194\n",
            "Epoch 2 Batch 50 Loss 0.0326\n",
            "Epoch 2 Batch 60 Loss 0.0249\n",
            "Epoch 2 Batch 70 Loss 0.0355\n",
            "Epoch 2 Batch 80 Loss 0.0222\n",
            "Epoch 2 Batch 90 Loss 0.0509\n",
            "Epoch 2 Batch 100 Loss 0.0396\n",
            "Epoch 2 Batch 110 Loss 0.0495\n",
            "Epoch 2 Batch 120 Loss 0.0530\n",
            "Epoch 2 Batch 130 Loss 0.0420\n",
            "Epoch 2 Batch 140 Loss 0.0380\n",
            "Epoch 2 Batch 150 Loss 0.0388\n",
            "Epoch 2 Batch 160 Loss 0.0715\n",
            "Epoch 2 Batch 170 Loss 0.1121\n",
            "Epoch 2 Batch 180 Loss 0.0747\n",
            "Epoch 2 Batch 190 Loss 0.0630\n",
            "Epoch 2 Loss 0.0005\n",
            "Input:      <start> i haven t done anything to you . <end>\n",
            "Predicted : i haven t done anything to you . <end> \n",
            "Input:      <start> we try to see how you are doing <end>\n",
            "Predicted : we thought you ought to be patient ! <end> \n",
            "Time taken for 1 epoch 44.30276608467102 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.0492\n",
            "Epoch 3 Batch 10 Loss 0.0339\n",
            "Epoch 3 Batch 20 Loss 0.0332\n",
            "Epoch 3 Batch 30 Loss 0.0274\n",
            "Epoch 3 Batch 40 Loss 0.0324\n",
            "Epoch 3 Batch 50 Loss 0.0263\n",
            "Epoch 3 Batch 60 Loss 0.0240\n",
            "Epoch 3 Batch 70 Loss 0.0424\n",
            "Epoch 3 Batch 80 Loss 0.0371\n",
            "Epoch 3 Batch 90 Loss 0.0406\n",
            "Epoch 3 Batch 100 Loss 0.0399\n",
            "Epoch 3 Batch 110 Loss 0.0383\n",
            "Epoch 3 Batch 120 Loss 0.0509\n",
            "Epoch 3 Batch 130 Loss 0.0523\n",
            "Epoch 3 Batch 140 Loss 0.0356\n",
            "Epoch 3 Batch 150 Loss 0.0536\n",
            "Epoch 3 Batch 160 Loss 0.0434\n",
            "Epoch 3 Batch 170 Loss 0.0306\n",
            "Epoch 3 Batch 180 Loss 0.0443\n",
            "Epoch 3 Batch 190 Loss 0.0551\n",
            "Epoch 3 Loss 0.0005\n",
            "Input:      <start> i haven t done anything to you . <end>\n",
            "Predicted : i haven t done anything to you . <end> \n",
            "Input:      <start> we try to see how you are doing <end>\n",
            "Predicted : we seemed to you see how ! <end> \n",
            "Time taken for 1 epoch 44.110408544540405 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.0435\n",
            "Epoch 4 Batch 10 Loss 0.0267\n",
            "Epoch 4 Batch 20 Loss 0.0270\n",
            "Epoch 4 Batch 30 Loss 0.0307\n",
            "Epoch 4 Batch 40 Loss 0.0256\n",
            "Epoch 4 Batch 50 Loss 0.0298\n",
            "Epoch 4 Batch 60 Loss 0.0162\n",
            "Epoch 4 Batch 70 Loss 0.0387\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4 Batch 80 Loss 0.0205\n",
            "Epoch 4 Batch 90 Loss 0.0327\n",
            "Epoch 4 Batch 100 Loss 0.0352\n",
            "Epoch 4 Batch 110 Loss 0.0227\n",
            "Epoch 4 Batch 120 Loss 0.0419\n",
            "Epoch 4 Batch 130 Loss 0.0375\n",
            "Epoch 4 Batch 140 Loss 0.0327\n",
            "Epoch 4 Batch 150 Loss 0.0332\n",
            "Epoch 4 Batch 160 Loss 0.0399\n",
            "Epoch 4 Batch 170 Loss 0.0218\n",
            "Epoch 4 Batch 180 Loss 0.0525\n",
            "Epoch 4 Batch 190 Loss 0.0229\n",
            "Epoch 4 Loss 0.0004\n",
            "Input:      <start> i haven t done anything to you . <end>\n",
            "Predicted : i haven t done anything to you . <end> \n",
            "Input:      <start> we try to see how you are doing <end>\n",
            "Predicted : we seemed to you see how he ! <end> \n",
            "Time taken for 1 epoch 44.00268769264221 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0539\n",
            "Epoch 5 Batch 10 Loss 0.0188\n",
            "Epoch 5 Batch 20 Loss 0.0362\n",
            "Epoch 5 Batch 30 Loss 0.0324\n",
            "Epoch 5 Batch 40 Loss 0.0260\n",
            "Epoch 5 Batch 50 Loss 0.0140\n",
            "Epoch 5 Batch 60 Loss 0.0280\n",
            "Epoch 5 Batch 70 Loss 0.0232\n",
            "Epoch 5 Batch 80 Loss 0.0417\n",
            "Epoch 5 Batch 90 Loss 0.0325\n",
            "Epoch 5 Batch 100 Loss 0.0402\n",
            "Epoch 5 Batch 110 Loss 0.0376\n",
            "Epoch 5 Batch 120 Loss 0.0199\n",
            "Epoch 5 Batch 130 Loss 0.0280\n",
            "Epoch 5 Batch 140 Loss 0.0370\n",
            "Epoch 5 Batch 150 Loss 0.0247\n",
            "Epoch 5 Batch 160 Loss 0.0364\n",
            "Epoch 5 Batch 170 Loss 0.0402\n",
            "Epoch 5 Batch 180 Loss 0.0507\n",
            "Epoch 5 Batch 190 Loss 0.0361\n",
            "Epoch 5 Loss 0.0004\n",
            "Input:      <start> i haven t done anything to you . <end>\n",
            "Predicted : i haven t done anything to you . <end> \n",
            "Input:      <start> we try to see how you are doing <end>\n",
            "Predicted : we thought you ought to be damned ! <end> \n",
            "Time taken for 1 epoch 43.91047978401184 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0287\n",
            "Epoch 6 Batch 10 Loss 0.0243\n",
            "Epoch 6 Batch 20 Loss 0.0278\n",
            "Epoch 6 Batch 30 Loss 0.0258\n",
            "Epoch 6 Batch 40 Loss 0.0266\n",
            "Epoch 6 Batch 50 Loss 0.0308\n",
            "Epoch 6 Batch 60 Loss 0.0273\n",
            "Epoch 6 Batch 70 Loss 0.0202\n",
            "Epoch 6 Batch 80 Loss 0.0628\n",
            "Epoch 6 Batch 90 Loss 0.0262\n",
            "Epoch 6 Batch 100 Loss 0.0297\n",
            "Epoch 6 Batch 110 Loss 0.0357\n",
            "Epoch 6 Batch 120 Loss 0.0426\n",
            "Epoch 6 Batch 130 Loss 0.0570\n",
            "Epoch 6 Batch 140 Loss 0.0379\n",
            "Epoch 6 Batch 150 Loss 0.0328\n",
            "Epoch 6 Batch 160 Loss 0.0368\n",
            "Epoch 6 Batch 170 Loss 0.0432\n",
            "Epoch 6 Batch 180 Loss 0.0358\n",
            "Epoch 6 Batch 190 Loss 0.0226\n",
            "Epoch 6 Loss 0.0004\n",
            "Input:      <start> i haven t done anything to you . <end>\n",
            "Predicted : i haven t even heard you . <end> \n",
            "Input:      <start> we try to see how you are doing <end>\n",
            "Predicted : we thought you ought to see how ! <end> \n",
            "Time taken for 1 epoch 44.13815450668335 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0306\n",
            "Epoch 7 Batch 10 Loss 0.0383\n",
            "Epoch 7 Batch 20 Loss 0.0515\n",
            "Epoch 7 Batch 30 Loss 0.0237\n",
            "Epoch 7 Batch 40 Loss 0.0203\n",
            "Epoch 7 Batch 50 Loss 0.0095\n",
            "Epoch 7 Batch 60 Loss 0.0170\n",
            "Epoch 7 Batch 70 Loss 0.0208\n",
            "Epoch 7 Batch 80 Loss 0.0212\n",
            "Epoch 7 Batch 90 Loss 0.0294\n",
            "Epoch 7 Batch 100 Loss 0.0255\n",
            "Epoch 7 Batch 110 Loss 0.0276\n",
            "Epoch 7 Batch 120 Loss 0.0178\n",
            "Epoch 7 Batch 130 Loss 0.0133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7 Batch 140 Loss 0.0289\n",
            "Epoch 7 Batch 150 Loss 0.0325\n",
            "Epoch 7 Batch 160 Loss 0.0337\n",
            "Epoch 7 Batch 170 Loss 0.0358\n",
            "Epoch 7 Batch 180 Loss 0.0310\n",
            "Epoch 7 Batch 190 Loss 0.0229\n",
            "Epoch 7 Loss 0.0004\n",
            "Input:      <start> i haven t done anything to you . <end>\n",
            "Predicted : i haven t done anything to harm . <end> \n",
            "Input:      <start> we try to see how you are doing <end>\n",
            "Predicted : we thought you ought to obey his life ! <end> \n",
            "Time taken for 1 epoch 44.33566093444824 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0256\n",
            "Epoch 8 Batch 10 Loss 0.0211\n",
            "Epoch 8 Batch 20 Loss 0.0220\n",
            "Epoch 8 Batch 30 Loss 0.0133\n",
            "Epoch 8 Batch 40 Loss 0.0255\n",
            "Epoch 8 Batch 50 Loss 0.0211\n",
            "Epoch 8 Batch 60 Loss 0.0159\n",
            "Epoch 8 Batch 70 Loss 0.0243\n",
            "Epoch 8 Batch 80 Loss 0.0242\n",
            "Epoch 8 Batch 90 Loss 0.0379\n",
            "Epoch 8 Batch 100 Loss 0.0301\n",
            "Epoch 8 Batch 110 Loss 0.0161\n",
            "Epoch 8 Batch 120 Loss 0.0308\n",
            "Epoch 8 Batch 130 Loss 0.0385\n",
            "Epoch 8 Batch 140 Loss 0.0333\n",
            "Epoch 8 Batch 150 Loss 0.0258\n",
            "Epoch 8 Batch 160 Loss 0.0415\n",
            "Epoch 8 Batch 170 Loss 0.0330\n",
            "Epoch 8 Batch 180 Loss 0.0275\n",
            "Epoch 8 Batch 190 Loss 0.0306\n",
            "Epoch 8 Loss 0.0004\n",
            "Input:      <start> i haven t done anything to you . <end>\n",
            "Predicted : i haven t done anything to you . <end> \n",
            "Input:      <start> we try to see how you are doing <end>\n",
            "Predicted : we seemed to you see how many ? <end> \n",
            "Time taken for 1 epoch 44.31550741195679 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0185\n",
            "Epoch 9 Batch 10 Loss 0.0325\n",
            "Epoch 9 Batch 20 Loss 0.0196\n",
            "Epoch 9 Batch 30 Loss 0.0297\n",
            "Epoch 9 Batch 40 Loss 0.0396\n",
            "Epoch 9 Batch 50 Loss 0.0199\n",
            "Epoch 9 Batch 60 Loss 0.0385\n",
            "Epoch 9 Batch 70 Loss 0.0298\n",
            "Epoch 9 Batch 80 Loss 0.0316\n",
            "Epoch 9 Batch 90 Loss 0.0275\n",
            "Epoch 9 Batch 100 Loss 0.0419\n",
            "Epoch 9 Batch 110 Loss 0.0374\n",
            "Epoch 9 Batch 120 Loss 0.0277\n",
            "Epoch 9 Batch 130 Loss 0.0191\n",
            "Epoch 9 Batch 140 Loss 0.0431\n",
            "Epoch 9 Batch 150 Loss 0.0317\n",
            "Epoch 9 Batch 160 Loss 0.0338\n",
            "Epoch 9 Batch 170 Loss 0.0283\n",
            "Epoch 9 Batch 180 Loss 0.0411\n",
            "Epoch 9 Batch 190 Loss 0.0378\n",
            "Epoch 9 Loss 0.0005\n",
            "Input:      <start> i haven t done anything to you . <end>\n",
            "Predicted : i haven t done anything to you . <end> \n",
            "Input:      <start> we try to see how you are doing <end>\n",
            "Predicted : we thought to you see their house ! <end> \n",
            "Time taken for 1 epoch 44.15337610244751 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0348\n",
            "Epoch 10 Batch 10 Loss 0.0217\n",
            "Epoch 10 Batch 20 Loss 0.0435\n",
            "Epoch 10 Batch 30 Loss 0.0441\n",
            "Epoch 10 Batch 40 Loss 0.0391\n",
            "Epoch 10 Batch 50 Loss 0.0606\n",
            "Epoch 10 Batch 60 Loss 0.0414\n",
            "Epoch 10 Batch 70 Loss 0.0233\n",
            "Epoch 10 Batch 80 Loss 0.0337\n",
            "Epoch 10 Batch 90 Loss 0.0266\n",
            "Epoch 10 Batch 100 Loss 0.0356\n",
            "Epoch 10 Batch 110 Loss 0.0323\n",
            "Epoch 10 Batch 120 Loss 0.0458\n",
            "Epoch 10 Batch 130 Loss 0.0261\n",
            "Epoch 10 Batch 140 Loss 0.0491\n",
            "Epoch 10 Batch 150 Loss 0.0378\n",
            "Epoch 10 Batch 160 Loss 0.0454\n",
            "Epoch 10 Batch 170 Loss 0.0867\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10 Batch 180 Loss 0.0683\n",
            "Epoch 10 Batch 190 Loss 0.0353\n",
            "Epoch 10 Loss 0.0005\n",
            "Input:      <start> i haven t done anything to you . <end>\n",
            "Predicted : i haven t done anything to you . <end> \n",
            "Input:      <start> we try to see how you are doing <end>\n",
            "Predicted : we didn t expect mark antony to hell ! <end> \n",
            "Time taken for 1 epoch 44.03478407859802 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.0559\n",
            "Epoch 11 Batch 10 Loss 0.0614\n",
            "Epoch 11 Batch 20 Loss 0.0470\n",
            "Epoch 11 Batch 30 Loss 0.0287\n",
            "Epoch 11 Batch 40 Loss 0.0510\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_aggregate_grads\u001b[0;34m(gradients)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_aggregate_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m   \"\"\"Aggregate gradients from multiple sources.\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-e76af3bc513e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m    849\u001b[0m     flat_grad = imperative_grad.imperative_grad(\n\u001b[1;32m    850\u001b[0m         \u001b[0m_default_vspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m         output_gradients=output_gradients)\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(vspace, tape, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m     61\u001b[0m   \"\"\"\n\u001b[1;32m     62\u001b[0m   return pywrap_tensorflow.TFE_Py_TapeGradient(\n\u001b[0;32m---> 63\u001b[0;31m       tape._tape, vspace, target, sources, output_gradients)  # pylint: disable=protected-access\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mrespect\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \"\"\"\n\u001b[0;32m--> 111\u001b[0;31m   \u001b[0mmock_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MockOp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m   \u001b[0mgrad_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgrad_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemError\u001b[0m: <class 'tensorflow.python.eager.backprop._MockOp'> returned a result with an error set"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Af3gLSAmvJhN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TODO: Eval\n"
      ]
    },
    {
      "metadata": {
        "id": "SCv9TJTVvFx3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO: Eval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TqHsArVZ3jFS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "dc821859-b447-419b-a1da-f0a5648ad96d"
      },
      "cell_type": "code",
      "source": [
        "def eval(encoder, decoder, lang, max_length,val_dataset):\n",
        "  \"\"\"\n",
        "    evaluate total-loss on one EPOC of the val_dataset\n",
        "  \"\"\"\n",
        "\n",
        "  \n",
        "  hidden = encoder.get_initial_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset): #dataset return (#batch,(x,y)\n",
        "      loss = 0\n",
        "\n",
        "      _ , enc_hidden = encoder(inp, hidden)\n",
        "\n",
        "      dec_hidden = enc_hidden\n",
        "      \n",
        "      #\n",
        "      dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
        "\n",
        "      # Teacher forcing - feeding the target as the next input\n",
        "      for t in range(1, targ.shape[1]):\n",
        "          # passing enc_output to the decoder\n",
        "          predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "          loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "          # using teacher forcing, TODO: change it to only sometimes run\n",
        "          dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "      total_loss += (loss / int(targ.shape[1]))\n",
        "\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((target_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)\n",
        "val_dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))\n",
        "eval(encoder, decoder, lang, max_length,val_dataset)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0d259ebe5cfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_tensor_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUFFER_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_and_drop_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'lang' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "EbQpyYs13jF_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "dacebdb6-f941-4845-9267-d52ef0875590"
      },
      "cell_type": "code",
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices((target_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)\n",
        "val_dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))\n",
        "\n",
        "for (batch, (inp, targ)) in enumerate(val_dataset):\n",
        "  print ('batch',batch,'inp',inp.shape,'targ',targ.shape)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch 0 inp (64, 64, 14) targ (64, 64, 14)\n",
            "batch 1 inp (64, 64, 14) targ (64, 64, 14)\n",
            "batch 2 inp (64, 64, 14) targ (64, 64, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "91jBNgAPxmTN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}
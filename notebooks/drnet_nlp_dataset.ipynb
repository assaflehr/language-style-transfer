{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "drnet_nlp_dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/assaflehr/language-style-transfer/blob/master/notebooks/drnet_nlp_dataset.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "9DZmHoW4hbsO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "outputId": "436d9dab-2968-450b-b36b-4c08f74a8cb9"
      },
      "cell_type": "code",
      "source": [
        "#!pip install spacy   #can take few minutes\n",
        "#!python -m spacy download en\n",
        "#!pip install git+https://github.com/fastai/fastai.git\n",
        "!pip install torch -U # 0.4 at-least\n",
        "\n",
        "!pip install torchtext  # for simpler datasets\n",
        "\n",
        "!pip install git+https://github.com/IBM/pytorch-seq2seq  #for seq2seq\n",
        "#!pip install git+https://github.com/shahsohil/stableGAN  # for AdamPre adv optimizer\n",
        "!pip install dill  #req of seq2seq\n",
        "!pip install tqdm  #req of seq2seq\n",
        "\n",
        "#!pip install revtok"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 28kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x59f3c000 @  0x7f8926ca61c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.1\n",
            "Collecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/90/474d5944d43001a6e72b9aaed5c3e4f77516fbef2317002da2096fd8b5ea/torchtext-0.2.3.tar.gz (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.3MB/s \n",
            "\u001b[?25hCollecting tqdm (from torchtext)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e6/19dfaff08fcbee7f3453e5b537e65a8364f1945f921a36d08be1e2ff3475/tqdm-4.24.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.18.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2018.8.13)\n",
            "Building wheels for collected packages: torchtext\n",
            "  Running setup.py bdist_wheel for torchtext ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/42/a6/f4/b267328bde6bb680094a0c173e8e5627ccc99543abded97204\n",
            "Successfully built torchtext\n",
            "Installing collected packages: tqdm, torchtext\n",
            "Successfully installed torchtext-0.2.3 tqdm-4.24.0\n",
            "Collecting git+https://github.com/IBM/pytorch-seq2seq\n",
            "  Cloning https://github.com/IBM/pytorch-seq2seq to /tmp/pip-req-build-5y02l8i_\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from seq2seq==0.1.6) (1.14.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from seq2seq==0.1.6) (0.4.1)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from seq2seq==0.1.6) (0.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext->seq2seq==0.1.6) (4.24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->seq2seq==0.1.6) (2.18.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->seq2seq==0.1.6) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->seq2seq==0.1.6) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->seq2seq==0.1.6) (2018.8.13)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->seq2seq==0.1.6) (1.22)\n",
            "Building wheels for collected packages: seq2seq\n",
            "  Running setup.py bdist_wheel for seq2seq ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-20odrl3o/wheels/98/b5/06/771c406b3ecc8ed34f07da72d7baf65b87e561bd9f808e91bd\n",
            "Successfully built seq2seq\n",
            "Installing collected packages: seq2seq\n",
            "Successfully installed seq2seq-0.1.6\n",
            "Collecting dill\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/78/8b96476f4ae426db71c6e86a8e6a81407f015b34547e442291cd397b18f3/dill-0.2.8.2.tar.gz (150kB)\n",
            "\u001b[K    100% |████████████████████████████████| 153kB 5.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: dill\n",
            "  Running setup.py bdist_wheel for dill ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/e2/5d/17/f87cb7751896ac629b435a8696f83ee75b11029f5d6f6bda72\n",
            "Successfully built dill\n",
            "Installing collected packages: dill\n",
            "Successfully installed dill-0.2.8.2\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.24.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "81NtEq8xlXed",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#copied VERBATIM from  git+https://github.com/shahsohil/stableGAN (lacks setup.py)\n",
        "\n",
        "import math\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "class AdamPre(Optimizer):\n",
        "    \"\"\"Implements Adam algorithm with prediction step.\n",
        "    This class implements lookahead version of Adam Optimizer.\n",
        "    The structure of class is similar to Adam class in Pytorch.\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0, name='NotGiven'):\n",
        "        self.name = name\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay)\n",
        "        super(AdamPre, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = grad.new().resize_as_(grad).zero_()\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = grad.new().resize_as_(grad).zero_()\n",
        "\n",
        "                    state['oldWeights'] = p.data.clone()\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad = grad.add(group['weight_decay'], p.data)\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "\n",
        "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "\n",
        "                bias_correction1 = 1 - beta1 ** min(state['step'],1022)\n",
        "                bias_correction2 = 1 - beta2 ** min(state['step'],1022)\n",
        "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
        "        return loss\n",
        "\n",
        "    def stepLookAhead(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                state = self.state[p]\n",
        "                temp_grad = p.data.sub(state['oldWeights'])\n",
        "                state['oldWeights'].copy_(p.data)\n",
        "                p.data.add_(temp_grad)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def restoreStepLookAhead(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                state = self.state[p]\n",
        "                p.data.copy_(state['oldWeights'])\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qHWOFxkWSubZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset with torchtext"
      ]
    },
    {
      "metadata": {
        "id": "vQn5WXMiSzae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "outputId": "7da4594f-f569-4484-af2e-879843d549fb"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "import logging\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchtext.data as data\n",
        "%matplotlib inline\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.DEBUG)\n",
        "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# on sample of the dataset\n",
        "TimeExample = namedtuple('TimeExample',['sent_0','sent_1','sent_x','is_x_0','sent_0_target']) #'src' is out\n",
        "\n",
        "class TimeStyleDataset(Dataset):\n",
        "  \n",
        "  def __init__(self,max_id,time_mod_3_result,label_smoothing=False): \n",
        "    \"\"\"\n",
        "    max_id how many samples are in this dataset. Size of one epoc!\n",
        "    time_mod_3_result - to make sure time is train/test/valid is different, pass 0,1,2\n",
        "    label_smoothing - trick from \"soumith/ganhacks\", instead of 0/1 labels, pass 0-0.3, 0.7-1.2 labels\n",
        "    \"\"\"\n",
        "    self.max_id =int(max_id)\n",
        "    #TODO : add more!!# see here: https://docs.python.org/2/library/datetime.html month can be: %b,%B,%m , year: %y,%Y\n",
        "    self.formats = [\"%b %d %Y %H:%M:%S\", \"%m/%d/%Y %H:%M:%S\", \"%d-%b-%Y %H.%M.%S.00\",\n",
        "                    \"%B %d %y %H:%M:%S\" ,\"%B %d %H:%M:%S %Y\", \"%d/%B/%Y %H:%M:%S\"]\n",
        "    self.time_mod_3_result=time_mod_3_result\n",
        "    self.label_smoothing = label_smoothing \n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.max_id\n",
        "  \n",
        "  def _tvt(self,idx):\n",
        "    # for 10, if train, return 9, valid 10, test 11\n",
        "    return (idx//3) * 3 + self.time_mod_3_result #10//3 * 3 + x = 9+x\n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "    if idx > self.max_id:\n",
        "      raise IndexError(f'TimeStyleDataset {idx} is out of range {self.max_id}')\n",
        "    \n",
        "    \"\"\" x -  x[0] semantic0   , style0\n",
        "             x[1] semantic0   , style1   \n",
        "             x[2] semantic0orX, style0 (1/2 the time 0 , half X)\n",
        "    \"\"\"\n",
        "    max_time= 2*int(2e9)  #means 1970-2001\n",
        "    \n",
        "    random_ids = np.random.randint(low=0,high=max_time,size=2)\n",
        "    idx = random_ids[0] #other_idx is 50% same, 50% other\n",
        "    other_idx = random_ids[1] if np.random.randint(0,2)==0 else random_ids[0]\n",
        "\n",
        "    idx = self._tvt(idx)\n",
        "    other_idx = self._tvt(other_idx)\n",
        "   \n",
        "    # two random formats\n",
        "    random_fs= np.random.choice(len(self.formats),size=2,replace=False)\n",
        "    \n",
        "    sent_0= time.strftime(self.formats[random_fs[0]],time.gmtime(idx)) \n",
        "    sent_1= time.strftime(self.formats[random_fs[1]],time.gmtime(idx)) \n",
        "    #TODO: question: isn't it toally obvios? just do 1 less than the other\n",
        "    sent_x= time.strftime(self.formats[random_fs[0]],time.gmtime(other_idx)) \n",
        "    \n",
        "    y = torch.FloatTensor(np.array([idx!=other_idx],np.float32))\n",
        "    if self.label_smoothing:\n",
        "      y[y==1.0] = (0.7+0.5* np.random.rand()) \n",
        "      y[y==0.0] = (0.3 * np.random.rand())\n",
        "\n",
        "    return TimeExample(sent_0,sent_0,sent_1,sent_x,y,sent_0)\n",
        "\n",
        "def test():\n",
        "  dataset = TimeStyleDataset(1e3,1)\n",
        "  for i in range(3):\n",
        "    sample = dataset[i] ; print ('test sample',sample)  \n",
        "  #print ('len',len(dataset))\n",
        "  \n",
        "test()\n",
        "\n",
        "\n",
        "def revers_vocab(vocab,sent,seperator):\n",
        "  return seperator.join([vocab.itos[token] for token in sent])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from seq2seq.dataset import SourceField,TargetField\n",
        "\"\"\"\" Wrapper class of torchtext.data.Field that forces batch_first to be True \n",
        "and prepend <sos> and append <eos> to sequences in preprocessing step. \"\"\"\n",
        "TEXT_TARGET = TargetField(batch_first =True,sequential=True,use_vocab=True, lower=False, init_token=TargetField.SYM_SOS,\n",
        "                 eos_token=TargetField.SYM_EOS, tokenize=lambda x: list(x)) # fix_length=10\n",
        "TEXT = SourceField(batch_first =True,sequential=True,use_vocab=True, lower=False,\n",
        "                            tokenize=lambda x: list(x)) # fix_length=10\n",
        "LABEL= data.Field(batch_first=True,sequential=False,use_vocab=False, tensor_type =torch.FloatTensor)\n",
        "\n",
        "\n",
        "fields = [('sent_0',TEXT),('sent_1',TEXT),('sent_x',TEXT),('is_x_0',LABEL),('sent_0_target',TEXT_TARGET)] \n",
        "ds_train = data.Dataset(TimeStyleDataset(1e3,1,label_smoothing=True), fields)\n",
        "ds_eval = data.Dataset(TimeStyleDataset(1e3,2), fields)\n",
        "\n",
        "print ('printing dataset directly, before tokenizing:')\n",
        "print ('sent_0',ds_train[2].sent_0) #not processed\n",
        "print ('is_x_0',ds_train[2].is_x_0) #not processed\n",
        "\n",
        "print ('\\nbuilding vocab:')\n",
        "#TEXT.build_vocab(ds, max_size=80000)\n",
        "TEXT_TARGET.build_vocab(ds_train, max_size=80000)\n",
        "TEXT.vocab = TEXT_TARGET.vocab #same except from the added <sos>,<eos>\n",
        "\n",
        "print ('vocab TEXT: len',len(TEXT.vocab), 'common',TEXT.vocab.freqs.most_common()[:10])\n",
        "print ('vocab TEXT_TARGET:',len(TEXT_TARGET.vocab), TEXT_TARGET.vocab.freqs.most_common()[:10])\n",
        "print ('vocab ',TEXT_TARGET.SYM_SOS, TEXT_TARGET.sos_id,TEXT_TARGET.vocab.stoi[TEXT_TARGET.SYM_SOS])\n",
        "print ('vocab ',TEXT_TARGET.SYM_EOS, TEXT_TARGET.eos_id,TEXT_TARGET.vocab.stoi[TEXT_TARGET.SYM_EOS])\n",
        "print ('vocab ','out-of-vocab', TEXT_TARGET.eos_id,TEXT_TARGET.vocab.stoi['out-of-vocab'])\n",
        "\n",
        "       \n",
        "device = None if torch.cuda.is_available() else -1\n",
        "# READ:  https://github.com/mjc92/TorchTextTutorial/blob/master/01.%20Getting%20started.ipynb\n",
        "sort_within_batch=True\n",
        "train_iter = iter(data.BucketIterator( dataset=ds_train, device=device,batch_size=32, sort_within_batch=sort_within_batch,sort_key=lambda x: len(x.sent_0))) \n",
        "eval_iter =  iter(data.BucketIterator( dataset=ds_eval, device=device, batch_size=32, sort_within_batch=sort_within_batch,sort_key=lambda x: len(x.sent_0))) \n",
        "#performance note: the first next, takes 3.5s, the next are fast (10000 is 1s)\n",
        "\n",
        "\n",
        "for i in range(1):\n",
        "  b= next(train_iter)\n",
        "  # usage\n",
        "  print ('\\nb.is_x_0',b.is_x_0[0],b.is_x_0.type())\n",
        "  #print ('b.src is values+len tuple',b.src[0].shape,b.src[1].shape )\n",
        "  print ('b.sent_0_target',b.sent_0_target.shape,b.sent_0_target[0],revers_vocab(TEXT_TARGET.vocab,b.sent_0_target[0],''))\n",
        "  print ('b_sent0',b.sent_0[0].shape,b.sent_0[1].shape,b.sent_0[0][0],revers_vocab(TEXT.vocab,b.sent_0[0][0],''))\n",
        "  print ('b_sent1',b.sent_1[0].shape,b.sent_1[1].shape,b.sent_1[0][0],revers_vocab(TEXT.vocab,b.sent_1[0][0],''))\n",
        "  print ('b_sentx',b.sent_x[0].shape,b.sent_x[1].shape,b.sent_x[0][0],revers_vocab(TEXT.vocab,b.sent_x[0][0],''))\n",
        "  print ('b_y',b.is_x_0.shape,b.is_x_0)\n",
        "  print (b.sent_0[1])\n",
        "\n",
        "\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test sample TimeExample(src='16-May-1981 14.42.10.00', sent_0='16-May-1981 14.42.10.00', sent_1='May 16 14:42:10 1981', sent_x='16-May-1981 14.42.10.00', is_x_0=tensor([0.]), sent_0_target='16-May-1981 14.42.10.00')\n",
            "test sample TimeExample(src='December 27 00:45:28 1995', sent_0='December 27 00:45:28 1995', sent_1='27/December/1995 00:45:28', sent_x='December 27 00:45:28 1995', is_x_0=tensor([0.]), sent_0_target='December 27 00:45:28 1995')\n",
            "test sample TimeExample(src='25/April/2000 15:55:28', sent_0='25/April/2000 15:55:28', sent_1='April 25 00 15:55:28', sent_x='25/April/2000 15:55:28', is_x_0=tensor([0.]), sent_0_target='25/April/2000 15:55:28')\n",
            "printing dataset directly, before tokenizing:\n",
            "sent_0 16/November/2030 14:18:16\n",
            "is_x_0 tensor([0.0233])\n",
            "\n",
            "building vocab:\n",
            "vocab TEXT: len 45 common [('0', 2665), (' ', 2073), ('2', 2068), ('1', 1983), (':', 1724), ('3', 1011), ('4', 931), ('5', 881), ('9', 817), ('/', 652)]\n",
            "vocab TEXT_TARGET: 45 [('0', 2665), (' ', 2073), ('2', 2068), ('1', 1983), (':', 1724), ('3', 1011), ('4', 931), ('5', 881), ('9', 817), ('/', 652)]\n",
            "vocab  <sos> 2 2\n",
            "vocab  <eos> 3 3\n",
            "vocab  out-of-vocab 3 0\n",
            "\n",
            "b.is_x_0 tensor(0.0640, device='cuda:0') torch.cuda.FloatTensor\n",
            "b.sent_0_target torch.Size([32, 27]) tensor([ 2, 36, 17, 26, 17, 32, 24, 17, 19,  5,  4,  9,  5,  6,  7,  8,  9,  9,\n",
            "         8, 10, 12,  5,  6,  4, 14,  7,  3], device='cuda:0') <sos>December 03 21:33:49 2081<eos>\n",
            "b_sent0 torch.Size([32, 25]) torch.Size([32]) tensor([36, 17, 26, 17, 32, 24, 17, 19,  5,  4,  9,  5,  6,  7,  8,  9,  9,  8,\n",
            "        10, 12,  5,  6,  4, 14,  7], device='cuda:0') December 03 21:33:49 2081\n",
            "b_sent1 torch.Size([32, 25]) torch.Size([32]) tensor([ 4,  9, 13, 36, 17, 26, 17, 32, 24, 17, 19, 13,  6,  4, 14,  7,  5,  6,\n",
            "         7,  8,  9,  9,  8, 10, 12], device='cuda:0') 03/December/2081 21:33:49\n",
            "b_sentx torch.Size([32, 26]) torch.Size([32]) tensor([36, 17, 26, 17, 32, 24, 17, 19,  5,  4,  9,  5,  6,  7,  8,  9,  9,  8,\n",
            "        10, 12,  5,  6,  4, 14,  7,  1], device='cuda:0') December 03 21:33:49 2081<pad>\n",
            "b_y torch.Size([32]) tensor([0.0640, 0.1679, 1.0519, 1.1312, 1.1020, 0.8947, 0.2799, 0.1468, 0.2869,\n",
            "        0.8862, 0.2454, 0.9832, 1.1698, 0.9184, 0.8639, 0.0668, 0.0730, 0.8050,\n",
            "        0.8830, 0.2920, 0.1743, 0.2504, 0.9506, 0.1616, 0.0119, 0.2597, 0.0749,\n",
            "        0.0701, 1.0878, 0.1175, 0.0726, 0.9643], device='cuda:0')\n",
            "tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
            "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A0Xw_Gvg2D8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "0429e2d3-1f09-4c62-a9ba-376044424117"
      },
      "cell_type": "code",
      "source": [
        "#!wget http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv\n",
        "#!head quora_duplicate_questions.tsv\n",
        "\n",
        "#note that some entences, like qid=18840 are in many rows , with both true and falsse duplicate. maybe worth choosing them specifically"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id\tqid1\tqid2\tquestion1\tquestion2\tis_duplicate\r\n",
            "0\t1\t2\tWhat is the step by step guide to invest in share market in india?\tWhat is the step by step guide to invest in share market?\t0\r\n",
            "1\t3\t4\tWhat is the story of Kohinoor (Koh-i-Noor) Diamond?\tWhat would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\t0\r\n",
            "2\t5\t6\tHow can I increase the speed of my internet connection while using a VPN?\tHow can Internet speed be increased by hacking through DNS?\t0\r\n",
            "3\t7\t8\tWhy am I mentally very lonely? How can I solve it?\tFind the remainder when [math]23^{24}[/math] is divided by 24,23?\t0\r\n",
            "4\t9\t10\tWhich one dissolve in water quikly sugar, salt, methane and carbon di oxide?\tWhich fish would survive in salt water?\t0\r\n",
            "5\t11\t12\tAstrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\tI'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\t1\r\n",
            "6\t13\t14\tShould I buy tiago?\tWhat keeps childern active and far from phone and video games?\t0\r\n",
            "7\t15\t16\tHow can I be a good geologist?\tWhat should I do to be a great geologist?\t1\r\n",
            "8\t17\t18\tWhen do you use シ instead of し?\t\"When do you use \"\"&\"\" instead of \"\"and\"\"?\"\t0\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B3URbab-qj03",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c509c32d-c1bb-4e64-b57a-0acf15b05489"
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "SOURCE_INT = data.Field(batch_first=True,sequential=False,use_vocab=False)# tensor_type =torch.IntTensor)\n",
        "ds = data.TabularDataset('quora_duplicate_questions.tsv',format='tsv',skip_header=True,\n",
        "     fields=[('id',SOURCE_INT), ('qid1',SOURCE_INT), ('qid2',SOURCE_INT),('question1',SOURCE_INT),('question2',SOURCE_INT),('is_duplicate',SOURCE_INT)] )\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dups 0, not_dups 537933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xfFp6hL63Axk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "0f9beaaf-98e8-4afa-d0cc-45dbd1a87639"
      },
      "cell_type": "code",
      "source": [
        "dups=defaultdict(list)\n",
        "\n",
        "not_dups = defaultdict(list)\n",
        "id2q = {}\n",
        "for i,sample in enumerate(ds):\n",
        "  q1,q2 = int(sample.qid1),int(sample.qid2)\n",
        "  id2q[q1] = sample.question1\n",
        "  id2q[q2] = sample.question2\n",
        "  d = dups if int(sample.is_duplicate)==int(1) else not_dups\n",
        "  d[q1].append(q2)\n",
        "  d[q2].append(q1)\n",
        "  #if i>100: break\n",
        "\n",
        "#36.9% are duplicates\n",
        "print (f'dups {len(dups)}, not_dups {len(not_dups)}')\n",
        "dups_key_list = list(dups)\n",
        "\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dups 149650, not_dups 413109\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-6b34f6b807d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0msem_style_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSemStyleDS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msem_style_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-108-6b34f6b807d8>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \"\"\"\n\u001b[1;32m     38\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdups_key_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0msent_0\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mid2q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0msent_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#choose one of the dups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'random_id' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HUzyuMXje2i1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "ad37aa25-0b68-41a7-cfc0-cb2740804380"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class SemStyleDS(Dataset):\n",
        "  \n",
        "  def __init__(self,ds,label_smoothing=False): \n",
        "    \"\"\"\n",
        "    ds - source dataset from which samples are drawn using ds[i]\n",
        "    label_smoothing - trick from \"soumith/ganhacks\", instead of 0/1 labels, pass 0\n",
        "    \"\"\"\n",
        "    self.ds = ds\n",
        "    self.label_smoothing = label_smoothing \n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.ds)\n",
        "  \n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "    \"\"\" x -  x[0] semantic0   , style0\n",
        "             x[1] semantic0   , style1   \n",
        "             x[2] semantic0orX, style0 (1/2 the time 0 , half X)\n",
        "    \"\"\"\n",
        "    q = random.choice(dups_key_list)\n",
        "    sent_0= id2q[q]\n",
        "    \n",
        "    sent_1 = random.choice(dups[q]) #choose one of the dups\n",
        "    sent_1= id2q[sent_1] \n",
        "    \n",
        "    y = np.random.randint(0,2)\n",
        "    if y==0:\n",
        "      sent_x = sent_0  #same style, same semantics\n",
        "    else:\n",
        "      if q in not_dups:\n",
        "        sent_x = id2q[random.choice(not_dups[q])]\n",
        "      else:\n",
        "        sent_x = id2q[random.choice(dups_key_list)] #just total stranger (can fail once in a lot)\n",
        "    print(y,type(y))        \n",
        "    y = y+0.0# torch.FloatTensor([y],np.float32)\n",
        "    if self.label_smoothing:\n",
        "      y[y==1.0] = (0.7+0.5* np.random.rand()) \n",
        "      y[y==0.0] = (0.3 * np.random.rand())\n",
        "\n",
        "    return TimeExample(sent_0,sent_1,sent_x,y,sent_0)  \n",
        "                      \n",
        "                      \n",
        "#def test():\n",
        "sem_style_ds = SemStyleDS(ds)\n",
        "for i in range(5):\n",
        "  print (sem_style_ds[i])\n",
        "\n",
        "\n"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 <class 'int'>\n",
            "TimeExample(src='What is money and real cost?', sent_0='What is money and real cost?', sent_1='What is the Difference between money cost and real cost?', sent_x='What do you like about Quora?', is_x_0=1.0, sent_0_target='What is money and real cost?')\n",
            "1 <class 'int'>\n",
            "TimeExample(src='What is the main difference between iPhone 6 and 6s?', sent_0='What is the main difference between iPhone 6 and 6s?', sent_1='Which iPhone should I buy between iPhone 6 and 6S?', sent_x='What is load bearing structure?', is_x_0=1.0, sent_0_target='What is the main difference between iPhone 6 and 6s?')\n",
            "1 <class 'int'>\n",
            "TimeExample(src='Who is a slut?', sent_0='Who is a slut?', sent_1='Who is slut?', sent_x='My girlfriend has been touched (boobs) severally by a relative and there has been 2-3 incidents where someone else touched her boobs. Is she slut?', is_x_0=1.0, sent_0_target='Who is a slut?')\n",
            "1 <class 'int'>\n",
            "TimeExample(src='Are chewing gums made up of animal fat as published in many articles?', sent_0='Are chewing gums made up of animal fat as published in many articles?', sent_1='Is chewing gum made up of pig fat? If not, then what is it made up of?', sent_x='What is it like to study medicine at Oxford?', is_x_0=1.0, sent_0_target='Are chewing gums made up of animal fat as published in many articles?')\n",
            "0 <class 'int'>\n",
            "TimeExample(src='Can you see who who viewed your videos on Instagram?', sent_0='Can you see who who viewed your videos on Instagram?', sent_1='Who views Instagram?', sent_x='Can you see who who viewed your videos on Instagram?', is_x_0=0.0, sent_0_target='Can you see who who viewed your videos on Instagram?')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PdoVxGFbP-GR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "Mx_wvDraYghh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# models define"
      ]
    },
    {
      "metadata": {
        "id": "nMc6XeMxWxha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Documentation"
      ]
    },
    {
      "metadata": {
        "id": "wnsebPEMW0d4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "e4db52af-c201-4cfa-dbed-88f308518bdc"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# semantic_encoder : take sentence return a vector with only semantics\n",
        "# semantic similiary objective: given a pair of sentences with the same semantics,\n",
        "#    expect their results to be close (MeanSquared)    \n",
        "\n",
        "# see idea from https://github.com/edenton/drnet-py/blob/master/models/classifiers.py\n",
        "# in her train method:\n",
        "# FRAMES: VID A: FRAME 1 x_c1 , ?x_p2? (1/2 of times)\n",
        "#                FRAME 2 x_c2 , x_p1\n",
        "#         VID B: FRAME 1      , ?x_p2? (1/2 of times)\n",
        "#  \n",
        "# x_p1,x_p2 chosen randonly from same/not-same video\n",
        "# x_c1,x_c2 are always same-video\n",
        "# c1 and p1 are same content, different pose.\n",
        "\n",
        "# train_scene_discriminator()\n",
        "# h_p1,h_p2 = netEP applied to x[0],x[1]. assumption: SAME VIDEO\n",
        "# important: detach both!\n",
        "# override half of h_p2 by random-permutations of the batch. \n",
        "#   [1,2,3,4,5,6]\n",
        "#   [2,3,1,4,5,6] after 1st half permute\n",
        "#   [1,1,1,0,0,0] set unequal the labels (1=unequal, 0 equal. does it matter? should it be 0.9,0.1?)\n",
        "# apply BCE on inpit: concat of [h_p1,h_p2] \n",
        "# run backward, and optimizer on the netC classifier ONLY! emphasize! not on the encoder\n",
        "\n",
        "\n",
        "# train()\n",
        "# h_c1,h_c2 = netEC(x_c1), netEC(x_c2)  where input is x[0],x[1] sim loss is MSE directly on the hidden content-semantics\n",
        "# h_p1,h_p2 = netEP(x_p1),netEP(x_p2) where input is x[2],x[3]\n",
        "# rec = netD([h_c1, h_p1]) h_c1 is DIFFERENT FRAME , but same content, than h_p1\n",
        "# netC is the semantic-discriminator given h_p1,h_p2, target 0.5 (max-entropy). \n",
        "# emphasize! don't optimize netC is this stage\n",
        "\n",
        "#Imp notes\n",
        "# BIDI: when using bidi-encoder, we decided to merge the two D-dim vectors using +, getting D-dim embedding. empiracally, better result than concat\n",
        "# OPTIMIZER: adv training is known to be unstable. We used the following known methods to make it more stable:\n",
        "#            ??? sensativity to lr (fail to converge on close lr)- use AdamPre optimizer instead of Adam\n",
        "#            ??? don't use ReLU layer, use Leaky instead (I used PreRELU)\n",
        "#            never allow G or D to achieve high accuracy (before the end)  \n",
        "#            IMPORTANT: noisy input via dropout\n",
        "#            IMPORTANT: label_smoothing on the ds\n",
        "# ANTI-ADVERSERTIAL LOSS FUNCTION, target is 0.5, so min value is 0.6931471805599453\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "x = np.arange(1,100)/100.0\n",
        "plt.title(\"anti adv-loss with target of 0.5. min is 0.693147 \")\n",
        "plt.plot(x, -0.5*(np.log(x)+np.log(1-x)))\n",
        "#-0.5*(np.log(0.53)+np.log(1-0.53))\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0d07b350f0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEHCAYAAACzy817AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0cc24c9860>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "2qo5XKYPW25Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## PARAMS"
      ]
    },
    {
      "metadata": {
        "id": "ytDrOHZtXMuc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "I9CZBDxzYbt_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6062949a-f9dd-411d-a97b-cfdfe200a4b8"
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import sys\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--epocs', type=int, default=2, help='number of epochs to train for')\n",
        "parser.add_argument('--epoch_size', type=int, default=600, help='epoch size')\n",
        "parser.add_argument('--batch_size', default=32, type=int, help='batch size')\n",
        "\n",
        "parser.add_argument('--optimizer', default='adampre', help='optimizer to train with. only Adam, AdamPre supported. Without AdamPre you must pinpoint the right learning-rate, otherwise training will not converge.')\n",
        "parser.add_argument('--lr', default=0.0025, type=float, help='learning rate. source=0.002')\n",
        "parser.add_argument('--adv_disc_lr', default=0.01, type=float, help='learning rate')\n",
        "\n",
        "parser.add_argument('--beta1', default=0.5, type=float, help='momentum term for adam')\n",
        "\n",
        "parser.add_argument('--semantics_dim', type=int, default=200, help='size of the semantics vector')\n",
        "parser.add_argument('--style_dim', type=int, default=60, help='size of the style vector')\n",
        "parser.add_argument('--sd_weight', type=float, default=0.05, help='weight on adversarial loss 0.0001 originally. 0.5 is good value!')\n",
        "parser.add_argument('--sem_sim_weight', type=float, default=100, help='weight on semantic similiarity loss')\n",
        "\n",
        "parser.add_argument('--max_sent_len', type=int, default=40, help='max size of sentence. sentences typically will be shorter')\n",
        "\n",
        "\n",
        "'''\n",
        "parser.add_argument('--max_step', type=int, default=20, help='maximum distance between frames')\n",
        "parser.add_argument('--seed', default=1, type=int, help='manual seed')\n",
        "parser.add_argument('--log_dir', default='logs', help='base directory to save logs')\n",
        "parser.add_argument('--data_root', default='', help='root directory for data')\n",
        "\n",
        "parser.add_argument('--dataset', default='moving_mnist', help='dataset to train with')\n",
        "\n",
        "parser.add_argument('--sd_nf', type=int, default=100, help='number of layers')\n",
        "parser.add_argument('--content_model', default='dcgan_unet', help='model type (dcgan | dcgan_unet | vgg_unet)')\n",
        "parser.add_argument('--pose_model', default='dcgan', help='model type (dcgan | unet | resnet)')\n",
        "parser.add_argument('--data_threads', type=int, default=5, help='number of parallel data loading threads')\n",
        "parser.add_argument('--data_type', default='drnet', help='speed up data loading for drnet training')\n",
        "'''\n",
        "sys.argv=[\"nothing\"]\n",
        "opt = parser.parse_args()\n",
        "print (opt.lr,)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sIvO_Nes5nSF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c8a18047-4968-46fb-e93d-1ef04cb2fd68"
      },
      "cell_type": "code",
      "source": [
        "type(np.array([2\n",
        "         ]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "DSLXyMbdX99M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define Encoder, Decoder"
      ]
    },
    {
      "metadata": {
        "id": "G3ghi3B0ymmk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#   decoders"
      ]
    },
    {
      "metadata": {
        "id": "bXi51149ioGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "e991ea69-b7a4-4114-b95d-dec27dfa3d0e"
      },
      "cell_type": "code",
      "source": [
        "#trn_dl = DataLoader(TimePairsDataset(1e5,1), batch_size=opt.batch_size,)\n",
        "#x,y = next(iter(trn_dl)) \n",
        "#print ('input . x is vertically stacked sentences',x.shape,'y',y.shape)\n",
        "#batch,pair,sentence_len=x.shape\n",
        "\n",
        "\n",
        "from seq2seq.models import EncoderRNN, DecoderRNN\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "########################## UTILS ##########################\n",
        "cuda=True\n",
        "def T(arg):\n",
        "  if cuda:\n",
        "    if type(arg)==tuple:\n",
        "      arg = tuple( t.cuda() for t in arg ) #new tuple all cuda-d\n",
        "    else:\n",
        "      arg = arg.cuda()\n",
        "  return arg\n",
        "\n",
        "def N(arg):\n",
        "  if isinstance(arg,np.ndarray) or isinstance(arg,np.float64):\n",
        "    return arg #as is\n",
        "  # to numpy\n",
        "  return arg.cpu().numpy()\n",
        "\n",
        "########################## MODELS ##########################\n",
        "class EncoderWrapper(nn.Module):\n",
        "  \"\"\" wraps encoder, accpet in forward tuple of (data,len). return last hidden\"\"\"\n",
        "  def __init__(self,encoder):\n",
        "    super(EncoderWrapper,self).__init__()\n",
        "    self.encoder = encoder\n",
        "  \n",
        "  def forward(self,inp):\n",
        "\n",
        "      #in_data,in_len = in_tuple\n",
        "      output,hidden = self.encoder(*inp)#in_data,in_len)\n",
        "      # **output** (batch, seq_len, hidden_size): tensor containing the encoded features of the input sequence\n",
        "      # **hidden** (num_layers * num_directions, batch, hidden_size): tensor containing the features in the hidden state `h`\n",
        "      #return hidden[0,:,:]\n",
        "      #return hidden[:,:,:].# view(1,hidden.size(1),-1)[0,:,:] #BUG BUG BUG O: check dim order, 2xbsxdim -> 1xbsxdim*2 ??\n",
        "      \n",
        "      #in lstm hidden is a tuple\n",
        "      return torch.sum(hidden,dim=0)\n",
        "      #TODO : BUG HERE \n",
        "\n",
        "\n",
        "\n",
        "variable_lengths = False  # True means batch is ordered. this can't be done as sent0.len!=sent1.len, to make it happen need to seperate batches!!!\n",
        "encoder_bidi=True\n",
        "decoder_bidi=True #not supported True\n",
        "encoder_layers=1  #not supported>1\n",
        "decoder_layers=1\n",
        "\n",
        "# Arch. Question How to work with bidi and multiple layers? \n",
        "# currently, encoder combines with + (sum) bidi vectors. after 8 epocs of 100 batches(100 each) less then 1 recon_loss\n",
        "# multiple layers not supported\n",
        "\n",
        "\n",
        "en_sem = EncoderWrapper(EncoderRNN(len(TEXT.vocab),opt.max_sent_len, opt.semantics_dim,variable_lengths =variable_lengths,bidirectional = encoder_bidi,n_layers=encoder_layers,\n",
        "                                   input_dropout_p = 0.1 , dropout_p =0.0 , rnn_cell='gru'))\n",
        "                                   \n",
        "en_sty = EncoderWrapper(EncoderRNN(len(TEXT.vocab),opt.max_sent_len, opt.style_dim, variable_lengths = variable_lengths,bidirectional = encoder_bidi,n_layers=encoder_layers,\n",
        "                                   input_dropout_p = 0.1 , dropout_p =0.0,rnn_cell='gru'))\n",
        "\n",
        "decoder = DecoderRNN(len(TEXT.vocab),opt.max_sent_len, (1 if encoder_bidi else 1)*(opt.semantics_dim + opt.style_dim),sos_id=TEXT_TARGET.sos_id,eos_id=TEXT_TARGET.eos_id,\n",
        "                     bidirectional = decoder_bidi,n_layers=decoder_layers,\n",
        "                     input_dropout_p = 0.1 , dropout_p =0.0,rnn_cell='gru')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "adv_disc = nn.Sequential(\n",
        "  # input concat of two style\n",
        "  nn.Linear(2*opt.style_dim,30),\n",
        "  nn.PReLU(),\n",
        "  nn.Linear(30,20),\n",
        "  nn.PReLU(),\n",
        "  nn.Linear(20,1),\n",
        "  nn.Sigmoid() #depends on what we have as loss #Must be sigmoid, we apply later BCE\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "en_sem = T(en_sem)\n",
        "en_sty = T(en_sty)\n",
        "decoder =T(decoder)\n",
        "adv_disc = T(adv_disc)\n",
        "\n",
        "def train_gen() :\n",
        "  return iter(data.BucketIterator( dataset=ds_train, device=device,batch_size=32, sort_within_batch=True,sort_key=lambda x: len(x.sent_0))) \n",
        "\n",
        "\n",
        "\n",
        "merge_dim = 1 #1 before\n",
        "def test():\n",
        "  sample = next(train_gen())\n",
        "  \n",
        "  print (type(sample.sent_0))\n",
        "  in_var,in_len=sample.sent_0\n",
        "\n",
        "  #print ('length0',sample.sent_0[1])\n",
        "  #print ('length1',sample.sent_1[1])\n",
        "  sem_out = T(en_sem(sample.sent_0))\n",
        "  print ('result of en_sem',sem_out.shape)   #[1, 32, 20]\n",
        "  sty_out = T(en_sty(sample.sent_1))\n",
        "  print ('sty_out',sty_out.shape,\n",
        "        'concat',T(torch.cat([sty_out, sty_out],dim=merge_dim)).shape)\n",
        "  \n",
        "  merged = T(torch.cat([sty_out, sty_out],dim=merge_dim))\n",
        "  print ('merged1',merged.type(),merged.shape )\n",
        "  disc_out = T(adv_disc(merged))\n",
        "  print (disc_out.shape)\n",
        "\n",
        "  merged = T(torch.cat([sem_out,sty_out],dim=merge_dim))\n",
        "  merged.unsqueeze_(0)\n",
        "  print ('merged2',merged.shape)\n",
        "  decoder_outputs, _,_ = decoder(inputs=None,  # pass not None for teacher focring  (batch, seq_len, input_size)\n",
        "                            encoder_hidden=merged, #(num_layers * num_directions, batch_size, hidden_size)\n",
        "                            encoder_outputs = None, # pass not None for attention\n",
        "                            teacher_forcing_ratio=0 #range 0..1 , must pass inputs if >0\n",
        "                           ) \n",
        "  decoder_outputs = decoder_outputs\n",
        "  #**decoder_outputs** (seq_len, batch, vocab_size): list of tensors with size (batch_size, vocab_size) containing\n",
        "  #          the outputs of the decoding function.\n",
        "  print ('decoder_outputs',len(decoder_outputs),decoder_outputs[0].shape)\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "test()\n",
        "\n",
        "opt.optimizer= 'adam'\n",
        "\n",
        "if opt.optimizer=='adam':\n",
        "  optimizer = torch.optim.Adam\n",
        "elif opt.optimizer=='adampre':\n",
        "  optimizer = AdamPre\n",
        "\n",
        "optimizer_first_time = True  #relevant only to AdamPre\n",
        "\n",
        "optimizer_en_sem = optimizer(en_sem.parameters(), opt.lr) #(, betas=(opt.beta1, 0.999))\n",
        "optimizer_en_sty = optimizer(en_sty.parameters(), opt.lr) #), betas=(opt.beta1, 0.999))\n",
        "optimizer_decoder = optimizer(decoder.parameters(), opt.lr) #)opt.lr), betas=(opt.beta1, 0.999))\n",
        "optimizer_adv_disc = optimizer(adv_disc.parameters(), opt.adv_disc_lr) ##), betas=(opt.beta1, 0.999))\n",
        "\n",
        "print (optimizer)  \n",
        "\n",
        " "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tuple'>\n",
            "result of en_sem torch.Size([32, 200])\n",
            "sty_out torch.Size([32, 60]) concat torch.Size([32, 120])\n",
            "merged1 torch.cuda.FloatTensor torch.Size([32, 120])\n",
            "torch.Size([32, 1])\n",
            "merged2 torch.Size([1, 32, 260])\n",
            "decoder_outputs 40 torch.Size([32, 45])\n",
            "<class 'torch.optim.adam.Adam'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N9QCEv1m8iSt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# EVAL"
      ]
    },
    {
      "metadata": {
        "id": "I1CE35zP44Ez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "c1c7a95a-8782-41d0-9510-44041936c161"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def eval_sample(samples=3):\n",
        "  #back to eval mode\n",
        "  en_sty.eval()  # and not eval() mode\n",
        "  en_sem.eval()\n",
        "  decoder.eval()\n",
        "  adv_disc.eval()\n",
        "\n",
        "  eval_batch_generator = iter(data.BucketIterator( dataset=ds_eval, device=device,batch_size=32, sort_within_batch=False,sort_key=lambda x: len(x.sent_0))) \n",
        "  b = next(eval_batch_generator)\n",
        "  #b = next(training_batch_generator)\n",
        "  #rec loss of 0.4 means good style, half of chars correct (but half wrong)\n",
        "\n",
        "  sent0 = T(b.sent_0)\n",
        "  sent1 = T(b.sent_1)\n",
        "  sentX = T(b.sent_x)\n",
        "  \n",
        "  recon_target = b.sent_0_target  \n",
        "\n",
        "\n",
        "  h_sem0 = en_sem(sent0)\n",
        "  h_sem1 = en_sem(sent1)\n",
        "  h_semX = en_sem(sentX)\n",
        "  \n",
        "  h_sty0 = en_sty(sent0)\n",
        "  h_sty1 = en_sty(sent1)\n",
        "  h_styX = en_sty(sentX)\n",
        "  \n",
        "  recon_sem0_sty0, _,_ = decoder(inputs=None,  # pass not None for teacher focring  (batch, seq_len, input_size)\n",
        "                        #encoder_hidden=T(torch.cat([h_sem1,h_sty0],dim=1)).unsqueeze(0), #(num_layers * num_directions, batch_size, hidden_size)\n",
        "                        encoder_hidden=T(torch.cat([h_sem0,h_sty0],dim=merge_dim)).unsqueeze(0),#(num_layers * num_directions, batch_size, hidden_size)\n",
        "                        encoder_outputs = None, # pass not None for attention\n",
        "                        teacher_forcing_ratio=0 #range 0..1 , must pass inputs if >0\n",
        "                       ) \n",
        "  \n",
        "  \n",
        "  recon_semX_sty0, _,_ = decoder(inputs=None,  # pass not None for teacher focring  (batch, seq_len, input_size)\n",
        "                        #encoder_hidden=T(torch.cat([h_sem1,h_sty0],dim=1)).unsqueeze(0), #(num_layers * num_directions, batch_size, hidden_size)\n",
        "                        encoder_hidden=T(torch.cat([h_semX,h_sty0],dim=merge_dim)).unsqueeze(0),#(num_layers * num_directions, batch_size, hidden_size)\n",
        "                        encoder_outputs = None, # pass not None for attention\n",
        "                        teacher_forcing_ratio=0 #range 0..1 , must pass inputs if >0\n",
        "                       ) \n",
        "  \n",
        "  recon_semX_sty1, _,_ = decoder(inputs=None,  # pass not None for teacher focring  (batch, seq_len, input_size)\n",
        "                        encoder_hidden=T(torch.cat([h_semX,h_sty1],dim=merge_dim)).unsqueeze(0), #(num_layers * num_directions, batch_size, hidden_size)\n",
        "                        encoder_outputs = None, # pass not None for attention\n",
        "                        teacher_forcing_ratio=0 #range 0..1 , must pass inputs if >0\n",
        "                       ) \n",
        "  \n",
        "  recon_semX_sty1_tf1, _,_ = decoder(inputs=recon_target,  # pass not None for teacher focring  (batch, seq_len, input_size)\n",
        "                        encoder_hidden=T(torch.cat([h_semX,h_sty1],dim=merge_dim)).unsqueeze(0), #(num_layers * num_directions, batch_size, hidden_size)\n",
        "                        encoder_outputs = None, # pass not None for attention\n",
        "                        teacher_forcing_ratio=1 #range 0..1 , must pass inputs if >0\n",
        "                       ) \n",
        "\n",
        "  for i in range(samples):\n",
        "    print ('\\n%20s'%'sent0:', revers_vocab(TEXT_TARGET.vocab,sent0[0][i],''))\n",
        "    print ('%20s'%'sent0_targ:',revers_vocab(TEXT_TARGET.vocab,recon_target[i],''))\n",
        "    print ('%20s'%'sent1:',revers_vocab(TEXT_TARGET.vocab,sent1[0][i],''))\n",
        "    print ('%20s'%'sentX:',revers_vocab(TEXT_TARGET.vocab,sentX[0][i],''))\n",
        "\n",
        "    # recon_sent is a list of : batch x softmax array\n",
        "    tokens= (np.array(([N(torch.argmax(l, dim=1)[i]) for l in recon_sem0_sty0])))\n",
        "    print ('%20s'%'recon_sem0_sty0:[TF=0]',revers_vocab(TEXT_TARGET.vocab,tokens,''))\n",
        "\n",
        "    tokens= (np.array(([N(torch.argmax(l, dim=1)[i]) for l in recon_semX_sty0])))\n",
        "    print ('%20s'%'recon_semX_sty0:[TF=0]',revers_vocab(TEXT_TARGET.vocab,tokens,''))\n",
        "\n",
        "    tokens= (np.array(([N(torch.argmax(l, dim=1)[i]) for l in recon_semX_sty1])))\n",
        "    print ('%20s'%'recon_semX_sty1:[TF=0]',revers_vocab(TEXT_TARGET.vocab,tokens,''))\n",
        "    \n",
        "    tokens= (np.array(([N(torch.argmax(l, dim=1)[i]) for l in recon_semX_sty1_tf1])))\n",
        "    print ('%20s'%'recon_semX_sty1:[TF=1]',revers_vocab(TEXT_TARGET.vocab,tokens,''))\n",
        "  \n",
        "  en_sty.train()  # and not eval() mode\n",
        "  en_sem.train()\n",
        "  decoder.train()\n",
        "  adv_disc.train()\n",
        "    \n",
        "  \n",
        "  \n",
        "eval_sample(1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "              sent0: October 26 17:40:59 2045<pad><pad>\n",
            "         sent0_targ: <sos>October 26 17:40:59 2045<eos><pad><pad>\n",
            "              sent1: October 26 45 17:40:59<pad><pad><pad><pad>\n",
            "              sentX: October 26 17:40:59 2045<pad>\n",
            "recon_sem0_sty0:[TF=0] 0ss8/443..Nn7..O7ssp/O6J/6/c0<sos>ss8/443..N\n",
            "recon_semX_sty0:[TF=0] 0ss8/443..Nn7..O7ssp/O6J/6/c0<sos>ss8/443..N\n",
            "recon_semX_sty1:[TF=0] s8/443..Nn7..O7ssp/O6J/6/c0<sos>ss8/443..Nn7\n",
            "recon_semX_sty1:[TF=1] sJyhs/u6c/g/ci..sM.ArAs4.8<sos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XnxNY67yEtTt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ee319de5-40da-4fbc-9b17-083f738da716"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "np.log(0.5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.6931471805599453"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "GaVWTeleYkWk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train loop"
      ]
    },
    {
      "metadata": {
        "id": "_jDD-H1faFtq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5336
        },
        "outputId": "1618acc4-6cc2-45be-90c7-ba8ee25c2301"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    #copied code from: https://gist.github.com/harveyslash/725fcc68df112980328951b3426c0e0b#file-contrastive-loss-py\n",
        "    \"\"\"\n",
        "    Contrastive loss function.\n",
        "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        # 1 means same.  0 means not same\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
        "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
        "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "        return loss_contrastive\n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      # --------- training funtions ------------------------------------\n",
        "def train(b,epoch=0):\n",
        "    # x[0] semantic0   , style0\n",
        "    # x[1] semantic0   , style1   \n",
        "    # x[2] semantic0orX, style0 (1/2 the time 0 , half X)\n",
        "\n",
        "    en_sty.zero_grad() \n",
        "    en_sem.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    \n",
        "    if opt.optimizer=='adampre':\n",
        "      optimizer_adv_disc.stepLookAhead()\n",
        "  \n",
        "\n",
        "    #sent0 = b.sent_0 #sem A , style A\n",
        "    #sent1 = b.sent_1 #sem A, style B\n",
        "    #sentX = b.sent_x #semAorC , style A\n",
        "    recon_target = b.sent_0_target  #one-hot\n",
        "    \n",
        "    logger.debug(f'sent_0 {b.sent_0[0].shape} {b.sent_0[1].shape}')    \n",
        "\n",
        "    h_sem0 = en_sem(b.sent_0) \n",
        "    h_sem1 = en_sem(b.sent_1) \n",
        "    h_semX = en_sem(b.sent_x)\n",
        "    \n",
        "    ######### SIM LOSS #########\n",
        "    # Original was MSE \n",
        "    # if you want to use torch criterion, you need to copy the label and set it to not requreing gradiant\n",
        "    # so below is different than nn.MSELoss()(h_sem0,h_sem1.detach()). I wonder if only one get grad updates!\n",
        "    #sim_loss = torch.mean(torch.sum(torch.pow(h_sem0- h_sem1,2),dim=1))\n",
        "    #############################################\n",
        "    # But constractive loss is more reasnible     \n",
        "    sim_loss = ContrastiveLoss()(h_sem1,h_semX,T(torch.round(b.is_x_0)))\n",
        "    logger.debug(f'sem_loss: {h_sem0.shape} {h_sem1.shape} {sim_loss.shape} {sim_loss} {T(torch.round(b.is_x_0))}')\n",
        "    #TODO: is it 0 or 1??????????????? \n",
        "    logger.debug('#TODO: is it 0 or 1??????????????? ')\n",
        "    logger.debug('#TODO: is it 0 or 1??????????????? ')\n",
        "    logger.debug('#TODO: is it 0 or 1??????????????? ')\n",
        "    logger.debug('#TODO: is it 0 or 1??????????????? ')\n",
        "    logger.debug('#TODO: is it 0 or 1??????????????? ')\n",
        "    logger.debug('#TODO: is it 0 or 1??????????????? ')\n",
        "\n",
        "    ######### RECONSTRUCTION LOSS #########\n",
        "    # reconstruct sent0 from semantics of sent1 (==sem of sent0, different style), and style of sent0.\n",
        "    h_sty0 = en_sty(b.sent_0)\n",
        "    merged= torch.cat([h_sem1,h_sty0],dim=merge_dim)\n",
        "    merged.unsqueeze_(0) #32x25 -> 1x32x25 . 1 is for one hidden-layer (not-stacked)\n",
        "    logger.debug(f'h_sem1.h_sty0 {h_sem1.shape} {h_sty0.shape,merged.shape}')\n",
        "    \n",
        "    \n",
        "    recon_sent0, _,_ = decoder(inputs=recon_target,  # pass not None for teacher focring  (batch, seq_len, input_size)\n",
        "                          encoder_hidden=merged, #(num_layers * num_directions, batch_size, hidden_size)\n",
        "                          encoder_outputs = None, # pass not None for attention\n",
        "                          teacher_forcing_ratio=1 #in(0, 1-random.random()* epoch * 0.1) #range 0..1 , must pass inputs if >0. as epochs increase, it's lower\n",
        "                         ) \n",
        "    #print('$'*10,'recon_sent0 length',len(recon_sent0),'each',recon_sent0[0].shape)\n",
        "    #rec_loss = nn.MSELoss()(recon_sent0,sent0)\n",
        "    # see impl https://github.com/IBM/pytorch-seq2seq/blob/master/seq2seq/loss/loss.py\n",
        "    acc_loss, norm_term=0,0\n",
        "    logger.debug(f'recon_target {recon_target.shape} while decoder outputs {len(recon_sent0)}')\n",
        "    #target shape [32, 26]  batch x words , actual len of 50\n",
        "    for step, step_output in enumerate(recon_sent0):\n",
        "      batch_size = recon_target.size(0)\n",
        "      #print('step_output at step ',step,step_output.shape,type(step_output))\n",
        "      outputs = step_output #step_output.contiguous().view(batch_size, -1)\n",
        "      \n",
        "      # what to do if output is too-long? decision here is to match only relevant parts\n",
        "      if step+1>=recon_target.size(1):  \n",
        "        #print ('breaking!!! at step',step)\n",
        "        break\n",
        "      gold = recon_target[:, step + 1] #tuple [0] is data. [1] is len\n",
        "      #print('output at step ',step,outputs.shape,type(outputs),'gold',gold.shape,type(gold))\n",
        "      curr_loss = nn.NLLLoss()(outputs, gold)\n",
        "      #logger.debug(f'loss for token {norm_term},{outputs.shape}, {gold.shape}, {curr_loss}')\n",
        "      acc_loss += curr_loss\n",
        "      norm_term += 1\n",
        "    rec_loss = acc_loss/norm_term\n",
        "    \n",
        "    \n",
        "    ######### ADV LOSS #########  TODO : willl it be better to use completely different sentences?\n",
        "    h_styX = en_sty(b.sent_x)\n",
        "    adv_disc_p =  adv_disc(torch.cat([h_sty0,h_styX],dim=merge_dim))\n",
        "    #logger.debug (N(adv_disc_p[0:3].data).T)\n",
        "    adv_target = T(torch.FloatTensor(np.full(shape =(b.sent_0[0].shape[0],1),fill_value=0.5)))\n",
        "    # the loss below is a parabula with min at log(0.5)=0.693... see documentation above\n",
        "    adv_disc_loss = nn.BCELoss()(adv_disc_p, adv_target) + np.log(0.5)  #np.log(0.5)=-0.693 , \n",
        "    logger.debug(f'### adv_disc_loss {N(adv_disc_p.data[:5]).T} target={N(adv_target.data[:5]).T} bce={adv_disc_loss}')\n",
        "    logger.debug(f'    sanity test: on first step, you expect adv_disc_loss to be near zero')\n",
        "    \n",
        "    \n",
        "    ######### BACKWARD #########\n",
        "    # full loss\n",
        "    loss = rec_loss + sim_loss*opt.sem_sim_weight + opt.sd_weight*adv_disc_loss #rec_loss + sim_loss + opt.sd_weight*adv_disc_loss\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer_en_sem.step()\n",
        "    optimizer_en_sty.step()\n",
        "    optimizer_decoder.step()\n",
        "\n",
        "    \n",
        "    \n",
        "    if opt.optimizer=='adampre':\n",
        "      optimizer_adv_disc.restoreStepLookAhead()\n",
        "\n",
        "    return N(sim_loss.data)*opt.sem_sim_weight, N(rec_loss.data) ,N(adv_disc_loss.data)*opt.sd_weight\n",
        "\n",
        "  \n",
        "def train_scene_discriminator(b):\n",
        "    global optimizer_first_time\n",
        "\n",
        "    sent0 = T(b.sent_0)\n",
        "    sentX = T(b.sent_x)\n",
        "    y = T(b.is_x_0)\n",
        "      \n",
        "    \n",
        "    adv_disc.zero_grad()\n",
        "    if opt.optimizer=='adampre' and not optimizer_first_time:\n",
        "      optimizer_decoder.stepLookAhead()\n",
        "      \n",
        "\n",
        "    h_sty0    = en_sty(sent0) \n",
        "    h_sty0or2 = en_sty(sentX)  #same style, same or different semantics with random chance\n",
        "    \n",
        "    merged = torch.cat([h_sty0, h_sty0or2],dim=merge_dim)\n",
        "    \n",
        "    logger.debug(f'merged {merged.shape}') #4x32xdim\n",
        "    out = adv_disc(merged) #\n",
        "    out = out.flatten()\n",
        "\n",
        "    #TODO: #Note BCELossWithLogits is faster and more stable, to use it remove sigmoid from network end\n",
        "    logger.debug(f'out {out.shape} y {y.shape}')\n",
        "    \n",
        "    \n",
        "    \n",
        "    #bce = nn.BCELoss()(out.flatten(), y.flatten()) #should wrapp in varaible? \n",
        "    bce = nn.BCELoss()(out, y) #should wrapp in varaible? \n",
        "    logger.debug('train_scene_discriminator {out} {y}')\n",
        "    \n",
        "    \n",
        "    bce.backward()\n",
        "    optimizer_adv_disc.step()\n",
        "    \n",
        "    if opt.optimizer=='adampre' and not optimizer_first_time:\n",
        "      optimizer_decoder.restoreStepLookAhead()\n",
        "    \n",
        "    optimizer_first_time = False\n",
        "    \n",
        "    \n",
        "    #print (out.shape) #torch.Size([16, 1])\n",
        "    acc =  np.round(N(out.detach()))==np.round(N(y))  #CHECK THIS DIMENSTIONS!!! \n",
        "    logger.debug(f'adv_disc out {out.shape} is_x_0 {y.shape}')\n",
        "    logger.debug(f'out {out.flatten()} y {y.flatten()} acc {acc} bce {bce.data}')\n",
        "    #print (acc.shape) #1,16\n",
        "    acc = acc.reshape(-1)#.float()\n",
        "    acc= acc.sum()/len(acc)\n",
        "    return N(bce.data), N(acc)\n",
        "  \n",
        "  \n",
        "\n",
        "def one_epoc(epoch):\n",
        "    #iter provides new epoc generator\n",
        "    training_batch_generator = iter(data.BucketIterator( dataset=ds_train, device=device,batch_size=32, sort_within_batch=False,sort_key=lambda x: len(x.sent_0))) \n",
        "\n",
        "    epoch_sim_loss, epoch_rec_loss, epoch_anti_disc_loss, epoch_sd_loss, epoch_sd_acc = 0, 0, 0, 0, 0\n",
        "\n",
        "    #training_batch_generator = iter(data.BucketIterator( dataset=ds, batch_size=opt.batch_size, sort_within_batch=False,sort_key=lambda x: len(x.sent_0))) \n",
        "    for i in range(opt.epoch_size):\n",
        "        #if i % 100==0 : print ('batch',i,'of',opt.epoch_size)\n",
        "        b = next(training_batch_generator)\n",
        "\n",
        "        # train scene discriminator\n",
        "        logger.debug (f'b_sent_0 {b.sent_0[0].shape}{b.sent_0[1].shape}')#TEXT.reverse(b.sent_0))\n",
        "\n",
        "        sd_loss, sd_acc = train_scene_discriminator(b)\n",
        "        epoch_sd_loss += sd_loss\n",
        "        epoch_sd_acc += sd_acc\n",
        "\n",
        "\n",
        "        # train main model\n",
        "        sim_loss, rec_loss, anti_disc_loss = train(b,epoch)\n",
        "          \n",
        "        epoch_sim_loss += sim_loss\n",
        "        epoch_rec_loss += rec_loss\n",
        "        epoch_anti_disc_loss += anti_disc_loss\n",
        "        \n",
        "        logger.setLevel(logging.INFO)\n",
        "    print('[%02d] rec loss: %.4f | sim loss: %.4f | anti_disc_loss: %.4f || scene disc %.4f %.3f%% ' % (epoch, epoch_rec_loss/opt.epoch_size, \n",
        "                epoch_sim_loss/opt.epoch_size, epoch_anti_disc_loss/opt.epoch_size,\n",
        "                epoch_sd_loss/opt.epoch_size, 100*epoch_sd_acc/opt.epoch_size))\n",
        "  \n",
        "\n",
        "# --------- training loop ------------------------------------\n",
        "opt.sem_sim_weight = 1\n",
        "opt.sd_weight=1 #1\n",
        "opt.epocs=300\n",
        "opt.epoch_size=100\n",
        "opt.batch_size= 32 #32 is much better than 100. lr 0.000625/beta0.5 is converging\n",
        "\n",
        "\n",
        "en_sty.train()  # and not eval() mode\n",
        "en_sem.train()\n",
        "decoder.train()\n",
        "adv_disc.train()\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.DEBUG) #not debug\n",
        "epoc_count=0\n",
        "%time one_epoc(epoc_count)    \n",
        "epoc_count+=1\n",
        "\n",
        "opt.lr=0.01\n",
        "for lr in [opt.lr/4,opt.lr/4,opt.lr/8,opt.lr/8,opt.lr/16,opt.lr/16]:\n",
        " \n",
        "  print ('lr',lr,opt.beta1)\n",
        "  optimizer_en_sem =   optimizer(en_sem.parameters(),  lr, betas=(opt.beta1, 0.999))\n",
        "  optimizer_en_sty =   optimizer(en_sty.parameters(),  lr, betas=(opt.beta1, 0.999))\n",
        "  optimizer_decoder =  optimizer(decoder.parameters(), lr, betas=(opt.beta1, 0.999))\n",
        "  optimizer_adv_disc = torch.optim.SGD(adv_disc.parameters(), lr)#, betas=(opt.beta1, 0.999)) #\n",
        "\n",
        "  for epoch in range(0,16): \n",
        "    one_epoc(epoc_count)\n",
        "    epoc_count+=1\n",
        "  eval_sample(3)\n",
        "    \n",
        "\n",
        "print ('training loop done')\n",
        "# TODO: save the model\n",
        "# converge to 0.5 recon, but 1.6 adv loss: sd_weight !!! for lr in [opt.lr/4,opt.lr/4,opt.lr/8,opt.lr/8,opt.lr/16,opt.lr/16]: beta=0.5\n",
        "\n",
        "print('todo discriminator too stong!!!!!')\n",
        "\n",
        "print('todo LSTMMMM!!!!')\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "07:54:51 DEBUG:b_sent_0 torch.Size([32, 26])torch.Size([32])\n",
            "07:54:51 DEBUG:merged torch.Size([32, 120])\n",
            "07:54:51 DEBUG:out torch.Size([32]) y torch.Size([32])\n",
            "07:54:51 DEBUG:train_scene_discriminator {out} {y}\n",
            "07:54:51 DEBUG:adv_disc out torch.Size([32]) is_x_0 torch.Size([32])\n",
            "07:54:51 DEBUG:out tensor([0.6042, 0.5915, 0.5723, 0.5544, 0.5765, 0.6067, 0.6014, 0.6083, 0.6083,\n",
            "        0.5920, 0.5889, 0.5732, 0.5924, 0.5876, 0.6047, 0.5965, 0.5754, 0.6075,\n",
            "        0.5898, 0.5842, 0.5814, 0.6089, 0.5890, 0.6021, 0.5731, 0.5756, 0.5859,\n",
            "        0.6040, 0.5880, 0.5910, 0.5883, 0.5890],\n",
            "       device='cuda:0', grad_fn=<AsStridedBackward>) y tensor([0.0352, 1.0399, 0.2679, 1.0876, 0.1792, 0.1900, 0.1697, 0.0069, 0.8917,\n",
            "        0.9345, 0.2729, 0.0830, 0.7622, 0.1709, 0.0963, 0.1367, 0.1397, 0.8284,\n",
            "        0.7583, 0.0095, 0.2234, 0.0100, 1.0373, 0.2888, 0.7551, 0.8474, 0.8410,\n",
            "        0.7247, 0.2602, 0.7512, 0.7910, 0.9049], device='cuda:0') acc [False  True False  True False False False False  True  True False False\n",
            "  True False False False False  True  True False False False  True False\n",
            "  True  True  True  True False  True  True  True] bce 0.7199746966362\n",
            "07:54:51 DEBUG:sent_0 torch.Size([32, 26]) torch.Size([32])\n",
            "07:54:52 DEBUG:sem_loss: torch.Size([32, 200]) torch.Size([32, 200]) torch.Size([]) 15.937655448913574 tensor([0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.],\n",
            "       device='cuda:0')\n",
            "07:54:52 DEBUG:#TODO: is it 0 or 1??????????????? \n",
            "07:54:52 DEBUG:#TODO: is it 0 or 1??????????????? \n",
            "07:54:52 DEBUG:#TODO: is it 0 or 1??????????????? \n",
            "07:54:52 DEBUG:#TODO: is it 0 or 1??????????????? \n",
            "07:54:52 DEBUG:#TODO: is it 0 or 1??????????????? \n",
            "07:54:52 DEBUG:#TODO: is it 0 or 1??????????????? \n",
            "07:54:52 DEBUG:h_sem1.h_sty0 torch.Size([32, 200]) (torch.Size([32, 60]), torch.Size([1, 32, 260]))\n",
            "07:54:52 DEBUG:recon_target torch.Size([32, 28]) while decoder outputs 27\n",
            "07:54:52 DEBUG:### adv_disc_loss [[0.58044606 0.5592619  0.5404345  0.5446672  0.5497814 ]] target=[[0.5 0.5 0.5 0.5 0.5]] bce=0.7011301517486572\n",
            "07:54:52 DEBUG:    sanity test: on first step, you expect adv_disc_loss to be near zero\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[00] rec loss: 1.2019 | sim loss: 1.5650 | anti_disc_loss: 0.0111 || scene disc 0.6845 52.719% \n",
            "CPU times: user 10.3 s, sys: 2.41 s, total: 12.7 s\n",
            "Wall time: 12.7 s\n",
            "lr 0.0025 0.5\n",
            "[01] rec loss: 0.7635 | sim loss: 0.1466 | anti_disc_loss: 0.0088 || scene disc 0.6575 68.656% \n",
            "[02] rec loss: 0.6990 | sim loss: 0.0864 | anti_disc_loss: 0.0026 || scene disc 0.6734 64.875% \n",
            "[03] rec loss: 0.6521 | sim loss: 0.0581 | anti_disc_loss: 0.0029 || scene disc 0.6724 62.500% \n",
            "[04] rec loss: 0.6264 | sim loss: 0.0408 | anti_disc_loss: 0.0036 || scene disc 0.6729 63.188% \n",
            "[05] rec loss: 0.5667 | sim loss: 0.0645 | anti_disc_loss: 0.0036 || scene disc 0.6764 59.812% \n",
            "[06] rec loss: 0.5336 | sim loss: 0.0691 | anti_disc_loss: 0.0042 || scene disc 0.6738 60.750% \n",
            "[07] rec loss: 0.4949 | sim loss: 0.0406 | anti_disc_loss: 0.0049 || scene disc 0.6710 62.375% \n",
            "[08] rec loss: 0.4665 | sim loss: 0.0295 | anti_disc_loss: 0.0052 || scene disc 0.6770 58.781% \n",
            "[09] rec loss: 0.4388 | sim loss: 0.0213 | anti_disc_loss: 0.0054 || scene disc 0.6763 59.062% \n",
            "[10] rec loss: 0.4092 | sim loss: 0.0192 | anti_disc_loss: 0.0052 || scene disc 0.6752 60.250% \n",
            "[11] rec loss: 0.3887 | sim loss: 0.0167 | anti_disc_loss: 0.0052 || scene disc 0.6783 58.844% \n",
            "[12] rec loss: 0.3638 | sim loss: 0.0188 | anti_disc_loss: 0.0060 || scene disc 0.6760 58.156% \n",
            "[13] rec loss: 0.3312 | sim loss: 0.0149 | anti_disc_loss: 0.0050 || scene disc 0.6786 58.344% \n",
            "[14] rec loss: 0.3123 | sim loss: 0.0179 | anti_disc_loss: 0.0047 || scene disc 0.6783 59.438% \n",
            "[15] rec loss: 0.2936 | sim loss: 0.0161 | anti_disc_loss: 0.0047 || scene disc 0.6773 60.000% \n",
            "[16] rec loss: 0.2801 | sim loss: 0.0329 | anti_disc_loss: 0.0049 || scene disc 0.6815 56.344% \n",
            "\n",
            "              sent0: May 20 21 14:08:11<pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos>May 20 21 14:08:11<eos><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sent1: May 20 14:08:11 2021<pad><pad><pad><pad><pad><pad>\n",
            "              sentX: May 20 21 14:08:11<pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] May 20 2014 18:01:04<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty0:[TF=0] May 20 2014 18:01:04<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=0] May 20 04:11:19 2089<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=1] May 20 00000:01:10<eos><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "\n",
            "              sent0: 15/August/2085 20:01:14<pad><pad><pad>\n",
            "         sent0_targ: <sos>15/August/2085 20:01:14<eos><pad><pad><pad>\n",
            "              sent1: August 15 85 20:01:14<pad><pad><pad><pad><pad>\n",
            "              sentX: 15/August/2085 20:01:14<pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] 15/August/2089 05:01:40<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty0:[TF=0] 15/August/2089 05:01:40<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=0] August 15 88 05:14:04<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=1] AAAAugust/1085 00:01:00<eos><pad><pad><pad>\n",
            "\n",
            "              sent0: 14-Feb-2020 15.03.41.00<pad><pad><pad>\n",
            "         sent0_targ: <sos>14-Feb-2020 15.03.41.00<eos><pad><pad><pad>\n",
            "              sent1: 14/February/2020 15:03:41<pad>\n",
            "              sentX: 01-Apr-2025 21.27.41.00<pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] 14-Feb-2020 15.30.10.00<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty0:[TF=0] 14-Feb-2020 15.30.10.00<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=0] 14/February/2025 01:30:01<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=1] 14/Febr2059 15.33.10.00<eos><pad><pad><pad>\n",
            "lr 0.0025 0.5\n",
            "[17] rec loss: 0.2642 | sim loss: 0.0302 | anti_disc_loss: 0.0047 || scene disc 0.6821 57.781% \n",
            "[18] rec loss: 0.2550 | sim loss: 0.1527 | anti_disc_loss: 0.0050 || scene disc 0.6811 57.156% \n",
            "[19] rec loss: 0.2366 | sim loss: 0.1224 | anti_disc_loss: 0.0051 || scene disc 0.6814 56.094% \n",
            "[20] rec loss: 0.2247 | sim loss: 0.0949 | anti_disc_loss: 0.0061 || scene disc 0.6808 57.062% \n",
            "[21] rec loss: 0.2184 | sim loss: 0.0564 | anti_disc_loss: 0.0063 || scene disc 0.6857 55.438% \n",
            "[22] rec loss: 0.2140 | sim loss: 0.0403 | anti_disc_loss: 0.0053 || scene disc 0.6931 52.812% \n",
            "[23] rec loss: 0.2077 | sim loss: 0.0496 | anti_disc_loss: 0.0045 || scene disc 0.6986 49.438% \n",
            "[24] rec loss: 0.2046 | sim loss: 0.0319 | anti_disc_loss: 0.0037 || scene disc 0.6940 49.781% \n",
            "[25] rec loss: 0.1968 | sim loss: 0.0380 | anti_disc_loss: 0.0051 || scene disc 0.6923 50.875% \n",
            "[26] rec loss: 0.1890 | sim loss: 0.0308 | anti_disc_loss: 0.0052 || scene disc 0.6919 49.719% \n",
            "[27] rec loss: 0.1769 | sim loss: 0.0340 | anti_disc_loss: 0.0067 || scene disc 0.6900 50.750% \n",
            "[28] rec loss: 0.1641 | sim loss: 0.0470 | anti_disc_loss: 0.0052 || scene disc 0.6907 50.219% \n",
            "[29] rec loss: 0.1668 | sim loss: 0.0241 | anti_disc_loss: 0.0065 || scene disc 0.6926 49.344% \n",
            "[30] rec loss: 0.1539 | sim loss: 0.0320 | anti_disc_loss: 0.0062 || scene disc 0.6897 51.000% \n",
            "[31] rec loss: 0.1527 | sim loss: 0.0313 | anti_disc_loss: 0.0067 || scene disc 0.6905 49.969% \n",
            "[32] rec loss: 0.1433 | sim loss: 0.0345 | anti_disc_loss: 0.0045 || scene disc 0.6921 49.719% \n",
            "\n",
            "              sent0: December 08 83 00:00:11<pad><pad>\n",
            "         sent0_targ: <sos>December 08 83 00:00:11<eos><pad><pad>\n",
            "              sent1: 08/December/2083 00:00:11<pad>\n",
            "              sentX: December 08 83 00:00:11<pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] December 08 03 08:00:10 2000<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty0:[TF=0] December 08 03 08:00:10 2000<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=0] 08/December/2083 00:01:01<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=1] 0ecember 08 02 00:01:19<eos><pad><pad>\n",
            "\n",
            "              sent0: September 29 91 17:37:44<pad>\n",
            "         sent0_targ: <sos>September 29 91 17:37:44<eos><pad>\n",
            "              sent1: 29/September/2091 17:37:44\n",
            "              sentX: August 04 43 13:17:32<pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] September 29 91 17:43:22<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty0:[TF=0] September 29 91 17:43:28<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=0] 29/September/1997 11:38:49<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=1] 2eptember 29 01 17:44:40<eos><pad>\n",
            "\n",
            "              sent0: Mar 28 2065 01:26:50<pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos>Mar 28 2065 01:26:50<eos><pad><pad><pad><pad><pad>\n",
            "              sent1: 28-Mar-2065 01.26.50.00<pad><pad><pad>\n",
            "              sentX: Mar 28 2065 01:26:50<pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] Mar 28 2065 01:52:58<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty0:[TF=0] Mar 28 2065 01:52:58<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=0] 28-Mar-2065 01.59.28.00<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=1] 2arc28 0065 01:55:55<eos><pad><pad><pad><pad><pad>\n",
            "lr 0.00125 0.5\n",
            "[33] rec loss: 0.1163 | sim loss: 0.0277 | anti_disc_loss: 0.0055 || scene disc 0.6901 50.031% \n",
            "[34] rec loss: 0.1111 | sim loss: 0.0245 | anti_disc_loss: 0.0040 || scene disc 0.6921 49.375% \n",
            "[35] rec loss: 0.1036 | sim loss: 0.0241 | anti_disc_loss: 0.0046 || scene disc 0.6896 50.906% \n",
            "[36] rec loss: 0.1016 | sim loss: 0.0181 | anti_disc_loss: 0.0059 || scene disc 0.6879 51.125% \n",
            "[37] rec loss: 0.0991 | sim loss: 0.0248 | anti_disc_loss: 0.0053 || scene disc 0.6900 49.594% \n",
            "[38] rec loss: 0.0923 | sim loss: 0.0286 | anti_disc_loss: 0.0054 || scene disc 0.6891 50.000% \n",
            "[39] rec loss: 0.0924 | sim loss: 0.0240 | anti_disc_loss: 0.0057 || scene disc 0.6880 51.000% \n",
            "[40] rec loss: 0.0907 | sim loss: 0.0300 | anti_disc_loss: 0.0059 || scene disc 0.6862 51.969% \n",
            "[41] rec loss: 0.0875 | sim loss: 0.0359 | anti_disc_loss: 0.0071 || scene disc 0.6859 51.438% \n",
            "[42] rec loss: 0.0855 | sim loss: 0.0317 | anti_disc_loss: 0.0064 || scene disc 0.6890 49.625% \n",
            "[43] rec loss: 0.0835 | sim loss: 0.0273 | anti_disc_loss: 0.0064 || scene disc 0.6864 50.906% \n",
            "[44] rec loss: 0.0831 | sim loss: 0.0246 | anti_disc_loss: 0.0061 || scene disc 0.6885 50.219% \n",
            "[45] rec loss: 0.0802 | sim loss: 0.0285 | anti_disc_loss: 0.0073 || scene disc 0.6855 52.219% \n",
            "[46] rec loss: 0.0820 | sim loss: 0.0245 | anti_disc_loss: 0.0060 || scene disc 0.6922 48.688% \n",
            "[47] rec loss: 0.0794 | sim loss: 0.0283 | anti_disc_loss: 0.0052 || scene disc 0.6893 50.500% \n",
            "[48] rec loss: 0.0741 | sim loss: 0.0253 | anti_disc_loss: 0.0050 || scene disc 0.6890 50.438% \n",
            "\n",
            "              sent0: 04/30/2090 19:46:56<pad><pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos>04/30/2090 19:46:56<eos><pad><pad><pad><pad><pad><pad>\n",
            "              sent1: 30/April/2090 19:46:56<pad><pad><pad><pad>\n",
            "              sentX: 10/24/2067 15:20:29<pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] 04/30/2090 16:46:55<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty0:[TF=0] 04/30/2090 16:46:55<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=0] 30/April/2090 16:46:52<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=1] 33/39/2099 19:56:55<eos><pad><pad><pad><pad><pad><pad>\n",
            "\n",
            "              sent0: 14/October/1998 07:47:50<pad>\n",
            "         sent0_targ: <sos>14/October/1998 07:47:50<eos><pad>\n",
            "              sent1: 14-Oct-1998 07.47.50.00<pad><pad><pad>\n",
            "              sentX: 14/October/1998 07:47:50<pad>\n",
            "recon_sem0_sty0:[TF=0] 14/October/1997 08:49:52<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty0:[TF=0] 14/October/1997 08:49:52<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=0] 14-Oct-1997 08.49.52.00<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=1] 14-October/1997 07:47:52<eos><pad>\n",
            "\n",
            "              sent0: September 11 81 02:44:11<pad>\n",
            "         sent0_targ: <sos>September 11 81 02:44:11<eos><pad>\n",
            "              sent1: 11/September/1981 02:44:11\n",
            "              sentX: September 11 81 02:44:11<pad>\n",
            "recon_sem0_sty0:[TF=0] September 11 81 02:41:49<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty0:[TF=0] September 11 81 02:41:49<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=0] 11/September/1981 13:44:49<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=1] 1eptember 11 01:12:44:19<eos><pad>\n",
            "lr 0.00125 0.5\n",
            "[49] rec loss: 0.0778 | sim loss: 0.0327 | anti_disc_loss: 0.0049 || scene disc 0.6900 49.656% \n",
            "[50] rec loss: 0.0727 | sim loss: 0.0313 | anti_disc_loss: 0.0045 || scene disc 0.6896 49.969% \n",
            "[51] rec loss: 0.0702 | sim loss: 0.0303 | anti_disc_loss: 0.0042 || scene disc 0.6919 48.312% \n",
            "[52] rec loss: 0.0665 | sim loss: 0.0184 | anti_disc_loss: 0.0044 || scene disc 0.6887 50.844% \n",
            "[53] rec loss: 0.0679 | sim loss: 0.0248 | anti_disc_loss: 0.0054 || scene disc 0.6904 49.688% \n",
            "[54] rec loss: 0.0661 | sim loss: 0.0176 | anti_disc_loss: 0.0050 || scene disc 0.6900 50.594% \n",
            "[55] rec loss: 0.0673 | sim loss: 0.0137 | anti_disc_loss: 0.0058 || scene disc 0.6864 51.750% \n",
            "[56] rec loss: 0.0650 | sim loss: 0.0171 | anti_disc_loss: 0.0055 || scene disc 0.6898 49.594% \n",
            "[57] rec loss: 0.0627 | sim loss: 0.0210 | anti_disc_loss: 0.0045 || scene disc 0.6914 49.344% \n",
            "[58] rec loss: 0.0587 | sim loss: 0.0199 | anti_disc_loss: 0.0051 || scene disc 0.6902 50.406% \n",
            "[59] rec loss: 0.0597 | sim loss: 0.0131 | anti_disc_loss: 0.0043 || scene disc 0.6917 49.031% \n",
            "[60] rec loss: 0.0607 | sim loss: 0.0195 | anti_disc_loss: 0.0042 || scene disc 0.6901 49.812% \n",
            "[61] rec loss: 0.0591 | sim loss: 0.0204 | anti_disc_loss: 0.0041 || scene disc 0.6931 47.781% \n",
            "[62] rec loss: 0.0553 | sim loss: 0.0200 | anti_disc_loss: 0.0045 || scene disc 0.6889 50.719% \n",
            "[63] rec loss: 0.0541 | sim loss: 0.0178 | anti_disc_loss: 0.0048 || scene disc 0.6903 49.312% \n",
            "[64] rec loss: 0.0560 | sim loss: 0.0244 | anti_disc_loss: 0.0053 || scene disc 0.6867 51.219% \n",
            "\n",
            "              sent0: April 17 10:42:53 2047<pad><pad><pad>\n",
            "         sent0_targ: <sos>April 17 10:42:53 2047<eos><pad><pad><pad>\n",
            "              sent1: 04/17/2047 10:42:53<pad><pad><pad><pad><pad><pad><pad>\n",
            "              sentX: April 17 10:42:53 2047<pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] April 17 10:42:52 2030<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty0:[TF=0] April 17 10:42:52 2030<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=0] 04/17/2047 10:42:55<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=1] 0uril 00 00:44:40 1011<eos><pad><pad><pad>\n",
            "\n",
            "              sent0: 09/10/2042 21:32:23<pad><pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos>09/10/2042 21:32:23<eos><pad><pad><pad><pad><pad><pad>\n",
            "              sent1: 10/September/2042 21:32:23\n",
            "              sentX: 09/10/2042 21:32:23<pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] 09/10/2042 21:32:22<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty0:[TF=0] 09/10/2042 21:32:22<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=0] 10/September/2042 21:32:22<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=1] 11/S2/2042 21:32:22<eos><pad><pad><pad><pad><pad><pad>\n",
            "\n",
            "              sent0: Jan 12 1980 23:15:14<pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos>Jan 12 1980 23:15:14<eos><pad><pad><pad><pad><pad>\n",
            "              sent1: 12-Jan-1980 23.15.14.00<pad><pad><pad>\n",
            "              sentX: Jan 12 1980 23:15:14<pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] Jan 12 2080 12:35:10<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty0:[TF=0] Jan 12 2080 12:35:10<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=0] 12-Jan-1980 23.15.13.00<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=1] 1anu22-0980 23:15:00<eos><pad><pad><pad><pad><pad>\n",
            "lr 0.000625 0.5\n",
            "[65] rec loss: 0.0456 | sim loss: 0.0156 | anti_disc_loss: 0.0061 || scene disc 0.6887 50.594% \n",
            "[66] rec loss: 0.0422 | sim loss: 0.0220 | anti_disc_loss: 0.0059 || scene disc 0.6887 50.406% \n",
            "[67] rec loss: 0.0396 | sim loss: 0.0156 | anti_disc_loss: 0.0061 || scene disc 0.6868 51.156% \n",
            "[68] rec loss: 0.0382 | sim loss: 0.0120 | anti_disc_loss: 0.0066 || scene disc 0.6861 52.156% \n",
            "[69] rec loss: 0.0380 | sim loss: 0.0275 | anti_disc_loss: 0.0059 || scene disc 0.6930 48.750% \n",
            "[70] rec loss: 0.0363 | sim loss: 0.0201 | anti_disc_loss: 0.0055 || scene disc 0.6894 50.594% \n",
            "[71] rec loss: 0.0376 | sim loss: 0.0229 | anti_disc_loss: 0.0052 || scene disc 0.6898 49.719% \n",
            "[72] rec loss: 0.0353 | sim loss: 0.0328 | anti_disc_loss: 0.0047 || scene disc 0.6894 50.281% \n",
            "[73] rec loss: 0.0371 | sim loss: 0.0259 | anti_disc_loss: 0.0052 || scene disc 0.6904 50.406% \n",
            "[74] rec loss: 0.0362 | sim loss: 0.0163 | anti_disc_loss: 0.0052 || scene disc 0.6871 51.531% \n",
            "[75] rec loss: 0.0330 | sim loss: 0.0156 | anti_disc_loss: 0.0057 || scene disc 0.6877 50.562% \n",
            "[76] rec loss: 0.0331 | sim loss: 0.0164 | anti_disc_loss: 0.0056 || scene disc 0.6897 49.844% \n",
            "[77] rec loss: 0.0338 | sim loss: 0.0180 | anti_disc_loss: 0.0052 || scene disc 0.6890 50.719% \n",
            "[78] rec loss: 0.0304 | sim loss: 0.0174 | anti_disc_loss: 0.0055 || scene disc 0.6879 50.969% \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2832ae74eed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[0mone_epoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoc_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m     \u001b[0mepoc_count\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m   \u001b[0meval_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-2832ae74eed2>\u001b[0m in \u001b[0;36mone_epoc\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m#if i % 100==0 : print ('batch',i,'of',opt.epoch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_batch_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# train scene discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0;31m# fast-forward if loaded from state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36minit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_state_this_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restored_from_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36mcreate_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m                                  self.batch_size_fn)\n\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             self.batches = pool(self.data(), self.batch_size,\n\u001b[0m\u001b[1;32m    239\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                                 \u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-fe95113e5b5e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mother_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m       \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "m5aLVfuOvoMU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "outputId": "9b869364-f834-4e03-99fb-85200e2c9aa7"
      },
      "cell_type": "code",
      "source": [
        "en_sty.train()  # and not eval() mode\n",
        "en_sem.train()\n",
        "decoder.train()\n",
        "adv_disc.train()\n",
        "for lr in [opt.lr/16,opt.lr/16]:\n",
        "  print ('lr',lr)\n",
        "  optimizer_en_sem = optimizer(en_sem.parameters(), lr) #(, betas=(opt.beta1, 0.999))\n",
        "  optimizer_en_sty = optimizer(en_sty.parameters(), lr) #), betas=(opt.beta1, 0.999))\n",
        "  optimizer_decoder = optimizer(decoder.parameters(), lr) #)opt.lr), betas=(opt.beta1, 0.999))\n",
        "  optimizer_adv_disc = optimizer(adv_disc.parameters(), lr=lr )#opt.adv_disc_lr) ##), betas=(opt.beta1, 0.999))\n",
        "\n",
        "  for epoch in range(0,8): \n",
        "    #NOT ADV\n",
        "    #0.01 from high to .4 in few epocs\n",
        "    #0.0025 from 0.4 down to 0.2 rec-loss (30 epocs of 100)\n",
        "    #0.00125 from 0.2 down to 0.158  rec-loss (30 epocs of 100)\n",
        "    #0.000625 from 0.158 to 0.135\n",
        "    \n",
        "    #ADV: \n",
        "    #0.01 till 0.67\n",
        "    #0.0025 till 0.58\n",
        "    #0.00125 till 0.5\n",
        "    #0.000625 till \n",
        "    \n",
        "    one_epoc(epoc_count)\n",
        "    epoc_count+=1"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lr 0.000625\n",
            "[18] rec loss: 0.2826 | sim loss: 0.0000 | anti_disc_loss: 0.1819 || scene disc 0.3959 93.312% \n",
            "[19] rec loss: 0.2787 | sim loss: 0.0000 | anti_disc_loss: 0.1815 || scene disc 0.4076 93.812% \n",
            "[20] rec loss: 0.2809 | sim loss: 0.0000 | anti_disc_loss: 0.1807 || scene disc 0.3995 92.969% \n",
            "[21] rec loss: 0.2778 | sim loss: 0.0000 | anti_disc_loss: 0.1821 || scene disc 0.3969 93.656% \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-4dc306f6e3ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#0.000625 till\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mone_epoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoc_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mepoc_count\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-80-365d90735cb7>\u001b[0m in \u001b[0;36mone_epoc\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;31m# train main model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0msim_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manti_disc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mepoch_sim_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msim_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mepoch_rec_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrec_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-80-365d90735cb7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(b, epoch)\u001b[0m\n\u001b[1;32m     41\u001b[0m                           \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#(num_layers * num_directions, batch_size, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                           \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# pass not None for attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                           \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;31m#in(0, 1-random.random()* epoch * 0.1) #range 0..1 , must pass inputs if >0. as epochs increase, it's lower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                          ) \n\u001b[1;32m     45\u001b[0m     \u001b[0;31m#print('$'*10,'recon_sent0 length',len(recon_sent0),'each',recon_sent0[0].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/seq2seq/models/DecoderRNN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                     \u001b[0mstep_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                 \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/seq2seq/models/DecoderRNN.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(step, step_output, step_attn)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mret_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDecoderRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKEY_ATTN_SCORE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0msymbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0msequence_symbols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "B_EFr6PYZBnc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "qTXN1yzzZB7n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "uDNCzdrLZCLy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "ZVLOrOIYp3d4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# end of train"
      ]
    },
    {
      "metadata": {
        "id": "ARPHhHtNit6k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "outputId": "b2e96e5e-f6f0-4643-f731-91ce7d0f1ecf"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "              sent0: Aug 23 2047 06:49:08<pad><pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos>Aug 23 2047 06:49:08<eos><pad><pad><pad><pad><pad><pad>\n",
            "              sent1: August 23 47 06:49:08<pad><pad><pad><pad>\n",
            "              sentX: Aug 23 2047 06:49:08<pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] Aug 23 2046 07:00:46<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty0:[TF=0] Aug 23 2046 07:00:46<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=0] Aug 23 2046 07:00:46<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=1] Aug 23 2046 06:00:44<eos><pad><pad><pad><pad><pad><pad>\n",
            "\n",
            "              sent0: 07/24/2023 23:20:17<pad><pad><pad><pad><pad><pad><pad>\n",
            "         sent0_targ: <sos>07/24/2023 23:20:17<eos><pad><pad><pad><pad><pad><pad><pad>\n",
            "              sent1: Jul 24 2023 23:20:17<pad><pad><pad><pad><pad>\n",
            "              sentX: 07/24/2023 23:20:17<pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] 07/23/2032 20:23:19<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty0:[TF=0] 07/23/2032 20:23:19<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=0] 07/23/2032 20:23:19<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=1] 07/23/2032 22:27:19<eos><pad><pad><pad><pad><pad><pad><pad>\n",
            "\n",
            "              sent0: 16-Mar-2081 11.46.32.00<pad><pad><pad>\n",
            "         sent0_targ: <sos>16-Mar-2081 11.46.32.00<eos><pad><pad><pad>\n",
            "              sent1: Mar 16 2081 11:46:32<pad><pad><pad><pad><pad>\n",
            "              sentX: 24-Sep-2074 21.18.35.00<pad><pad><pad>\n",
            "recon_sem0_sty0:[TF=0] 16-Mar-2081 18.26.43.00<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty0:[TF=0] 14-Sep-2078 18.25.37.00<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=0] 24-Sep-2074 21.50.37.00<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "recon_semX_sty1:[TF=1] 24-Sar-2077 17.47.37.00<eos><pad><pad><pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hp_41_yw0vpK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5237e5c6-05b8-425f-cf89-2d1b1676fe43"
      },
      "cell_type": "code",
      "source": [
        "''' BCE dynamics\n",
        "bce=lambda a,p: nn.BCELoss()(torch.tensor(a),torch.tensor(p)).numpy()\n",
        "#p= np.array([[0.5001803,  0.50018024, 0.5001803 ]])\n",
        "p = np.array([[1.0,1.0,1.0]]); a = np.array([[0.0,0.0,0.0]]); loss = bce(p,a) ; print (a,p,loss)\n",
        "p = np.array([[1.0,1.0,1.0]]); a = np.array([[1.0,0.0,0.0]]); loss = bce(p,a) ; print (a,p,loss)\n",
        "p = np.array([[1.0,1.0,0.0]]); a = np.array([[0.5,0.5,0.5]]); loss = bce(p,a) ; print (a,p,loss)\n",
        "p = np.array([[0.5,0.5,0.5]]); a = np.array([[0.5,0.5,0.5]]); loss = bce(p,a) ; print (a,p,loss)\n",
        "p = np.array([[0.6,0.4,0.5]]); a = np.array([[0.5,0.5,0.5]]); loss = bce(p,a) ; print (a,p,loss)\n",
        "\n",
        "#       -1*( y* log(p) + (1-y) log(1-p) )\n",
        "#p=0.5  -1   y* 0.693  + (1-y)* 0.693  == -0.693 (y+1-y) = -0.693\n",
        "\n",
        "#a=0.5  -1* (0.5*log(p)+ 0.5*log(1-p)) = -0.5(log(p)+log(1-p))\n",
        "'''\n",
        "1"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "GSPSoEXTgVV_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from here: http://anie.me/On-Torchtext/\n",
        "from torchtext.data import Field\n",
        "import torch\n",
        "import torch.nn\n",
        "\n",
        "class SplitReversibleField(Field):\n",
        "\n",
        "    def __init__(self, untokenize_char='', **kwargs):\n",
        "        super(SplitReversibleField, self).__init__(**kwargs)\n",
        "        self.untokenize_char = untokenize_char\n",
        "        \n",
        "\n",
        "    def reverse(self, batch):\n",
        "\n",
        "        if not self.batch_first:\n",
        "            batch = batch.t()\n",
        "        with torch.cuda.device_of(batch):\n",
        "            batch = batch.tolist()\n",
        "        batch = [[self.vocab.itos[ind] for ind in ex] for ex in batch]  # denumericalize\n",
        "\n",
        "        def trim(s, t):\n",
        "            sentence = []\n",
        "            for w in s:\n",
        "                if w == t:\n",
        "                    break\n",
        "                sentence.append(w)\n",
        "            return sentence\n",
        "\n",
        "        batch = [trim(ex, self.eos_token) for ex in batch]  # trim past frst eos\n",
        "\n",
        "        def filter_special(tok):\n",
        "            return tok not in (self.init_token, self.pad_token)\n",
        "\n",
        "        batch = [filter(filter_special, ex) for ex in batch]\n",
        "        return [self.untokenize_char.join(ex) for ex in batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sQLllWeCXlrK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fx9wR-zaDxTc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MY EncoderRNN self made buggy start of result"
      ]
    },
    {
      "metadata": {
        "id": "lRy7otShNctN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "vAVE29Z8hgHf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "139999cb-22e4-48ec-c033-df4d34f4cc82"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "%matplotlib inline\n",
        "\n",
        "sample = next(train_iter)\n",
        "device='cpu'\n",
        "'''\n",
        "class MyEncoderRNN(nn.Module):  #see https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        \"\"\" input:  1xbs :  example: 1   x 32\n",
        "            hidden: hxbs :  example: 100 x 32\n",
        "        \"\"\"\n",
        "        print ('EncoderRNN input',input.shape, 'hidden',hidden.shape)\n",
        "        assert input.shape[1]==hidden.shape[1] # batch\n",
        "        assert input.shape[0]==1\n",
        "        assert hidden.shape[0]==self.hidden_size\n",
        "        \n",
        "        \n",
        "        embedded = self.embedding(input).view(1, input.shape[1], -1)\n",
        "        print (embedded.shape)\n",
        "        output, hidden = self.gru(embedded, hidden) #input must be 3 dimension\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self,batch_size):\n",
        "        return torch.zeros( 1, self.hidden_size,batch_size, device=device)  \n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoder_rnn):\n",
        "        \"\"\" wrapper around EncoderRNN running on a full sequence in one fwd pass \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encoder_rnn = encoder_rnn\n",
        "    \n",
        "    def forward(self, input):\n",
        "        print (input.shape)\n",
        "        sen_len, batch_size = input.shape\n",
        "        h= self.encoder_rnn.init_hidden(batch_size)\n",
        "        for i in range(sen_len):\n",
        "          o,h = self.encoder_rnn(input[i:i+1], h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class MyDecoderRNN(nn.Module):\n",
        "    def __init__(self, input_hidden_size, output_vocab_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "      \n",
        "      \n",
        "#decoder= MyDecoderRNN(opt.semantics_dim+opt.style_dim)   \n",
        "\n",
        "\n",
        "en_sem = Encoder(EncoderRNN(len(TEXT.vocab), opt.semantics_dim))\n",
        "en_sty = Encoder(EncoderRNN(len(TEXT.vocab), opt.style_dim))\n",
        "\n",
        "en_sem(sample.sent_0)\n",
        "'''\n",
        "1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "metadata": {
        "id": "PLIjIkZpNdxR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# where this code came from?"
      ]
    },
    {
      "metadata": {
        "id": "IKcrmDmgLnJS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "      \n",
        "en_sem = nn.Sequential(\n",
        "  # expect sentence_len,so from outside, cut a pair into x[0],x[1] and pass seperatly each\n",
        "  nn.Linear(sentence_len,19),\n",
        "  nn.ReLU(inplace=True), \n",
        "  nn.Linear(19,opt.semantics_dim),\n",
        "  #nn.Tanh()\n",
        ")\n",
        "\n",
        "en_sty= nn.Sequential(\n",
        "  nn.Linear(sentence_len,19),\n",
        "  nn.ReLU(inplace=True),\n",
        "  nn.Linear(19,opt.style_dim),\n",
        "  #nn.Tanh()\n",
        ")\n",
        "decoder = nn.Sequential(\n",
        "  # input concat of semantic and style\n",
        "  # output sentence_len , each word/char has currently value in range 0..1\n",
        "  nn.Linear(opt.semantics_dim+opt.style_dim,19),\n",
        "  nn.ReLU(inplace=True),\n",
        "  nn.Linear(19,60),\n",
        "  nn.ReLU(inplace=True),\n",
        "  nn.Linear(60,sentence_len),\n",
        "  # we apply MSE on this\n",
        ")\n",
        "adv_disc = nn.Sequential(\n",
        "  # input concat of two style\n",
        "  nn.Linear(2*opt.style_dim,6),\n",
        "  nn.ReLU(inplace=True),\n",
        "  nn.Linear(6,1),\n",
        "  nn.Sigmoid()  #Must be sigmoid, we apply later BCE\n",
        ")\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XTYtFITBzdaC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jKl-9BxXNgvU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "crjmKimmLijT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''from torch.utils.data import Dataset#, DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "class TimePairsDataset(Dataset):\n",
        "  \n",
        "  def __init__(self,length,time_mod_3_result): #1e9 means 1970-2001\n",
        "    \"\"\"\n",
        "    time_mod_3_result - to make sure time is train/test/valid is different, pass\n",
        "    a different result here.\n",
        "    \"\"\"\n",
        "    #super().__init__()\n",
        "    self.length = length\n",
        "    self.formats = [\"%b %d %Y %H:%M:%S\", \"%m/%b/%Y %H:%M:%S\"] # STAY WITH 2 STYLES, DISCRIMINATOR SIMPLER #\"%d %b %Y %H:%M:%S\",\n",
        "    self.time_mod_3_result=time_mod_3_result\n",
        "  \n",
        "  \n",
        "  def __len__(self):\n",
        "    return int(self.length)\n",
        "  \n",
        "  def _choose(self,idx,style):\n",
        "    st = time.strftime(self.formats[style],time.gmtime(idx))\n",
        "    return np.array([ord(c) for c in list(st)][:24],np.float32)/120 #chars are up to that\n",
        "  \n",
        "  def untokenize_sample(self,sample):\n",
        "    \"\"\" sample x and y, x contains multiple sentences\"\"\"\n",
        "    x,y = sample\n",
        "    out=[] \n",
        "    for tokens in x:    \n",
        "      out.append(self.untokenize_tokens(tokens) )\n",
        "    return out,y\n",
        "\n",
        "  def untokenize_tokens(self,tokens):\n",
        "    \"\"\" one sentence\"\"\"\n",
        "    if type(tokens)==torch.Tensor:\n",
        "      tokens = tokens.detach().numpy()\n",
        "    return ''.join([chr(int(round(token*120))) for token in tokens])\n",
        "  \n",
        "  \n",
        "  def _tvt(self,idx):\n",
        "    # for 10, if train, return 9, valid 10, test 11\n",
        "    return (idx//3) * 3 + self.time_mod_3_result #10//3 * 3 + x = 9+x\n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "    \"\"\" x -  x[0] semantic0   , style0\n",
        "             x[1] semantic0   , style1   \n",
        "             x[2] semantic0orX, style0 (1/2 the time 0 , half X)\n",
        "    \"\"\"\n",
        "    \n",
        "    #other_idx is 50% same, 50% other\n",
        "    other_idx = (idx + np.random.randint(0,2)*(np.random.randint(10,self.length))) %self.length\n",
        "    #print (idx,other_idx)\n",
        "    idx = self._tvt(idx)\n",
        "    other_idx = self._tvt(other_idx)\n",
        "    #print (idx,other_idx)\n",
        "    \n",
        "    # two random formats\n",
        "    random_fs= np.random.choice(len(self.formats),size=2,replace=False)\n",
        "    #print (random_fs)\n",
        "    x_list = []\n",
        "    x_list.append(self._choose(idx,random_fs[0]))\n",
        "    x_list.append(self._choose(idx,random_fs[1]))\n",
        "    x_list.append(self._choose(other_idx,random_fs[0]))\n",
        "    \n",
        "    y = np.array([idx==other_idx],np.float32)\n",
        "    \n",
        "   \n",
        "    return  (np.vstack(x_list), y)\n",
        "\n",
        "def test():\n",
        "  dataset = TimePairsDataset(1e9,1)\n",
        "  sample = dataset[9] ;#print (sample)\n",
        "  print (dataset.untokenize_sample(sample))\n",
        "  print ('shapes x,y',sample[0].shape,sample[1].shape)\n",
        "  \n",
        "test()\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NswxRK1Pzexq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a66fouEILpSz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Seq2Seq Sanity"
      ]
    },
    {
      "metadata": {
        "id": "i9Q6pVYxLr7n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "7f96da18-e47e-4b92-c3e4-95c4f4607287"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torchtext\n",
        "\n",
        "import seq2seq\n",
        "from seq2seq.trainer import SupervisedTrainer\n",
        "from seq2seq.models import EncoderRNN, DecoderRNN, Seq2seq\n",
        "from seq2seq.loss import Perplexity\n",
        "from seq2seq.optim import Optimizer\n",
        "from seq2seq.dataset import SourceField, TargetField\n",
        "from seq2seq.evaluator import Predictor\n",
        "\n",
        "\n",
        "\n",
        "# Prepare dataset\n",
        "src = SourceField()\n",
        "tgt = TargetField()\n",
        "max_len = 20\n",
        "src = TEXT\n",
        "tgt = TEXT_TARGET\n",
        "input_vocab = src.vocab\n",
        "output_vocab = tgt.vocab\n",
        "train = ds\n",
        "dev = ds_eval\n",
        "\n",
        "# NOTE: If the source field name and the target field name\n",
        "# are different from 'src' and 'tgt' respectively, they have\n",
        "# to be set explicitly before any training or inference\n",
        "seq2seq.src_field_name = 'sent_0'\n",
        "seq2seq.tgt_field_name = 'sent_0_target'\n",
        "\n",
        "# Prepare loss\n",
        "weight = torch.ones(len(tgt.vocab))\n",
        "pad = tgt.vocab.stoi[tgt.pad_token]\n",
        "loss = Perplexity(weight, pad)\n",
        "if torch.cuda.is_available():\n",
        "    loss.cuda()\n",
        "\n",
        "seq2seq = None\n",
        "optimizer = None\n",
        "# Initialize model\n",
        "hidden_size=128\n",
        "bidirectional = True\n",
        "encoder = EncoderRNN(len(src.vocab), max_len, hidden_size,\n",
        "                     bidirectional=bidirectional, variable_lengths=True)\n",
        "decoder = DecoderRNN(len(tgt.vocab), max_len, hidden_size * 2 if bidirectional else hidden_size,\n",
        "                     dropout_p=0.2, use_attention=True, bidirectional=bidirectional,\n",
        "                     eos_id=tgt.eos_id, sos_id=tgt.sos_id)\n",
        "seq2seq = Seq2seq(encoder, decoder)\n",
        "if torch.cuda.is_available():\n",
        "    seq2seq.cuda()\n",
        "\n",
        "for param in seq2seq.parameters():\n",
        "    param.data.uniform_(-0.08, 0.08)\n",
        "\n",
        "# Optimizer and learning rate scheduler can be customized by\n",
        "# explicitly constructing the objects and pass to the trainer.\n",
        "#\n",
        "# optimizer = Optimizer(torch.optim.Adam(seq2seq.parameters()), max_grad_norm=5)\n",
        "# scheduler = StepLR(optimizer.optimizer, 1)\n",
        "# optimizer.set_scheduler(scheduler)\n",
        "\n",
        "# train\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')\n",
        "\n",
        "t = SupervisedTrainer(loss=loss, batch_size=32,\n",
        "                    checkpoint_every=50,\n",
        "                    print_every=10, expt_dir='.')\n",
        "\n",
        "seq2seq = t.train(seq2seq, train,\n",
        "                  num_epochs=2, dev_data=dev,\n",
        "                  optimizer=optimizer,\n",
        "                  teacher_forcing_ratio=0.5,\n",
        "                  resume=False,)\n",
        "\n",
        "predictor = Predictor(seq2seq, input_vocab, output_vocab)\n",
        "seq_str = \"November 21 77 14:07:40\" #raw_input(\"Type in a source sequence:\")\n",
        "seq = seq_str.strip().split()\n",
        "''.join(predictor.predict(\"NoveMBer 21 77 14:07:40\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "02:08:29 INFO:Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            "), Scheduler: None\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:321: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train), lengths\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n",
            "02:08:32 INFO:Finished epoch 1: Train Perplexity: 40.7963, Dev Perplexity: 33.6183, Accuracy: 0.1151\n",
            "02:08:35 INFO:Finished epoch 2: Train Perplexity: 30.6331, Dev Perplexity: 23.3884, Accuracy: 0.1210\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'00000000000000000000'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}
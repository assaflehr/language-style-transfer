{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "drnet_nlp_dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/assaflehr/language-style-transfer/blob/master/notebooks/drnet_nlp_dataset.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "9DZmHoW4hbsO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "97aaf4ed-d312-42cf-d3f5-999557eba535"
      },
      "cell_type": "code",
      "source": [
        "#!pip install spacy   #can take few minutes\n",
        "#!python -m spacy download en\n",
        "#!pip install git+https://github.com/fastai/fastai.git\n",
        "!pip install torch -U # 0.4 at-least\n",
        "\n",
        "!pip install torchtext  # for simpler datasets\n",
        "\n",
        "!pip install git+https://github.com/IBM/pytorch-seq2seq  #for seq2seq\n",
        "!pip install dill  #req of seq2seq\n",
        "!pip install tqdm  #req of seq2seq\n",
        "\n",
        "#!pip install revtok"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 27kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5871e000 @  0x7ff3b14e81c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.1\n",
            "Collecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/90/474d5944d43001a6e72b9aaed5c3e4f77516fbef2317002da2096fd8b5ea/torchtext-0.2.3.tar.gz (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 3.2MB/s \n",
            "\u001b[?25hCollecting tqdm (from torchtext)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e6/19dfaff08fcbee7f3453e5b537e65a8364f1945f921a36d08be1e2ff3475/tqdm-4.24.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.18.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2018.4.16)\n",
            "Building wheels for collected packages: torchtext\n",
            "  Running setup.py bdist_wheel for torchtext ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/42/a6/f4/b267328bde6bb680094a0c173e8e5627ccc99543abded97204\n",
            "Successfully built torchtext\n",
            "Installing collected packages: tqdm, torchtext\n",
            "Successfully installed torchtext-0.2.3 tqdm-4.24.0\n",
            "Collecting git+https://github.com/IBM/pytorch-seq2seq\n",
            "  Cloning https://github.com/IBM/pytorch-seq2seq to /tmp/pip-req-build-nj05jeiq\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from seq2seq==0.1.6) (1.14.5)\r\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from seq2seq==0.1.6) (0.4.1)\r\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from seq2seq==0.1.6) (0.2.3)\r\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext->seq2seq==0.1.6) (4.24.0)\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->seq2seq==0.1.6) (2.18.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->seq2seq==0.1.6) (2018.4.16)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->seq2seq==0.1.6) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->seq2seq==0.1.6) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->seq2seq==0.1.6) (2.6)\n",
            "Building wheels for collected packages: seq2seq\n",
            "  Running setup.py bdist_wheel for seq2seq ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-jd42spq0/wheels/98/b5/06/771c406b3ecc8ed34f07da72d7baf65b87e561bd9f808e91bd\n",
            "Successfully built seq2seq\n",
            "Installing collected packages: seq2seq\n",
            "Successfully installed seq2seq-0.1.6\n",
            "Collecting dill\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/78/8b96476f4ae426db71c6e86a8e6a81407f015b34547e442291cd397b18f3/dill-0.2.8.2.tar.gz (150kB)\n",
            "\u001b[K    100% |████████████████████████████████| 153kB 6.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: dill\n",
            "  Running setup.py bdist_wheel for dill ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/e2/5d/17/f87cb7751896ac629b435a8696f83ee75b11029f5d6f6bda72\n",
            "Successfully built dill\n",
            "Installing collected packages: dill\n",
            "Successfully installed dill-0.2.8.2\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.24.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qHWOFxkWSubZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset with torchtext"
      ]
    },
    {
      "metadata": {
        "id": "vQn5WXMiSzae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "0d369823-9c9f-4512-fdc9-093e6a6a363b"
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset#, DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import torchtext.data as data\n",
        "from collections import namedtuple\n",
        "\n",
        "\n",
        "TimeExample = namedtuple('TimeExample',['src','sent_0','sent_1','sent_x','is_x_0','sent_0_target'])\n",
        "\n",
        "class TimeStyleDataset(Dataset):\n",
        "  \n",
        "  def __init__(self,max_id,time_mod_3_result): \n",
        "    \"\"\"\n",
        "    max_id how many samples are in this dataset. Size of one epoc!\n",
        "    time_mod_3_result - to make sure time is train/test/valid is different, pass 0,1,2\n",
        "    \"\"\"\n",
        "    self.max_id =int(max_id)\n",
        "    #TODO : add more!!# see here: https://docs.python.org/2/library/datetime.html month can be: %b,%B,%m , year: %y,%Y\n",
        "    self.formats = [\"%b %d %Y %H:%M:%S\", \"%m/%d/%Y %H:%M:%S\", \"%d-%b-%Y %H.%M.%S.00\",\n",
        "                    \"%B %d %y %H:%M:%S\" ,\"%B %d %H:%M:%S %Y\", \"%d/%B/%Y %H:%M:%S\"]\n",
        "    self.time_mod_3_result=time_mod_3_result\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.max_id\n",
        "  \n",
        "  def _tvt(self,idx):\n",
        "    # for 10, if train, return 9, valid 10, test 11\n",
        "    return (idx//3) * 3 + self.time_mod_3_result #10//3 * 3 + x = 9+x\n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "    if idx > self.max_id:\n",
        "      raise IndexError(f'TimeStyleDataset {idx} is out of range {self.max_id}')\n",
        "    \n",
        "    \"\"\" x -  x[0] semantic0   , style0\n",
        "             x[1] semantic0   , style1   \n",
        "             x[2] semantic0orX, style0 (1/2 the time 0 , half X)\n",
        "    \"\"\"\n",
        "    max_time= int(2e9)  #means 1970-2001\n",
        "    \n",
        "    random_ids = np.random.randint(low=0,high=max_time,size=2)\n",
        "    idx = random_ids[0] #other_idx is 50% same, 50% other\n",
        "    other_idx = random_ids[1] if np.random.randint(0,2)==0 else random_ids[0]\n",
        "\n",
        "    idx = self._tvt(idx)\n",
        "    other_idx = self._tvt(other_idx)\n",
        "   \n",
        "    # two random formats\n",
        "    random_fs= np.random.choice(len(self.formats),size=2,replace=False)\n",
        "    \n",
        "    sent_0= time.strftime(self.formats[random_fs[0]],time.gmtime(idx)) \n",
        "    sent_1= time.strftime(self.formats[random_fs[1]],time.gmtime(idx)) \n",
        "    sent_x= time.strftime(self.formats[random_fs[0]],time.gmtime(other_idx))\n",
        "    y = np.array([idx==other_idx],np.float32)\n",
        "    return TimeExample(sent_0,sent_0,sent_1,sent_x,y,sent_0)\n",
        "\n",
        "def test():\n",
        "  dataset = TimeStyleDataset(1e3,1)\n",
        "  for i in range(3):\n",
        "    sample = dataset[i] ; print (sample)  \n",
        "  print ('len',len(dataset))\n",
        "  \n",
        "test()\n",
        "\n",
        "\n",
        "from seq2seq.dataset import SourceField,TargetField\n",
        "TEXT = SourceField(batch_first =True,sequential=True,use_vocab=True, lower=False,\n",
        "                            tokenize=lambda x: list(x)) # fix_length=10\n",
        "TEXT_TARGET = TargetField(batch_first =True,sequential=True,use_vocab=True, lower=False,\n",
        "                            tokenize=lambda x: list(x)) # fix_length=10\n",
        "LABEL= data.Field(batch_first=True,sequential=False,use_vocab=False)\n",
        "\n",
        "fields = [('src',TEXT),('sent_0',TEXT),('sent_1',TEXT),('sent_x',TEXT),('is_x_0',LABEL),('sent_0_target',TEXT_TARGET)] \n",
        "ds = data.Dataset(TimeStyleDataset(1e3,1), fields)\n",
        "ds_eval = data.Dataset(TimeStyleDataset(1e3,2), fields)\n",
        "\n",
        "# usage\n",
        "print (ds[2].sent_0) #not processed\n",
        "print (ds[2].is_x_0) #not processed\n",
        "print (f'building vocab on {len(ds)}')\n",
        "TEXT.build_vocab(ds, max_size=80000)\n",
        "TEXT_TARGET.build_vocab(ds, max_size=80000)\n",
        "print (TEXT.vocab.freqs)\n",
        "# READ:  https://github.com/mjc92/TorchTextTutorial/blob/master/01.%20Getting%20started.ipynb\n",
        "train_iter = iter(data.BucketIterator( dataset=ds, batch_size=4)) \n",
        "eval_iter =  iter(data.BucketIterator( dataset=ds_eval, batch_size=4))\n",
        "#performance note: the first next, takes 3.5s, the next are fast (10000 is 1s)\n",
        "b= next(train_iter)\n",
        "print (b.sent_0[0].shape,b.sent_0[1].shape)#TEXT.reverse(b.sent_0))\n",
        "print ('b.src is values+len tuple',b.src[0].shape,b.src[1].shape)#TEXT.reverse(b.sent_0))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TimeExample(src='24/May/2007 16:19:37', sent_0='24/May/2007 16:19:37', sent_1='May 24 2007 16:19:37', sent_x='12/May/2005 17:37:52', is_x_0=array([0.], dtype=float32), sent_0_target='24/May/2007 16:19:37')\n",
            "TimeExample(src='16-Jan-2030 20.48.13.00', sent_0='16-Jan-2030 20.48.13.00', sent_1='16/January/2030 20:48:13', sent_x='16-Jan-2030 20.48.13.00', is_x_0=array([1.], dtype=float32), sent_0_target='16-Jan-2030 20.48.13.00')\n",
            "TimeExample(src='12/29/1980 07:19:10', sent_0='12/29/1980 07:19:10', sent_1='December 29 80 07:19:10', sent_x='12/29/1980 07:19:10', is_x_0=array([1.], dtype=float32), sent_0_target='12/29/1980 07:19:10')\n",
            "len 1000\n",
            "03-Dec-1973 00.08.07.00\n",
            "[1.]\n",
            "building vocab on 1000\n",
            "Counter({'0': 10381, '1': 9282, ' ': 8070, '2': 7714, ':': 6702, '9': 4091, '3': 3870, '4': 3347, '5': 3328, '7': 2678, '/': 2636, '8': 2593, 'e': 2167, '6': 2048, '.': 1959, 'r': 1743, 'u': 1345, '-': 1306, 'a': 1170, 'b': 970, 'J': 852, 'y': 757, 'c': 709, 't': 640, 'n': 589, 'A': 559, 'p': 554, 'M': 541, 'm': 496, 'o': 436, 'l': 430, 'F': 298, 'O': 288, 'g': 270, 'S': 265, 'D': 263, 'N': 260, 'v': 260, 's': 174, 'i': 167, 'h': 158})\n",
            "torch.Size([4, 23]) torch.Size([4])\n",
            "b.src is values+len tuple torch.Size([4, 23]) torch.Size([4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a66fouEILpSz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Seq2Seq Sanity"
      ]
    },
    {
      "metadata": {
        "id": "i9Q6pVYxLr7n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1832
        },
        "outputId": "20ab99a3-b5a3-49d4-a94d-04b59ba88ccf"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torchtext\n",
        "\n",
        "import seq2seq\n",
        "from seq2seq.trainer import SupervisedTrainer\n",
        "from seq2seq.models import EncoderRNN, DecoderRNN, Seq2seq\n",
        "from seq2seq.loss import Perplexity\n",
        "from seq2seq.optim import Optimizer\n",
        "from seq2seq.dataset import SourceField, TargetField\n",
        "from seq2seq.evaluator import Predictor\n",
        "\n",
        "\n",
        "\n",
        "# Prepare dataset\n",
        "src = SourceField()\n",
        "tgt = TargetField()\n",
        "max_len = 20\n",
        "src = TEXT\n",
        "tgt = TEXT_TARGET\n",
        "input_vocab = src.vocab\n",
        "output_vocab = tgt.vocab\n",
        "train = ds\n",
        "dev = ds_eval\n",
        "\n",
        "# NOTE: If the source field name and the target field name\n",
        "# are different from 'src' and 'tgt' respectively, they have\n",
        "# to be set explicitly before any training or inference\n",
        "seq2seq.src_field_name = 'sent_0'\n",
        "seq2seq.tgt_field_name = 'sent_0_target'\n",
        "\n",
        "# Prepare loss\n",
        "weight = torch.ones(len(tgt.vocab))\n",
        "pad = tgt.vocab.stoi[tgt.pad_token]\n",
        "loss = Perplexity(weight, pad)\n",
        "if torch.cuda.is_available():\n",
        "    loss.cuda()\n",
        "\n",
        "seq2seq = None\n",
        "optimizer = None\n",
        "# Initialize model\n",
        "hidden_size=128\n",
        "bidirectional = True\n",
        "encoder = EncoderRNN(len(src.vocab), max_len, hidden_size,\n",
        "                     bidirectional=bidirectional, variable_lengths=True)\n",
        "decoder = DecoderRNN(len(tgt.vocab), max_len, hidden_size * 2 if bidirectional else hidden_size,\n",
        "                     dropout_p=0.2, use_attention=True, bidirectional=bidirectional,\n",
        "                     eos_id=tgt.eos_id, sos_id=tgt.sos_id)\n",
        "seq2seq = Seq2seq(encoder, decoder)\n",
        "if torch.cuda.is_available():\n",
        "    seq2seq.cuda()\n",
        "\n",
        "for param in seq2seq.parameters():\n",
        "    param.data.uniform_(-0.08, 0.08)\n",
        "\n",
        "# Optimizer and learning rate scheduler can be customized by\n",
        "# explicitly constructing the objects and pass to the trainer.\n",
        "#\n",
        "# optimizer = Optimizer(torch.optim.Adam(seq2seq.parameters()), max_grad_norm=5)\n",
        "# scheduler = StepLR(optimizer.optimizer, 1)\n",
        "# optimizer.set_scheduler(scheduler)\n",
        "\n",
        "# train\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')\n",
        "\n",
        "t = SupervisedTrainer(loss=loss, batch_size=32,\n",
        "                    checkpoint_every=50,\n",
        "                    print_every=10, expt_dir='.')\n",
        "\n",
        "seq2seq = t.train(seq2seq, train,\n",
        "                  num_epochs=20, dev_data=dev,\n",
        "                  optimizer=optimizer,\n",
        "                  teacher_forcing_ratio=0.5,\n",
        "                  resume=False,)\n",
        "\n",
        "predictor = Predictor(seq2seq, input_vocab, output_vocab)\n",
        "seq_str = \"November 21 77 14:07:40\" #raw_input(\"Type in a source sequence:\")\n",
        "seq = seq_str.strip().split()\n",
        "''.join(predictor.predict(\"NoveMBer 21 77 14:07:40\"))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "11:49:04 INFO:Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            "), Scheduler: None\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "11:49:06 INFO:Progress: 3%, Train Perplexity: 52.3218\n",
            "11:49:07 INFO:Progress: 4%, Train Perplexity: 16.1965\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:321: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train), lengths\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n",
            "11:49:09 INFO:Finished epoch 1: Train Perplexity: 22.4753, Dev Perplexity: 16.6471, Accuracy: 0.1384\n",
            "11:49:10 INFO:Progress: 6%, Train Perplexity: 16.2952\n",
            "11:49:11 INFO:Progress: 7%, Train Perplexity: 16.3324\n",
            "11:49:12 INFO:Progress: 9%, Train Perplexity: 15.3651\n",
            "11:49:14 INFO:Finished epoch 2: Train Perplexity: 16.2093, Dev Perplexity: 14.8108, Accuracy: 0.1441\n",
            "11:49:15 INFO:Progress: 10%, Train Perplexity: 17.5116\n",
            "11:49:16 INFO:Progress: 12%, Train Perplexity: 16.5808\n",
            "11:49:17 INFO:Progress: 14%, Train Perplexity: 15.9468\n",
            "11:49:19 INFO:Finished epoch 3: Train Perplexity: 15.9596, Dev Perplexity: 13.9061, Accuracy: 0.1912\n",
            "11:49:20 INFO:Progress: 15%, Train Perplexity: 13.7106\n",
            "11:49:21 INFO:Progress: 17%, Train Perplexity: 12.8190\n",
            "11:49:21 INFO:Progress: 18%, Train Perplexity: 11.0532\n",
            "11:49:24 INFO:Finished epoch 4: Train Perplexity: 11.7235, Dev Perplexity: 11.5441, Accuracy: 0.2262\n",
            "11:49:24 INFO:Progress: 20%, Train Perplexity: 10.4254\n",
            "11:49:25 INFO:Progress: 21%, Train Perplexity: 9.8445\n",
            "11:49:26 INFO:Progress: 23%, Train Perplexity: 9.3247\n",
            "11:49:27 INFO:Progress: 25%, Train Perplexity: 9.1270\n",
            "11:49:29 INFO:Finished epoch 5: Train Perplexity: 9.5244, Dev Perplexity: 9.1211, Accuracy: 0.2686\n",
            "11:49:30 INFO:Progress: 26%, Train Perplexity: 7.5330\n",
            "11:49:31 INFO:Progress: 28%, Train Perplexity: 7.7999\n",
            "11:49:32 INFO:Progress: 29%, Train Perplexity: 6.8102\n",
            "11:49:34 INFO:Finished epoch 6: Train Perplexity: 7.3167, Dev Perplexity: 7.8059, Accuracy: 0.3247\n",
            "11:49:35 INFO:Progress: 31%, Train Perplexity: 6.0094\n",
            "11:49:36 INFO:Progress: 32%, Train Perplexity: 6.0785\n",
            "11:49:37 INFO:Progress: 34%, Train Perplexity: 6.1512\n",
            "11:49:40 INFO:Finished epoch 7: Train Perplexity: 6.1961, Dev Perplexity: 7.2211, Accuracy: 0.3337\n",
            "11:49:40 INFO:Progress: 35%, Train Perplexity: 5.4631\n",
            "11:49:41 INFO:Progress: 37%, Train Perplexity: 4.2208\n",
            "11:49:42 INFO:Progress: 39%, Train Perplexity: 3.6121\n",
            "11:49:44 INFO:Finished epoch 8: Train Perplexity: 3.9649, Dev Perplexity: 4.5853, Accuracy: 0.5196\n",
            "11:49:45 INFO:Progress: 40%, Train Perplexity: 4.0391\n",
            "11:49:46 INFO:Progress: 42%, Train Perplexity: 4.5005\n",
            "11:49:47 INFO:Progress: 43%, Train Perplexity: 2.6421\n",
            "11:49:49 INFO:Finished epoch 9: Train Perplexity: 3.6212, Dev Perplexity: 2.6296, Accuracy: 0.7193\n",
            "11:49:50 INFO:Progress: 45%, Train Perplexity: 3.4464\n",
            "11:49:51 INFO:Progress: 46%, Train Perplexity: 1.9612\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11:49:52 INFO:Progress: 48%, Train Perplexity: 2.1860\n",
            "11:49:53 INFO:Progress: 50%, Train Perplexity: 1.8706\n",
            "11:49:55 INFO:Finished epoch 10: Train Perplexity: 2.1273, Dev Perplexity: 1.9786, Accuracy: 0.8401\n",
            "11:49:56 INFO:Progress: 51%, Train Perplexity: 1.6025\n",
            "11:49:57 INFO:Progress: 53%, Train Perplexity: 2.6599\n",
            "11:49:58 INFO:Progress: 54%, Train Perplexity: 2.2558\n",
            "11:50:00 INFO:Finished epoch 11: Train Perplexity: 2.1167, Dev Perplexity: 1.6357, Accuracy: 0.9004\n",
            "11:50:01 INFO:Progress: 56%, Train Perplexity: 1.4109\n",
            "11:50:02 INFO:Progress: 57%, Train Perplexity: 1.4681\n",
            "11:50:03 INFO:Progress: 59%, Train Perplexity: 1.0850\n",
            "11:50:05 INFO:Finished epoch 12: Train Perplexity: 1.2900, Dev Perplexity: 1.3348, Accuracy: 0.9541\n",
            "11:50:06 INFO:Progress: 60%, Train Perplexity: 1.0690\n",
            "11:50:06 INFO:Progress: 62%, Train Perplexity: 1.3512\n",
            "11:50:07 INFO:Progress: 64%, Train Perplexity: 2.4706\n",
            "11:50:10 INFO:Finished epoch 13: Train Perplexity: 1.5873, Dev Perplexity: 1.0415, Accuracy: 0.9936\n",
            "11:50:10 INFO:Progress: 65%, Train Perplexity: 1.0105\n",
            "11:50:11 INFO:Progress: 67%, Train Perplexity: 1.0096\n",
            "11:50:12 INFO:Progress: 68%, Train Perplexity: 1.0124\n",
            "11:50:15 INFO:Finished epoch 14: Train Perplexity: 1.0098, Dev Perplexity: 1.0248, Accuracy: 0.9949\n",
            "11:50:15 INFO:Progress: 70%, Train Perplexity: 1.0149\n",
            "11:50:16 INFO:Progress: 71%, Train Perplexity: 1.0906\n",
            "11:50:17 INFO:Progress: 73%, Train Perplexity: 1.0405\n",
            "11:50:18 INFO:Progress: 75%, Train Perplexity: 1.0097\n",
            "11:50:20 INFO:Finished epoch 15: Train Perplexity: 1.0465, Dev Perplexity: 1.5775, Accuracy: 0.9534\n",
            "11:50:21 INFO:Progress: 76%, Train Perplexity: 1.1587\n",
            "11:50:22 INFO:Progress: 78%, Train Perplexity: 1.0310\n",
            "11:50:23 INFO:Progress: 79%, Train Perplexity: 1.1202\n",
            "11:50:26 INFO:Finished epoch 16: Train Perplexity: 1.0979, Dev Perplexity: 1.0074, Accuracy: 0.9993\n",
            "11:50:26 INFO:Progress: 81%, Train Perplexity: 1.0045\n",
            "11:50:27 INFO:Progress: 82%, Train Perplexity: 1.0016\n",
            "11:50:28 INFO:Progress: 84%, Train Perplexity: 1.0007\n",
            "11:50:31 INFO:Finished epoch 17: Train Perplexity: 1.0011, Dev Perplexity: 1.0009, Accuracy: 0.9998\n",
            "11:50:31 INFO:Progress: 85%, Train Perplexity: 1.0005\n",
            "11:50:32 INFO:Progress: 87%, Train Perplexity: 1.0009\n",
            "11:50:33 INFO:Progress: 89%, Train Perplexity: 1.0101\n",
            "11:50:35 INFO:Finished epoch 18: Train Perplexity: 1.0036, Dev Perplexity: 1.0044, Accuracy: 0.9996\n",
            "11:50:36 INFO:Progress: 90%, Train Perplexity: 1.0002\n",
            "11:50:37 INFO:Progress: 92%, Train Perplexity: 1.0079\n",
            "11:50:37 INFO:Progress: 93%, Train Perplexity: 1.0046\n",
            "11:50:40 INFO:Finished epoch 19: Train Perplexity: 1.0042, Dev Perplexity: 1.0001, Accuracy: 1.0000\n",
            "11:50:41 INFO:Progress: 95%, Train Perplexity: 1.0007\n",
            "11:50:42 INFO:Progress: 96%, Train Perplexity: 1.0010\n",
            "11:50:43 INFO:Progress: 98%, Train Perplexity: 1.0030\n",
            "11:50:43 INFO:Progress: 100%, Train Perplexity: 1.0004\n",
            "11:50:45 INFO:Finished epoch 20: Train Perplexity: 1.0014, Dev Perplexity: 1.0001, Accuracy: 1.0000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['7', '/', '4', '6', ':', '0', '6', ':', '4', '6', ':', '4', '6', ':', '4', '6', ':', '4', '9', ':']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "efa1qLb_QXOo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "2fe66e9f-96ad-446a-aa23-e69681d5382c"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ovember 21 77 14:07:'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "metadata": {
        "id": "PdoVxGFbP-GR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "oX4qPrM7bjUn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's implement another Time\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QnDXClYdBhZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0e8f1008-815f-4743-8a1f-1c4d770251bc"
      },
      "cell_type": "code",
      "source": [
        "'''from torch.utils.data import Dataset#, DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "class TimePairsDataset(Dataset):\n",
        "  \n",
        "  def __init__(self,length,time_mod_3_result): #1e9 means 1970-2001\n",
        "    \"\"\"\n",
        "    time_mod_3_result - to make sure time is train/test/valid is different, pass\n",
        "    a different result here.\n",
        "    \"\"\"\n",
        "    #super().__init__()\n",
        "    self.length = length\n",
        "    self.formats = [\"%b %d %Y %H:%M:%S\", \"%m/%b/%Y %H:%M:%S\"] # STAY WITH 2 STYLES, DISCRIMINATOR SIMPLER #\"%d %b %Y %H:%M:%S\",\n",
        "    self.time_mod_3_result=time_mod_3_result\n",
        "  \n",
        "  \n",
        "  def __len__(self):\n",
        "    return int(self.length)\n",
        "  \n",
        "  def _choose(self,idx,style):\n",
        "    st = time.strftime(self.formats[style],time.gmtime(idx))\n",
        "    return np.array([ord(c) for c in list(st)][:24],np.float32)/120 #chars are up to that\n",
        "  \n",
        "  def untokenize_sample(self,sample):\n",
        "    \"\"\" sample x and y, x contains multiple sentences\"\"\"\n",
        "    x,y = sample\n",
        "    out=[] \n",
        "    for tokens in x:    \n",
        "      out.append(self.untokenize_tokens(tokens) )\n",
        "    return out,y\n",
        "\n",
        "  def untokenize_tokens(self,tokens):\n",
        "    \"\"\" one sentence\"\"\"\n",
        "    if type(tokens)==torch.Tensor:\n",
        "      tokens = tokens.detach().numpy()\n",
        "    return ''.join([chr(int(round(token*120))) for token in tokens])\n",
        "  \n",
        "  \n",
        "  def _tvt(self,idx):\n",
        "    # for 10, if train, return 9, valid 10, test 11\n",
        "    return (idx//3) * 3 + self.time_mod_3_result #10//3 * 3 + x = 9+x\n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "    \"\"\" x -  x[0] semantic0   , style0\n",
        "             x[1] semantic0   , style1   \n",
        "             x[2] semantic0orX, style0 (1/2 the time 0 , half X)\n",
        "    \"\"\"\n",
        "    \n",
        "    #other_idx is 50% same, 50% other\n",
        "    other_idx = (idx + np.random.randint(0,2)*(np.random.randint(10,self.length))) %self.length\n",
        "    #print (idx,other_idx)\n",
        "    idx = self._tvt(idx)\n",
        "    other_idx = self._tvt(other_idx)\n",
        "    #print (idx,other_idx)\n",
        "    \n",
        "    # two random formats\n",
        "    random_fs= np.random.choice(len(self.formats),size=2,replace=False)\n",
        "    #print (random_fs)\n",
        "    x_list = []\n",
        "    x_list.append(self._choose(idx,random_fs[0]))\n",
        "    x_list.append(self._choose(idx,random_fs[1]))\n",
        "    x_list.append(self._choose(other_idx,random_fs[0]))\n",
        "    \n",
        "    y = np.array([idx==other_idx],np.float32)\n",
        "    \n",
        "   \n",
        "    return  (np.vstack(x_list), y)\n",
        "\n",
        "def test():\n",
        "  dataset = TimePairsDataset(1e9,1)\n",
        "  sample = dataset[9] ;#print (sample)\n",
        "  print (dataset.untokenize_sample(sample))\n",
        "  print ('shapes x,y',sample[0].shape,sample[1].shape)\n",
        "  \n",
        "test()\n",
        "'''"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from torch.utils.data import Dataset#, DataLoader\\nimport numpy as np\\nimport time\\n\\nfrom torch.utils.data import Dataset, DataLoader\\nimport numpy as np\\n%matplotlib inline\\n\\nclass TimePairsDataset(Dataset):\\n  \\n  def __init__(self,length,time_mod_3_result): #1e9 means 1970-2001\\n    \"\"\"\\n    time_mod_3_result - to make sure time is train/test/valid is different, pass\\n    a different result here.\\n    \"\"\"\\n    #super().__init__()\\n    self.length = length\\n    self.formats = [\"%b %d %Y %H:%M:%S\", \"%m/%b/%Y %H:%M:%S\"] # STAY WITH 2 STYLES, DISCRIMINATOR SIMPLER #\"%d %b %Y %H:%M:%S\",\\n    self.time_mod_3_result=time_mod_3_result\\n  \\n  \\n  def __len__(self):\\n    return int(self.length)\\n  \\n  def _choose(self,idx,style):\\n    st = time.strftime(self.formats[style],time.gmtime(idx))\\n    return np.array([ord(c) for c in list(st)][:24],np.float32)/120 #chars are up to that\\n  \\n  def untokenize_sample(self,sample):\\n    \"\"\" sample x and y, x contains multiple sentences\"\"\"\\n    x,y = sample\\n    out=[] \\n    for tokens in x:    \\n      out.append(self.untokenize_tokens(tokens) )\\n    return out,y\\n\\n  def untokenize_tokens(self,tokens):\\n    \"\"\" one sentence\"\"\"\\n    if type(tokens)==torch.Tensor:\\n      tokens = tokens.detach().numpy()\\n    return \\'\\'.join([chr(int(round(token*120))) for token in tokens])\\n  \\n  \\n  def _tvt(self,idx):\\n    # for 10, if train, return 9, valid 10, test 11\\n    return (idx//3) * 3 + self.time_mod_3_result #10//3 * 3 + x = 9+x\\n  \\n  def __getitem__(self,idx):\\n    \"\"\" x -  x[0] semantic0   , style0\\n             x[1] semantic0   , style1   \\n             x[2] semantic0orX, style0 (1/2 the time 0 , half X)\\n    \"\"\"\\n    \\n    #other_idx is 50% same, 50% other\\n    other_idx = (idx + np.random.randint(0,2)*(np.random.randint(10,self.length))) %self.length\\n    #print (idx,other_idx)\\n    idx = self._tvt(idx)\\n    other_idx = self._tvt(other_idx)\\n    #print (idx,other_idx)\\n    \\n    # two random formats\\n    random_fs= np.random.choice(len(self.formats),size=2,replace=False)\\n    #print (random_fs)\\n    x_list = []\\n    x_list.append(self._choose(idx,random_fs[0]))\\n    x_list.append(self._choose(idx,random_fs[1]))\\n    x_list.append(self._choose(other_idx,random_fs[0]))\\n    \\n    y = np.array([idx==other_idx],np.float32)\\n    \\n   \\n    return  (np.vstack(x_list), y)\\n\\ndef test():\\n  dataset = TimePairsDataset(1e9,1)\\n  sample = dataset[9] ;#print (sample)\\n  print (dataset.untokenize_sample(sample))\\n  print (\\'shapes x,y\\',sample[0].shape,sample[1].shape)\\n  \\ntest()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "metadata": {
        "id": "zDrO9682vv7k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        " \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mx_wvDraYghh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# models define"
      ]
    },
    {
      "metadata": {
        "id": "nMc6XeMxWxha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Documentation"
      ]
    },
    {
      "metadata": {
        "id": "wnsebPEMW0d4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "786674e3-f1ee-4e7b-a1b6-7762451d81f6"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# semantic_encoder : take sentence return a vector with only semantics\n",
        "# semantic similiary objective: given a pair of sentences with the same semantics,\n",
        "#    expect their results to be close (MeanSquared)    \n",
        "\n",
        "# see idea from https://github.com/edenton/drnet-py/blob/master/models/classifiers.py\n",
        "# in her train method:\n",
        "# FRAMES: VID A: FRAME 1 x_c1 , ?x_p2? (1/2 of times)\n",
        "#                FRAME 2 x_c2 , x_p1\n",
        "#         VID B: FRAME 1      , ?x_p2? (1/2 of times)\n",
        "#  \n",
        "# x_p1,x_p2 chosen randonly from same/not-same video\n",
        "# x_c1,x_c2 are always same-video\n",
        "# c1 and p1 are same content, different pose.\n",
        "\n",
        "# train_scene_discriminator()\n",
        "# h_p1,h_p2 = netEP applied to x[0],x[1]. assumption: SAME VIDEO\n",
        "# important: detach both!\n",
        "# override half of h_p2 by random-permutations of the batch. \n",
        "#   [1,2,3,4,5,6]\n",
        "#   [2,3,1,4,5,6] after 1st half permute\n",
        "#   [1,1,1,0,0,0] set unequal the labels (1=unequal, 0 equal. does it matter? should it be 0.9,0.1?)\n",
        "# apply BCE on inpit: concat of [h_p1,h_p2] \n",
        "# run backward, and optimizer on the netC classifier ONLY! emphasize! not on the encoder\n",
        "\n",
        "\n",
        "# train()\n",
        "# h_c1,h_c2 = netEC(x_c1), netEC(x_c2)  where input is x[0],x[1] sim loss is MSE directly on the hidden content-semantics\n",
        "# h_p1,h_p2 = netEP(x_p1),netEP(x_p2) where input is x[2],x[3]\n",
        "# rec = netD([h_c1, h_p1]) h_c1 is DIFFERENT FRAME , but same content, than h_p1\n",
        "# netC is the semantic-discriminator given h_p1,h_p2, target 0.5 (max-entropy). \n",
        "# emphasize! don't optimize netC is this stage\n",
        "\n",
        "\n",
        "# ANTI-ADVERSERTIAL LOSS FUNCTION, target is 0.5, so min value is 0.6931471805599453\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "x = np.arange(1,100)/100.0\n",
        "plt.title(\"anti adv-loss with target of 0.5. min is 0.693147 \")\n",
        "plt.plot(x, -0.5*(np.log(x)+np.log(1-x)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f83215e6e10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEHCAYAAACzy817AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl8HPV5+PHPrlb3bWllybZ821+M\nL7A5bIwNGMeADTRcSUtIQgJpDpImaX9Jf22aNMevJU1CCYEmgbYJJU1IgIQr5gaDOWzwjc/Hpyzf\nkqz71h6/P2ZkL0Jaray9ZvW8Xy+/vLszO/t8Z1fPfOeZ78y4gsEgSimlnMud6ACUUkoNjyZypZRy\nOE3kSinlcJrIlVLK4TSRK6WUw2kiV0oph/MkOoBUYYwxwGgRWWOMuQG4TkQ+O4zl/RdwRES+G+H8\nE4F9IpLQ79QY8yrwDRHZZIz5nIj8p/3668B/icj/DvL+0+sx9tFCaIxDeM9fAL8EnhKRL/aZNhf4\nBVAK1AFfEJH3+1lGFeAHenpfE5Fzhhr/APF9GWsdfnsI73kEeFxEno1w/gzg58ASrHb8QkR+NsC8\nNwA/AtKAzcBnRKTZGDMVeBAYD7QDt4vIZvs9NwHfBrI4sx6329PKgN8Ck0Rkaj+f5wbWArtE5HZj\nzELg131mmwLME5FtkbQ32Wkij54bsNbnGhF5EngywfEkhIhcCWCMSQN+DAwpSRKyHqMc2ocMI8br\nsTZK/SXK3wP/ICJPGWOux0o4swdYzpUiUjXEzx6UiDxwFu/51BDf8rfAKOAcIA/YYox5R0Q2hM5k\njJmElfAXA/uBe4Frgd9hrZvfiMgDxpirgMeMMdOBSqwN5QUicsgY81XgV8BFxphRwBvA88CkAWL7\nIjAa2GW3ba0dZ29MFwP3A9uH2OakpYm8H8aYO4G/w1o/x4FP2j+o24GVQDPWD9MH3AJMBv4B6DbG\nFAPbgNtEZFk/y/42cJu97F32fI3GmBLgUWAasBOrh3LEGPMjIFtEvmK/vxQ4BIwRkaYB4ncDPwBu\nsl9aB9wlIm3GmFuAf8bqHfUAfyMirw/0esgypwBviMg4+/kvgDkissh+/gzwMPDvdvu+DxQaY3YD\n19iLmWT3zKdhJepPiEgg5DOuC12PIvJ3YdbXd4GxwFyspPBL4BFgEbAD2ASU2z2ycVi9ZGN/1FdF\n5Hng5dAYReTgYOsQuBO42Y6xXEQ+F/Ke2UCRiDwFICLPGGP+0xgzQ0R29fddDYX9+7sW6ML6/QnW\nev43rB7mt0XkIXvdjBORO+31/QxwI1biWwPcKiLBPst+HXuPyRjz/7B+1y7gCNY6P9YnnFuAb9nf\nX7Mx5gn7tQ195rsN+KOI7LOff83+vALgImApgIi8aIzpwfo+T9oxHrLf86rdToAg8FGgAmuD2ncd\nVQBfwdpgnN/vioT7gL/ruw6cTGvkfdi7bQ8AHxGRacA+rF28XiuAn4vIdGA18DV7d/RJ4D4R+bsw\ny54PfBm4ECuZZdrPAf4eqBWRSVgJ4yr79SeA60IWcx3w6kBJ3PYxrOQ5H5gJFAFft6f9HFgpIjOA\nL3Hmj2Gg1wEQkf1AwBhTab80H8gwxmQaY1zAQnt99Pos4BeRc0IS5OV2XAa4Aivphn7GB9bjIOsL\nrO9ihYj8FCvBjgEmAJ8DPhMy3/8AW+zvbAXwv/aGs78Yw65DEbkvJMbP9XnPdOBAn9cOENIb7OPH\nxphtxpj1du89ElcB38NaHzOAb2Al9Tv44O801HXAR+z4lgKXDLRwY8xMrLbPstfXk8CHOiT2svaH\nPN9P/+2ci7XRe9kYs8cY80tjTA5WQoYP5qBWYKqIHBeRl+14PMDtwNMAItIgIjJQ/MBPsdbPQJ2c\nlUCHiLwZZhmOo4m8DxGpAQpE5Ij90ptYPe5eO0Vko/14E1Z9L9JlbwQqRaTZ7sm8E7LsJcBj9nxV\nWLuPiMh7gMuuvYJVenhskI9aCfyPiLSJiB+rPrjcnlYDfMEYM0FE3hKRvx3k9VCrgYV2EuwEtgAX\nAOcCh0SkYZC4/igiHSLSCuwFxoWbeZD1BfCuiNTZjxcDT4iIz+7JrQIwxuRibTTutZe5D+s7XTlI\nrOHW4UBysNZLqA4gt595fw/8h4jMxipT/K9dMx7MThHZIyJdWOvwJTu+bVgbsv48Ya/3NmAP4X+z\njYAX+IS9V3S/iDzSz3x92zpQO4uwNiKfwOohTwH+UURagHeBvzXGuIwxy4BZWDVxAOySykms7/bv\nw8TcO//VQLGIPBpmtm8CPxlsWU6jibwPu276fWPMTmOMAP/CB9dT6Jbej1WKiHTZOcD9xhixl/2l\nkGWP6rPs0KT4R+B6OyldCjxtjLnIGLPb/nd3n4/y9nl/A1BmP74eKAc2GmM2G2MuG+T1UKuxet6L\nsQ4mrcXqVS/G2v0dTHPI40HX3SDrC6A+5HFxn+dH7f8LsUoE7/SuL6yNT9EgsYZbhwNpIyQR2XKw\nepofICL/t7d0ZfcOX2fwDQVAS8hjf8iy/Qz89xzxb1ZEjmKVYW4Bqo0xq0L2wkL1bWu/7bQ/+ykR\nqbE3JL/gTDs/gfX7Eay9gLewNiS9sdyHddD4p1jfX/ZAcdvTfoL1GxlonnFYG4sXBprHqbRG/mEf\nx0pqS0SkzhjzOawfXDR8DWuXeL6ItBpj/gWrzgtWoigMmdfLmd30J7Dqejuw6tQtwHt88ADOxJD3\nngRKQp6X2K/1lkg+Y9eAP4VVXx470Ot94l8NfAEIYO0x7MGqz7Zg1aejLdz66qsZ66Bbrwr7/xqs\n5HWBvSdwWp911teA6zCM3Vg9zt7lu4CpWMc8Qj83E6uEsCPkZQ8hI1gSSURWA6vtjsNPgB/y4b+B\n3Vht22s/7z2209chPvi79tv/en+Lpzdexpj9wDZjzAys3+Qrdh37UWPMA1gluS0DhD0faw/vLWvg\nE9lYpT+viPTufa0EXrb3YFKK9sg/rAyospN4CVZPIW+Q94D1RzhYL68M2G0npQlY9dreZa/FKpv0\nHli8NOR9a7GOwt/O4GUVgD8Dtxljcuwa4x3AKmOM165VFtilinVAcKDX+y7ULlkUYZUq3sHqSU3H\n+iN6q8/sPYDbGJMfQbx939e7HsOtr77eA24yxrjtHuQ1dsw+rDLLF8Dq5RtjfmXPEy7GftdhuMBF\nZCdQa4y51X7p01glpz19Zs0B1hprWFzvQdJFwCvhlh8Pxpjlxpj/MMa47R70Vvr5LWD9Dr9ijEmz\nDzD+JfCHAeb7uDFmnL23ewd2O40xzxhrmCHGmE8C1fZvzAs8YowZY09bBKTz4eMPp9nlwCIRKReR\ncuCrwB9CkjhY9fphH3RORprIP+xRoMQYs89+/E9ApTHmnkHe9yxWjfmJMPP8ErjMLhPcg1UbvdIY\n8zXgbmCCMeYg1tCoP/W+ye6VPIV10CmScb5PAM8BG7GGWB0GfiYitVi7leuNMTux6rR3DPT6AMt+\nG8gRkTo7rgPASRFp7zPfcazkXm2MGfDgWj9C12O49dXXL7FqtvuB/7Db0JuAvmgvZzfWcY0DInJ4\nkBj7XYcRxH8r8DfGmL1YB2BP92Tt0s5o+1jCx4Bf2m17BGtkyEF7vleNMfMi+KxYWIO1odljjNmB\ntYf6nX7muw84hrUxXw18X0S2Ahhj7jbGfAFARNYB38Vaz7uxxoT/0F7Gv2GVMQ9gHXj+lP2eNVgl\nzVfs7+znwF+KNfb8Ovu1R4Dx9jqNpKwHVo/9RMRrwkFcej1ylSqMMa7eIWXGmB8DHhH5+iBvU8rx\ntEeuUoKxhu+tN9ZwyDyseujaBIelVFzowU6VKlZh1dB3YR2M/TNWeUSplKelFaWUcjgtrSillMPF\nvbRSW9sypF2A4uIcGhr6DohIfSO13TBy267tHlmG2m6vN9810LSk75F7PBGfOJlSRmq7YeS2Xds9\nskSz3UmfyJVSSoWniVwppRxOE7lSSjmcJnKllHI4TeRKKeVwmsiVUsrhNJErpZTDOSaRn2rq5PHX\n99HVk3LXhFdKjQCr1lax/1i4W+2ePcck8i376nh+XTU7q+oHn1kppZJIfXMnf3zjAK9vPjr4zGfB\nMYk8K8M6C6qlPSnuhqWUUhHrzVvZGbG5KopjEnledjoAbR2ayJVSztLaaeWt3jwWbY5L5K2ayJVS\nDtPbAc3VRK6JXCnlTL15a8T3yHM1kSulHEoTuS0ny4PLpTVypZTzaCK3uV0ucrPSae30JToUpZQa\nkjM18hE+agWs8oqWVpRSTtPaYXVAR3yPHCAv20NbRw96w2illJO0dvTgSXORmR6buyFF1M83xvwI\nWGzPf7eI/Clk2hXA3YAfEOBOEQnEIFbystLxB4J0dvvJzoz77UaVUuqstHX0kJudjss14G03h2XQ\nHrmdqGeJyELgauCnfWZ5CLhZRBYB+fY8MaFDEJVSTtTa0ROzsgpEVlpZA9xiP24Eco0xofsH80Xk\niP24FiiJYnwfoEMQlVJO4w8EaO/ykZcVu0Q+aH1CRPxAm/30DuA5+7Xe6c0AxpgKYDnw7XDLKy7O\nGfLdo73efABGl+ZZQWekn34tlY2ENg5kpLZd2516mlq7ABhVlP2hdkar3REXmo0xf4GVyJf3M60M\neBb4koicCrechob2IQXo9eZTW9sCgCtgld6PnmiisiR7SMtxmtB2jzQjte3a7tR0/JTVD053uz7Q\nzqG2O1zSj/Rg51XAt4CrRaSpz7QC4HngWyLyUsRRnQWtkSulnCbWJwNBBIncGFMI/BhYJiL9XQz8\nHuBeEXkh2sH1pTVypZTTJEUiBz4OlAKPGWN6X3sN2Aa8CHwKmGaMudOe9jsReSjagULopWz17E6l\nlDO0xvisTojsYOdDWEMMB5IZvXDC603kLR3d8fpIpZQalnj0yB13ZifohbOUUs6hibyPdE8aGenu\n09ctUEqpZNemifzD8vTCWUopB+nteMbq7kDgxESelX76/ndKKZXsTh/szIrdwU7HJfLc7HS6uv34\n/DG5LpdSSkVVW0cPOZke0tyxS7eOS+R6UpBSyklifcEs0ESulFIxEwwGabUvYRtLjkvkuadPCtJE\nrpRKbp3dfvyBoPbI+9IeuVLKKc4MPYztjXAcmMitFaKJXCmV7HpH2GlppQ/tkSulnCIeZ3WCAxN5\nrl44SynlEJrIB6A9cqWUU/R2ODWR96GJXCnlFNojH0B2pgeXCz1NXymV9DSRD8DtcpGbla7jyJVS\nSS8eVz6EyO/Z+SNgsT3/3SLyp5Bpy4B/BfzAcyLyg1gEGkqvgKiUcoIzdwdKcI/cGHMFMEtEFgJX\nAz/tM8vPgJuARcByY8y5UY+yj7zsdNo6fASDwVh/lFJKnbXWjh7SPW4y09Ni+jmRlFbWALfYjxuB\nXGNMGoAxZjJQLyKHRSQAPAdcGZNIQ+RlpxMIBuno0iGISqnkFY8LZkFk9+z0A2320zuwyid++3k5\nUBsyew0wJdzyiotz8HiGtnXyevM/8LykOBuAjOxMvKW5Q1qWk/Rt90gyUtuu7U4t7V0+yopzBmxf\ntNod8QUAjDF/gZXIl4eZzTXYchoa2iP9SMBqaG1tywde89ifUn20EU8wNa9L3l+7R4qR2nZtd2rx\n+QO0d/rISnf3276htjtc0o/0YOdVwLeAq0WkKWTSMaxeea+x9msxpWPJlVLJrq0zPicDQWQHOwuB\nHwPXikh96DQRqQIKjDETjTEe4FrgpVgEGkovZauUSnbxGkMOkfXIPw6UAo8ZY3pfew3YJiJPAl8E\nHrVf/4OI7Il6lH0U5mYA0NjaFeuPUkqps9KbnwrsfBVLkRzsfAh4KMz0NcDCaAY1GG+hdbCztqkz\nnh+rlFIRq2vsAMBblB3zz3LcmZ0AJYVZwJkVpZRSyabO7miW2vkqlhyZyLMzPeRlp2uPXCmVtGrt\njmZpofbIB1RamMWppg4CenanUioJ1TV1kuZ2UZyfGfPPcm4iL8rG5w/S1Nqd6FCUUupD6ho7KCnI\nwu0e9PSaYXNsIvfadadarZMrpZJMV4+f5vYeSotiXx8HByfyUvtIcF2TJnKlVHI5c6Az9vVxcHAi\n7+2R1+kBT6VUkjkz9FB75GGd7pE3aiJXSiUX7ZFHqKSgt0eupRWlVHI5PfRQe+ThpXvcFOVlUKs9\ncqVUktEe+RCUFmVT39KJz5+al7JVSjlTXVMHGeluCnJif8EscHgi9xZmEQxCfYtePEsplTzqGjsp\nLczG5Yr9GHJweCLv3W3Ra64opZJFe2cP7V2+uFxjpZezE3mRDkFUSiWX3uN23jjVx8Hhibx3RenI\nFaVUsujNR/EasQKR3+ptFvA0cK+IPNBn2l3AbYAf2CAiX4t6lAM43SPXkStKqSTR2yOP14gViOxW\nb7nA/cCr/UwrAL4BLBaRS4FzjTELoh7lAIrzM0lzu6jVHrlSKkmcsku98TqrEyIrrXQBK+j/psrd\n9r88+56dOUB9P/PFRJrbTXF+pvbIlVJJo7djGc+DnZHc6s0H+ELu1xk6rdMY8z3gANAB/H6we3YW\nF+fg8aQNKUivN3/AaWO8eby/r46Cohwy04e23GQXrt2pbqS2XdvtfA2t3eRmpzOhctSg80ar3RHV\nyAdil1b+EZgONAOvGWPmisjWgd7T0NA+pM/wevOprW0ZcHqhPeB+975axpTmDmnZyWywdqeykdp2\nbbfzBYNBTta3UT4qZ9A2DbXd4ZL+cEetzAAOiEidiHQDbwLzh7nMIdHL2SqlkkVzew/dPYG4Dj2E\n4SfyKmCGMaY36guAvcNc5pCcucGE1smVUolVF+eLZfUatLRijJkP3ANMBHqMMTcDzwAHReRJY8yP\ngdXGGB/wjoi8GcuA+yorzgHgZP3QSjZKKRVtJ+w8VFYU3x55JAc7NwKXh5n+IPBgFGMakrGlubiA\nwzWtiQpBKaWAM3loXFleXD/X0Wd2AmRmpFFWnM3hmlaCwWCiw1FKjWCnE7lXE/mQVZbl0d7lo0Gv\ngqiUSpBgMMjhmla8RVlkZw5rQOCQpUwiB6jW8opSKkGa2rpp7eihsiz+Y+JTIpH31qO0Tq6USpQz\nZZX4n8+SEom8UhO5UirBevOP9sjPUklBFjmZHk3kSqmEOZ3IR8f3QCekSCJ3uVyMK8ujpr6drm5/\nosNRSo1Ah2taycpIi+vFsnqlRCIHq7wSBI7WtSU6FKXUCNPj83PiVDvjyvJwx+k+naFSKpEDHK5J\njYvvKKWc41hdO4Fg8HQeircUTORaJ1dKxVe13YGsjPOJQL1SJpGPLc3F5dJErpSKvzMjVjSRD0tG\nehrlo3I4Uqun6iul4utITSsu4n9qfq+USeRgbQ07uvzUNeklbZVS8dF7an5ZcTaZGYm5S1nKJXKw\nto5KKRUPDS1dtHX6ElZWgRRL5L27NVonV0rFS6IuXRsqpRL5hHLr1NgDx5sTHIlSaqQ4aOebCaMT\ndwPpiK61aIyZBTwN3CsiD/SZVgk8CmQAm0TkC1GPMkJFeZl4i7LYd6SJQDCYkIH5SqmRZc/hRlzA\n1HGFCYth0B65MSYXuB94dYBZ7gHuEZGLAL8xZnwU4xuy6eOKaO/ycbRWz/BUSsWWzx/gwLFmxnpz\nyc1KT1gckZRWuoAVwLG+E4wxbmAx1j08EZG7RKQ6qhEO0bTKIsDaSiqlVCwdOtFCty9wOu8kSiT3\n7PQBPmNMf5O9QAtwrzFmHvCmiPxDuOUVF+fg8QxtiI7XG3ntacHcsTz8/G6qa9uG9L5k5PT4h2Ok\ntl3b7Sxvbj8BwAXnlp9VG6LV7uHej8gFjAXuA6qAVcaYlSKyaqA3NDQM7W73Xm8+tbWRXz8lPRik\nICedbftqqalpxuXQOvlQ251KRmrbtd3Os3l3DQDlhVlDbsNQ2x0u6Q931EodcEhE9ouIH6uOPnOY\nyxwWl8vFtMoiGlu7qdUTg5RSMRIIBtl7pJHSwiyK8zMTGsuwErlddjlgjJlmvzQfkGFHNUzTx1n1\nqr1aJ1dKxcixujbaOn1MT3B9HCIorRhj5mONTJkI9BhjbsY6uHlQRJ4EvgY8bB/43AY8G7twIzM9\n5IDnotkVCY5GKZWKegdUOCKRi8hG4PIw0/cBl0YxpmGrLMsjKyONPUeaEh2KUipF9SbyaQkcP94r\npc7s7OV2u5g6tpCT9e00tXUnOhylVIoJBoPsPdJEQU465aNyEh1OaiZyOLO7o3VypVS01TV10tDS\nxbTKoqQYGZfyiXzPEU3kSqnoOl0fH5f4+jikcCKfVJFPusfNrqqGRIeilEoxO+28kgwHOiGFE3m6\nJ40ZE4o5WtdGXWNHosNRSqWIQCDItgOnKMrLYPzoxF26NlTKJnKAuVNLAdi6/1SCI1FKpYoDx5tp\n7ehhzpTSpKiPQ4on8jmTSwDYur8uwZEopVLF+3Y+mTulJMGRnJHSibykMItx3jx2H2qkq9uf6HCU\nUilg675TeNLczJhYnOhQTkvpRA4wd2oJPn+AnYfqEx2KUsrh6ps7OVzTyjnji8jKGO41B6Mn9RP5\nFLtOvk/r5Eqp4XnfPt7We/wtWaR8Ip88poC87HTe319HMBhMdDhKKQfbus+qj89Jovo4jIBE7na7\nmD15FI2t3VSfbE10OEoph+ru8bPrUANjSnPxFmUnOpwPSPlEDqHDEHX0ilLq7OyubqDbF0iq0Sq9\nRkQinzVpFG6Xiy17NZErpc7OFvs4W7KVVWCEJPKcrHRmTCym6kQLJ4d4qzmllPL5A2zYXUNBTjpT\nk+CytX1FlMiNMbOMMfuNMV8OM8/dxpjXoxZZlC04dzQA7+44meBIlFJOs+NgPa0dPVw0YzRp7uTr\n/w4akTEmF7gf636cA81zLrAkinFF3bzpXtI9btbtPKmjV5RSQ/LuTqsDuGBmeYIj6V8km5YuYAVw\nLMw89wDfikpEMZKd6WHu1FJO1Ldz6KQz79itlIq/zm4fm/bWUlaUzaSKge9kn0iR3OrNB/iMMf1O\nN8bcDrwBVEXygcXFOXg8aZFHCHi90Vl5Vy2cyIbdNbx/sIELZ4+NyjJjKVrtdqKR2nZtd/J5feNh\nunsCLL1wPGVlBVFddrTaPaxzTI0xo4DPAMuAiDJjwxAPNnq9+dTWRqcHPb4kh5xMD6s3Hubai8fj\ndifHlcv6E812O81Ibbu2Ozm9/O4hAGZPLIpqnENtd7ikP9yq/VLAC7wJPAnMM8bcO8xlxky6x80F\n55TR1NqNVOsNJ5RS4TW3d7P9QD0TyvOpKMlNdDgDGlYiF5EnRORcEVkA3ABsEpGvRye02OgdvbJ2\np45eUUqFt35XDYFg8HTeSFaDllaMMfOxDmZOBHqMMTcDzwAHReTJ2IYXfdPHF1Gcn8lGqeETy6aT\nmTG0er1SauR4Z/txXMBFMxyeyEVkI3B5BPNVRTJforldLhbPqeCZt6tYt/MEl52X/Ac9lVLxd/B4\nMwePt3De1FKK8zMTHU5YyTeyPQ4uO28sbpeL1ZuO6phypVS/Vm8+CsAV85K/szciE3lxfibnTyul\nuqaVA8eaEx2OUirJtHX28N7Ok3iLspg5aVSiwxnUiEzkcGYr+9qmowmORCmVbN7edoJuX4DLz7f2\n3pPdiE3kMyYUUz4qh/W7T9LS3p3ocJRSSSIYDLJ681E8aW4unV2R6HAiMmITucvl4orzx+LzB3lr\n2/FEh6OUShK7DjVwsr6di2aUkZ+TkehwIjJiEznAotnlZHjcrN50lEBAD3oqpc6UW51wkLPXiE7k\nOVnpLJxVTl1TJxukJtHhKKUS7PipNjbvqWVieT6TK6J7XZVYGtGJHODqi8fjcsFzaw/pUESlRrjn\n360mCKxYMAGXAw5y9hrxiXx0cQ4XnlNGdU0r2w7UJzocpVSC1Dd3snb7CSpKcphnvIkOZ0hGfCIH\nWLlwIgCr1lYlMgylVAK98F41/kCQay6e4Ighh6E0kQOVZXnMmVLC3iNN7DncmOhwlFJx1tzezZot\nxygpyGTBzOS+rkp/NJHbrj3dKz+U2ECUUnH3yoYjdPsCXHXReDxpzkuLzos4RqaOK2R6ZRHbDpzS\n0/aVGkFaO3p4deMR8nPSWTx3TKLDOSuayEPcsHgSAE+8vk9HsCg1Qjy39hAdXT5WLJhAZrozL2ut\niTyEGV/MnCkl7K5uZPtBHcGiVKo71dTJKxuPUFKQyVIHnQDUlybyPm66bAou4InX9xPQXrlSKe2p\ntw7g8wf46OLJpA/xpvDJJKKbLxtjZgFPA/eKyAN9pl0B3A34AQHuFJFAtAONl8qyPBbMLGftjhO8\nu/MkC2eWJzokpVQMHKlt5Z1tJxjnzXX83/mgPXJjTC5wP/DqALM8BNwsIouAfODq6IWXGDcsnoQn\nzcWTaw7Q43PsNkkpFcaf3jhAELj58im43c4aN95XJKWVLmAFcGyA6fNF5Ij9uBYoiUZgiVRalM3S\neeOoa+rkpfXViQ5HKRVl2w+eYsu+OqZXFjF7suNTFq5IR2cYY74L1PUtrYRMrwDeBC4WkVMDLcfn\n8wc9DqhFtXb08IUfvkJnt5+ff3MpZcU5iQ5JKRUFPT4/X/7xak6cauOnf3s5k8YUJjqkSA242xBR\njXwwxpgy4FngS+GSOEBDQ/uQlu315lNb2zKM6M7ezZdN4b9X7eLnj23hrhtnx/WzE9nuRBupbdd2\nx8ez71RxrK6NZfPHkZfuTtg6H2q7vd78AacNe9SKMaYAeB74JxF5abjLSyYLZ5UzdVwhG/fUsu1A\n2O2TUsoB6ho7WPVOFQW5GXx08eREhxM10Rh+eA/WaJYXorCspOJ2ufjkcoPb5eK3L++hx+dPdEhK\nqWF49NW9dPsCfPyKqeRkRaUgkRQGbYkxZj5Wsp4I9BhjbgaeAQ4CLwKfAqYZY+603/I7EXkoNuHG\nX2VZHkvnj+WVDUd49p0qblwyJdEhKaXOwkapYfNe6wCnEy+MFc6giVxENgKXh5klM2rRJKkbFk9m\n8546nltbzfnTvExy0J1DlFLW1Q0feVHwpLn59NXGUTeNiISe2RmB7EwPn1lxDoFgkF+t2qVjy5Vy\nmN++tIeW9h5uXDKZipLcRIcTdZrII3TuxFFcMW8sR+vaeObtg4kORykVofW7a1i/u4apYwtZfmFl\nosOJCU3kQ3DL5VMoLcziuXUtvNfGAAAUUklEQVSH2H+sKdHhKKUG0dTWzW9eFNI9bj67cobjz+Ac\niCbyIcjK8PDZFTMgCA8+vYP2Tl+iQ1JKDSAQDPJff95Ja0cPNy2ZTPmo1D2pTxP5EJ0zoZiVl0yg\nrqmTh1/YrdctVypJPb/uEDsO1jN7cgnLUrSk0ksT+Vn4i0snMW1cIRt21/DGloEuQaOUSpR9R5p4\ncs1BivIyuOPaGY67mfJQaSI/C2luN5+/fia5WR4efXUvh2taEx2SUsrW2tHDg89sJ0iQz18/k4Kc\njESHFHOayM/SqIIs7lh5Lj2+AA/86X1aO3oSHZJSI54/EODBZ3ZwqrmL6xdNwowvTnRIcaGJfBjO\nm1bKtZdMoLaxkwef3o4/oOPLlUqkP75+gB0H65kzpYTrLpmY6HDiRhP5MH108WTmTilhR1UDj6/e\nn+hwlBqx1m4/wQvvVVM+Koe/vm5myg417I8m8mFyu1x87rqZVJTk8NL6w7y97XiiQ1JqxDl4vJlf\nP7+b7Mw0vnLT7JS6IFYkNJFHQU6Wh6/cNIecTA8PP7+bnVX1iQ5JqRGjprGD+x7fij8Q4PPXz0zJ\nU/AHo4k8SspH5fCVm2bjcsEDf9pG9cmRd4MApeKtpb2be/+wheb2Hj7xkenMmVKa6JASQhN5FJnx\nxdx57bl0dvv56eNbOdXUmeiQlEpZXT1+fvbE+5xs6GDFggksnTcu0SEljCbyKLtoxmg+dsVUGlu7\nuecPW2hq6050SEqlHJ8/wC+e2s7+Y80smDmaGy9Lnbv9nI2IErkxZpYxZr8x5sv9TFtmjHnPGLPW\nGPPt6IfoPFddVMnVF43nRH079/x+s44xVyqK/IEADz69g/f3n2LWpFF8dkXqn7k5mEETuTEmF7gf\neHWAWX4G3AQsApYbY86NXnjO5HK5uOWKKSydN5YjtW38+x+26AW2lIqCQCDIf/95Fxv31HLO+CK+\nfONsPGlaWIhkDXQBK4APXVTEGDMZqBeRwyISAJ4DroxuiM7kcrm49SPTuXROBVUnWrj3cU3mSg1H\nIBDk18/tYt3Ok0wdW8jf3DyHjPS0RIeVFCK51ZsP8Blj+ptcDtSGPK8Bwt7Usrg4B49naCvf680f\n0vzJ5P988kI8v9/E6xuP8NMntvK9v76EgtzIrv3g5HYP10htu7a7fz5/gH//3Sbe3n6C6eOL+P5f\nX0JudnqcooudaH3f0R41P2ihqqGhfUgL9Hrzqa119lC+266cRsDnZ83W43zz/jX8n4+fR2Fe+Fud\npkK7z9ZIbbu2u389Pj+/eGoHW/bVMX1cIV+9aQ7trZ20tzp7VNhQv+9wSX+4xaVjWL3yXmPppwQz\n0rndLj519TlcOX8cR2vb+OFvN1HT2JHosJRKeh1dPu574n227Ktj5sRivv6x88jOHFlnbUZiWIlc\nRKqAAmPMRGOMB7gWeCkagaUat8vFrcumsXLhBE42dPCvj2yg6kRzosNSKmk1tHTxw99uYmdVA+dN\nLeVvbp5DZobWxPsz6KbNGDMfuAeYCPQYY24GngEOisiTwBeBR+3Z/yAie2IUq+O5XC5uumwKRXmZ\n/O7lPfzbbzfzpRtmMXtySaJDUyqpHKtr497HtnCquYvLzxvDJ5ZPJ82to1MGEsnBzo3A5WGmrwEW\nRjGmlHfl/HEU5WXy0LM7uO/x9/mrZdNYOm8srhE+FlYpgO0HT/GLp3bQ0eXjxiWTWblwgv5tDEI3\ncQky33j5xl+dT162h9++vIdHXhR8fr2euRq5gsEgL68/zL2PbaXH5+fOa2dw7SUTNYlHQBN5Ak0d\nW8i3P30h48vyeGPLMX7yez2lX41MPT4/v35+N4++upf8nAz+/tZ5XDKrItFhOYYm8gQrKcziH26b\nzwXGy57DjXz31++x53BjosNSKm5OnGrjX36zkbfeP86E0fl859MXMGVsYaLDchQdx5MEMjPS+OJH\nZ/HCe9X88fUD/Oh3m/l0UyeXzhytu5UqpW3eW8uvVu2irdPHkrkV3Lpsup6teRY0kScJl8vFNRdP\nYMqYQn7x9HZ+/eedrN95gjtWzBj05CGlnKa7x89jq/fx2qajZKSnccfKGSyaraWUs6WllSQzvbKI\n737mIuaZMrYfqOc7v3qPrfvqEh2WUlFzuKaVH/zPBl7bdJSxpbnc89UlmsSHSRN5EirMzeCf71zA\nX1457fSZbQ8/v5uOLr3olnIufyDAc+sO8YP/Wc/RujaunDeOb3/6AiZWFCQ6NMfT0kqScrtdLL+w\nkhkTivnPZ3ewZusxdhw8xe3XzGDmpFGJDk+pITla18avVu3k4PEWCnIzuP2aczhv6si8LVssaCJP\ncpVleXzn9gt59u0qVq09xD1/2MKlsyv42NKp5KXA1d9UauvxWb3wVWur8PmDLJg5mluXTdffbpRp\nIncAT5qbG5ZMZt50L796bhdvbTvOln11fHzpVC6ZVa4jW1RS2nWogd+8KJyob6coL4NPLjecP92b\n6LBSkiZyB5lQns93br+Al9cf4am3DvDfq3axZusxbl02nQnlI/M61ir51Dd38vjr+3l350lcWJek\nuHHJZL1qYQzpmnWYNLebqy8ez4XnlPG7V/aweW8d3394PYvnjuHGJZMjvmmFUtHW3ePnxfeqWbXu\nEN09ASaW5/PJqwyT9GBmzGkid6iSwiy+ctMcdlTV8+gre1mz9Rjv7TrJNRePZ/mF4/VynypuAoEg\nb28/zlNvHqShpYuC3Aw+8ZHJLJpdMeJvihwvmsgdbubEUXzvsxfy+uZjPPP2QZ588yCvbT7K9Ysm\nsXhOhd6YVsVMMBhk675T/HHNfo7WtpHucbNiwQRWLpygZZQ407WdAtLcbq6cP45LZpXzwrvVvLi+\nmt+8KDy39hDXLZrIJbPKNaGrqAkGg2w7UM9Tbx6g6kQLLhdcOruCjy6exKiCrESHNyJpIk8h2Zke\nblgymaXzxrJq3SFe33yMh5/fzZ/fqWLFggksml1O+hBvfK1Ur0AwyNa9dfx57SEOHrfubnXhOWVc\nf+kkxpbmJji6kS2iRG6MuRdYAASBr4rI+pBpdwG3AX5gg4h8LRaBqsgV5mVy67LpXHPxBFatrWLN\n1uM88qLw9FsHWX5RJZfNHUtOlm7DVWR8/gDv7TrJ8+uqOVrXBsD86V6uv3QSlWV5CY5OQWS3ersM\nmCYiC40xM4BfYd8RyBhTAHwDmCoiPmPMS8aYBSKyLqZRq4gU52dy23LDdZdM5KX1h3lt81EeX72f\nZ96uYsmcMSy7YBzeouxEh6mSVGtHD29sOcorG4/Q1NqN2+XiklnlXLNggvbAk0wk3bIrgacARGSX\nMabYGFMgIs1At/0vzxjTCuQA9TGLVp2VwrxMbrliKisWTuCNLcd4ZcNhXt5wmFc2HGbu1FKumDeW\nmZNG6QgDBUDViWZe23SU93aepNsXICsjjeUXVrJs/jhKdcOflCJJ5OXAxpDntfZrzSLSaYz5HnAA\n6AB+P9jNl4uLc/AMsU7r9Y7Mk12i3W4vMLFyFLdecy5vbT3Kn986wJZ9dWzZV0dFSS4fuXg8Sy+o\npKQw8X+s+p3HV3tnD29uOcpL7x5iT7V1Y5PykhxWLprERy6aQG6MT6nX73t4zqZQerrbZpdW/hGY\nDjQDrxlj5orI1oHe3NDQPqQP83rzqa1tOYswnS3W7Z41vohZt87j4PFmVm86yru7TvLIc7v43+d3\nM2dKCZfMKmfu1FLSPfEf7aLfeXwEgkGkupF3th9n/e4aunsCuFwwZ0oJS+eNY9Zkay+tvbWT9tbO\nmMWh33fk8w8kkkR+DKsH3msMcNx+PAM4ICJ1AMaYN4H5wICJXCWXSRUFTFpZwF9eOZV3d55kzdbj\np3vpOZkeLpxRxsUzRjO9sgi3W0svThcMBjlc08p7u2pYt/ME9c1dAJQWZnHpnAounV2hQwgdKJJE\n/hLwPeBBY8w84JiI9G5GqoAZxphsEekALgCei0mkKqZystK5Yt44rpg3jsM1razdfoJ1O0/wxpZj\nvLHlGIW5GVxgyphvvEyrLCTNrePSnaI3eW+UWtbvruFEvbVXnJ2ZxqVzKrhkZjnTxxfpMRIHcwWD\nwUFnMsb8EFgCBIC7gPOBJhF50hjzeeAzgA94R0S+GW5ZtbUtg39gCN3tSpxAIMju6gbW765ho9TS\n2tEDQF52OnOnlnDeVC/nTiyO+ll8ydD2RIhmu33+AHsPN7Jl3yk2762lrskqjWR43MyZWsqF55Qx\nd0pJUtwfU7/viOcfcEsbUSKPJk3kkUm2dvv8AXZXN7BpTx2b99bS1NoNgCfNxfTKImZPLmHmxFGM\n9eYO+7K6ydb2eBluu+saO9hRVc/2g/XsOFhPZ7cfgKyMNOZMKWHedC9zppSQlZFc5xDo9x3x/AP+\nYSXXN6qSlifNzaxJJcyaVMJty6dz8FgzW/ef4v39deysamBnVQNg3aZuxsRizhlfjBlfRFlRtl4v\nPUaaWruQw43srm5kV1U9Jxs6Tk/zFmWxaHYFc6eUYMYXJ+SgtYofTeRqyNwuF1PGFjJlbCE3LplM\nY2sXOw7Ws7Oqnh1VDazbcZJ1O04CUJiXwbSxhUwdV8TUsYVUluVpUjkLgUCQ4/Xt7D/axN4jjew7\n0vSBxJ2VkcZ5U0uZOWkUMyeNYnSxbkBHEk3katiK8jJZNLuCRbMrCAaDHDvVjlQ3sPtQA3uONLFB\natkgtYBViqksy2NiRQETRuczYXQ+Y0pzNbmHCASCnGxo59DJFqpPtlJ1vJmDJ1roskslYB2onDV5\n1Ok9nwmj8/XCaCOYJnIVVS6Xi7GluYwtzWXpvHEEg0FqmzrZf6SJfceaqDreTPXJVg4eP1MbTHO7\nKC/Jsd7nzeOcSSXkpLspK8pO6QQfCASpa+7kxKk2jtW1U9/azb7DDRw71UZ3T+D0fC6gojSXSRX5\nTB5TyLSxhYwpzdXhoOo0TeQqplwuF2VF2ZQVZbNwlnU6Qo8vwJHa1tM9zsMnWzhS18bR2jbYVQNr\nDtjvhZKCLMqKrfeXFmVTUpBFSWEWo/IzKcjNSOpeaCAYpKW9h4aWTk41Wf/qmjqpaeygtrGD2sZO\nfP7AB97jSXNRUZJLZVke40fnM2G09b9e31uFo78OFXfpHrd1IlLILcCCwSCnmjo5UtdGa5effdX1\nHD/VTk1jh3UwlYYPLccFFORmUJibQYH9Lz8nndysdHKz08nN8pCV4SEn00NWRhoZ6W4y09NI96SR\n7nGR5naH7dUGgkH8/iA+f4AeX4DuHj9dPX46e/x0dPno7PLT3uWjraOH1s4eWtp7aGnrprm9m6a2\nbppau/EH+h+klZvlobIsl/JROZSX5FIxKodZ08tIJ6Bj9NWQaSJXScHlclFq97r7Dsvq6vZT29hB\nXVMnp5qtnm1DaxcNLV00tnRxsqGD6prWs/xc6+Cty+XC7YJA0NqoBINWIj8bnjQ3hbnpTKzIpygv\nk+K8TEoKs07vTZQVZ5Ob9eFrl4zUYXhq+DSRq6SXmZHGuLI8xoW59nVXt5/m9m5aO3po7eihraOH\ntk4fnd0+Orr8dHT76OkJ0O3z090TwBcI4PMF8AWCZxJ3IGgldDe4cOFJc5GW5sbjdpGenkamx016\nehpZGWlkZ6SRlWn19vOyrT2A/Ox0CnIzyMpI0xEjKq40kauUkJmRhjcjW6+vrkYkLcYppZTDaSJX\nSimH00SulFIOp4lcKaUcThO5Uko5nCZypZRyOE3kSinlcJrIlVLK4eJ+hyCllFLRpT1ypZRyOE3k\nSinlcJrIlVLK4TSRK6WUw2kiV0oph9NErpRSDqeJXCmlHC6pbixhjLkXWAAEga+KyPqQacuAfwX8\nwHMi8oPERBl9g7T7CuBurHYLcKeIBPpdkMOEa3fIPHcDC0Xk8jiHFzODfN+VwKNABrBJRL6QmCij\nb5B23wXchvU73yAiX0tMlLFhjJkFPA3cKyIP9Jk27NyWND1yY8xlwDQRWQjcAfyszyw/A24CFgHL\njTHnxjnEmIig3Q8BN4vIIiAfuDrOIcZEBO3G/o6XxDu2WIqg3fcA94jIRYDfGDM+3jHGQrh2G2MK\ngG8Ai0XkUuBcY8yCxEQafcaYXOB+4NUBZhl2bkuaRA5cCTwFICK7gGL7C8YYMxmoF5HDdm/0OXv+\nVDBgu23zReSI/bgWKIlzfLEyWLvBSmrfindgMRbud+4GFgPP2NPvEpHqRAUaZeG+7277X54xxgPk\nAPUJiTI2uoAVwLG+E6KV25IpkZdjJapetfZr/U2rASriFFeshWs3ItIMYIypAJZjfdGpIGy7jTG3\nA28AVXGNKvbCtdsLtAD3GmPesstKqWLAdotIJ/A94ABwCHhXRPbEPcIYERGfiHQMMDkquS2ZEnlf\n4W5Dnsq3KP9Q24wxZcCzwJdE5FT8Q4qL0+02xowCPoPVI091rj6PxwL3AZcB5xtjViYkqtgL/b4L\ngH8EpgOTgIuNMXMTFViCnVVuS6ZEfoyQHhkwBjg+wLSx9LOb4lDh2t37I38e+CcReSnOscVSuHYv\nxeqdvgk8CcyzD5SlgnDtrgMOich+EfFj1VRnxjm+WAnX7hnAARGpE5FurO99fpzjS5So5LZkSuQv\nATcDGGPmAcdEpAVARKqAAmPMRLuGdq09fyoYsN22e7COdL+QiOBiKNz3/YSInCsiC4AbsEZvfD1x\noUZVuHb7gAPGmGn2vPOxRiqlgnC/8ypghjEm235+AbA37hEmQLRyW1JdxtYY80OsUQoB4C7gfKBJ\nRJ40xiwB/s2e9Y8i8pMEhRl1A7UbeBFoANaGzP47EXko7kHGQLjvO2SeicDDKTb8MNzvfCrwMFYn\naxvwxRQabhqu3Z/HKqf5gHdE5JuJizS6jDHzsTpkE4Ee4CjWAe2D0cptSZXIlVJKDV0ylVaUUkqd\nBU3kSinlcJrIlVLK4TSRK6WUw2kiV0oph9NErpRSDqeJXCmlHO7/A2DEYj/KLtuQAAAAAElFTkSu\nQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f83215e6e48>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "2qo5XKYPW25Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## PARAMS"
      ]
    },
    {
      "metadata": {
        "id": "ytDrOHZtXMuc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "I9CZBDxzYbt_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56868b0a-49a6-4e3f-d852-3ffc07824f5d"
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import sys\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--epocs', type=int, default=2, help='number of epochs to train for')\n",
        "parser.add_argument('--epoch_size', type=int, default=600, help='epoch size')\n",
        "parser.add_argument('--batch_size', default=100, type=int, help='batch size')\n",
        "\n",
        "parser.add_argument('--optimizer', default='adam', help='optimizer to train with. only Adam supported')\n",
        "parser.add_argument('--lr', default=0.02, type=float, help='learning rate. source=0.002')\n",
        "parser.add_argument('--adv_disc_lr', default=0.002, type=float, help='learning rate')\n",
        "\n",
        "parser.add_argument('--beta1', default=0.5, type=float, help='momentum term for adam')\n",
        "\n",
        "parser.add_argument('--semantics_dim', type=int, default=20, help='size of the semantics vector')\n",
        "parser.add_argument('--style_dim', type=int, default=5, help='size of the style vector')\n",
        "parser.add_argument('--sd_weight', type=float, default=0.001, help='weight on adversarial loss 0.0001 originally. reducing')\n",
        "\n",
        "'''\n",
        "parser.add_argument('--max_step', type=int, default=20, help='maximum distance between frames')\n",
        "parser.add_argument('--seed', default=1, type=int, help='manual seed')\n",
        "parser.add_argument('--log_dir', default='logs', help='base directory to save logs')\n",
        "parser.add_argument('--data_root', default='', help='root directory for data')\n",
        "\n",
        "parser.add_argument('--dataset', default='moving_mnist', help='dataset to train with')\n",
        "\n",
        "parser.add_argument('--sd_nf', type=int, default=100, help='number of layers')\n",
        "parser.add_argument('--content_model', default='dcgan_unet', help='model type (dcgan | dcgan_unet | vgg_unet)')\n",
        "parser.add_argument('--pose_model', default='dcgan', help='model type (dcgan | unet | resnet)')\n",
        "parser.add_argument('--data_threads', type=int, default=5, help='number of parallel data loading threads')\n",
        "parser.add_argument('--data_type', default='drnet', help='speed up data loading for drnet training')\n",
        "'''\n",
        "sys.argv=[\"nothing\"]\n",
        "opt = parser.parse_args()\n",
        "print (opt.lr,opt.beta1)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.02 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tRl4xEzXDwZD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1366
        },
        "outputId": "4c28a902-eeaf-4af0-e927-d1f3b1037596"
      },
      "cell_type": "code",
      "source": [
        "from seq2seq.models import EncoderRNN\n",
        "\n",
        "en_sem = EncoderRNN(len(TEXT.vocab),50, opt.semantics_dim)\n",
        "\n",
        "#en_sty = EncoderRNN(len(TEXT.vocab),max_seq_length opt.style_dim\n",
        "sample = next(train_iter)\n",
        "en_sem(sample.sent_0)\n"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    697\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# printer registered in self.type_pprinters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_pprinters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                     \u001b[0;31m# deferred printer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreakable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;31m# Special case for 1-item tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    396\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                                 \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_default_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_safe_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_baseclass_reprs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;31m# A user-provided repr. Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# characters to replace unicode characters with.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0msuffix\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m', dtype='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         end = ([get_summarized_data(self[i]).view(-1)\n\u001b[1;32m    218\u001b[0m                for i in range(len(self) - PRINT_OPTS.edgeitems, len(self))])\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRINT_OPTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         end = ([get_summarized_data(self[i]).view(-1)\n\u001b[1;32m    218\u001b[0m                for i in range(len(self) - PRINT_OPTS.edgeitems, len(self))])\n",
            "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Call .contiguous() before .view(). at /pytorch/aten/src/TH/generic/THTensor.cpp:237"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "r5eh7qthtyX5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G3ghi3B0ymmk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# FC  decoders"
      ]
    },
    {
      "metadata": {
        "id": "bXi51149ioGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef9b9e01-62c0-4394-c9c3-940b351de6be"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "trn_dl = DataLoader(TimePairsDataset(1e5,1), batch_size=opt.batch_size,)\n",
        "x,y = next(iter(trn_dl)) \n",
        "print ('input . x is vertically stacked sentences',x.shape,'y',y.shape)\n",
        "\n",
        "batch,pair,sentence_len=x.shape\n",
        "\n",
        "\n",
        "      \n",
        "en_sem = nn.Sequential(\n",
        "  # expect sentence_len,so from outside, cut a pair into x[0],x[1] and pass seperatly each\n",
        "  nn.Linear(sentence_len,19),\n",
        "  nn.ReLU(inplace=True), \n",
        "  nn.Linear(19,opt.semantics_dim),\n",
        "  #nn.Tanh()\n",
        ")\n",
        "\n",
        "en_sty= nn.Sequential(\n",
        "  nn.Linear(sentence_len,19),\n",
        "  nn.ReLU(inplace=True),\n",
        "  nn.Linear(19,opt.style_dim),\n",
        "  #nn.Tanh()\n",
        ")\n",
        "decoder = nn.Sequential(\n",
        "  # input concat of semantic and style\n",
        "  # output sentence_len , each word/char has currently value in range 0..1\n",
        "  nn.Linear(opt.semantics_dim+opt.style_dim,19),\n",
        "  nn.ReLU(inplace=True),\n",
        "  nn.Linear(19,60),\n",
        "  nn.ReLU(inplace=True),\n",
        "  nn.Linear(60,sentence_len),\n",
        "  # we apply MSE on this\n",
        ")\n",
        "adv_disc = nn.Sequential(\n",
        "  # input concat of two style\n",
        "  nn.Linear(2*opt.style_dim,6),\n",
        "  nn.ReLU(inplace=True),\n",
        "  nn.Linear(6,1),\n",
        "  nn.Sigmoid()  #Must be sigmoid, we apply later BCE\n",
        ")\n",
        "\n",
        "if opt.optimizer=='adam':\n",
        "  optimizer = torch.optim.Adam\n",
        "\n",
        "optimizer_en_sem = optimizer(en_sem.parameters(), lr=opt.lr) #(, betas=(opt.beta1, 0.999))\n",
        "optimizer_en_sty = optimizer(en_sty.parameters(), lr=opt.lr) #), betas=(opt.beta1, 0.999))\n",
        "optimizer_decoder = optimizer(decoder.parameters(), lr=0.1) #)opt.lr), betas=(opt.beta1, 0.999))\n",
        "optimizer_adv_disc = optimizer(adv_disc.parameters(), lr=opt.adv_disc_lr) ##), betas=(opt.beta1, 0.999))\n",
        " "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input . x is vertically stacked sentences torch.Size([32, 3, 20]) y torch.Size([32, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GaVWTeleYkWk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train loop"
      ]
    },
    {
      "metadata": {
        "id": "6Yu8nIAYyta5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_jDD-H1faFtq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "outputId": "e5bec71d-f2b2-42d7-82c2-4a975e0b2ff8"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --------- training funtions ------------------------------------\n",
        "def train(x,verbose=False):\n",
        "    #print (x.shape)\n",
        "    en_sty.zero_grad() \n",
        "    en_sem.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "\n",
        "    \n",
        "    # x[0] semantic0   , style0\n",
        "    # x[1] semantic0   , style1   \n",
        "    # x[2] semantic0orX, style0 (1/2 the time 0 , half X)\n",
        "    sent0 = x[:,0]\n",
        "    sent1 = x[:,1]\n",
        "    sentX = x[:,2]\n",
        "    h_sem0 = en_sem(sent0) \n",
        "    h_sem1 = en_sem(sent1) \n",
        "    # if you want to use torch criterion, you need to copy the label and set it to not requreing gradiant\n",
        "    # so below is different than nn.MSELoss()(h_sem0,h_sem1.detach()). I wonder if only one get grad updates!\n",
        "    sim_loss = torch.mean(torch.pow(  h_sem0- h_sem1,2)) \n",
        "    #sim_loss = mse_criterion(h_c1[0] if opt.content_model[-4:] == 'unet' else h_c1, h_c2)\n",
        "    \n",
        "    \n",
        "    h_sty0 = en_sty(sent0)\n",
        "    recon_sent0 = decoder(torch.cat([h_sem1,h_sty0],dim=1)) #assume h_sem0==h_sem1\n",
        "    rec_loss = nn.MSELoss()(recon_sent0,sent0)\n",
        "\n",
        "    # TODO : willl it be better to use completely different sentences?\n",
        "    h_styX = en_sty(sentX)\n",
        "    adv_disc_p =  adv_disc(torch.cat([h_sty0,h_styX],dim=1))\n",
        "    if verbose:\n",
        "      print (adv_disc_p[0:3].data.numpy().T)\n",
        "    adv_target = torch.FloatTensor(np.full(shape =(sent0.shape[0],1),fill_value=0.5))\n",
        "    # the loss below is a parabula with min at log(0.5)=0.693... see documentation above\n",
        "    adv_disc_loss = nn.BCELoss()(adv_disc_p, adv_target) - np.log(0.5)\n",
        "    #print (adv_disc_loss)\n",
        "    \n",
        "    # full loss\n",
        "    loss = rec_loss #+ sim_loss + opt.sd_weight*adv_disc_loss\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer_en_sem.step()\n",
        "    optimizer_en_sty.step()\n",
        "    optimizer_decoder.step()\n",
        "\n",
        "    return sim_loss.data.cpu().numpy(),rec_loss.data.cpu().numpy(),adv_disc_loss.data.cpu().numpy()*opt.sd_weight\n",
        "\n",
        "def train_scene_discriminator(x,y):\n",
        "    #print ('train_scene_discriminator',x.shape,y.shape)\n",
        "    \n",
        "    sent0 = x[:,0]\n",
        "    sentX = x[:,2]\n",
        "    \n",
        "    adv_disc.zero_grad()\n",
        "\n",
        "    h_sty0    = en_sty(sent0) #detach?\n",
        "    h_sty0or2 = en_sty(sentX)\n",
        "    \n",
        "  \n",
        "    out = adv_disc(torch.cat([h_sty0, h_sty0or2],dim=1))\n",
        "    #TODO: #Note BCELossWithLogits is faster and moer stable, to use it remove sigmoid from network end\n",
        "    bce = nn.BCELoss()(out, y) #should wrapp in varaible? \n",
        "\n",
        "    bce.backward()\n",
        "    optimizer_adv_disc.step()\n",
        "    #print (out.shape) #torch.Size([16, 1])\n",
        "    acc =  np.round(out.detach().numpy())==y  #CHECK THIS DIMENSTIONS!!! \n",
        "    #print (acc.shape) #1,16\n",
        "    acc = acc.reshape(-1).float()\n",
        "    acc= acc.sum()/len(acc)\n",
        "    return bce.data.cpu().numpy(), acc.numpy()\n",
        "\n",
        "# --------- training loop ------------------------------------\n",
        "train_ds = TimePairsDataset(1e9,1)\n",
        "eval_ds  = TimePairsDataset(1e9,2)\n",
        "training_batch_generator = iter(DataLoader(train_ds,batch_size=opt.batch_size))\n",
        "eval_batch_generator = iter(DataLoader(eval_ds,batch_size=opt.batch_size))\n",
        "\n",
        "x,y = next(training_batch_generator)\n",
        "print ('training_batch_generator',x.shape,y.shape) \n",
        "  \n",
        "opt.epocs=50 \n",
        "opt.epoch_size=500\n",
        "opt.batch_size= 32 #anti-adv-loss will be noiser\n",
        "\n",
        "  \n",
        "for epoch in range(opt.epocs):\n",
        "    #print ('new epoch')\n",
        "    en_sty.train()  # and not eval() mode\n",
        "    en_sem.train()\n",
        "    decoder.train()\n",
        "    adv_disc.train()\n",
        "    epoch_sim_loss, epoch_rec_loss, epoch_anti_disc_loss, epoch_sd_loss, epoch_sd_acc = 0, 0, 0, 0, 0\n",
        "\n",
        "    for i in range(opt.epoch_size):\n",
        "        #if i % 100==0 : print ('batch',i,'of',opt.epoch_size)\n",
        "        x,y = next(training_batch_generator)\n",
        "\n",
        "        # train scene discriminator\n",
        "        sd_loss, sd_acc = train_scene_discriminator(x,y)\n",
        "        epoch_sd_loss += sd_loss\n",
        "        epoch_sd_acc += sd_acc\n",
        "\n",
        "        # train main model\n",
        "        # verbose= i%5000==0\n",
        "        sim_loss, rec_loss, anti_disc_loss = train(x,verbose)\n",
        "        epoch_sim_loss += sim_loss\n",
        "        epoch_rec_loss += rec_loss\n",
        "        epoch_anti_disc_loss += anti_disc_loss\n",
        "    #print (epoch, epoch_rec_loss/opt.epoch_size, epoch_sim_loss/opt.epoch_size)\n",
        "    #print (epoch_sd_acc/opt.epoch_size)\n",
        "    #print (epoch*opt.epoch_size*opt.batch_size)\n",
        "    print('[%02d] rec loss: %.4f | sim loss: %.4f | anti_disc_loss: %.4f || scene disc %.4f %.3f%% ' % (epoch, epoch_rec_loss/opt.epoch_size, \n",
        "                epoch_sim_loss/opt.epoch_size, epoch_anti_disc_loss/opt.epoch_size,\n",
        "                epoch_sd_loss/opt.epoch_size, 100*epoch_sd_acc/opt.epoch_size))\n",
        "\n",
        "# back to eval mode\n",
        "en_sty.eval()  # and not eval() mode\n",
        "en_sem.eval()\n",
        "decoder.eval()\n",
        "adv_disc.eval()\n",
        "print ('training loop done')\n",
        "# TODO: save the model\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training_batch_generator torch.Size([32, 3, 20]) torch.Size([32, 1])\n",
            "[00] rec loss: 0.0156 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6937 49.794% \n",
            "[01] rec loss: 0.0135 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6933 49.462% \n",
            "[02] rec loss: 0.0135 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6933 49.350% \n",
            "[03] rec loss: 0.0136 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.194% \n",
            "[04] rec loss: 0.0136 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.150% \n",
            "[05] rec loss: 0.0135 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.500% \n",
            "[06] rec loss: 0.0134 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.038% \n",
            "[07] rec loss: 0.0134 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6931 50.650% \n",
            "[08] rec loss: 0.0134 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.638% \n",
            "[09] rec loss: 0.0135 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.531% \n",
            "[10] rec loss: 0.0135 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6933 49.281% \n",
            "[11] rec loss: 0.0133 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.450% \n",
            "[12] rec loss: 0.0134 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.825% \n",
            "[13] rec loss: 0.0134 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6933 49.731% \n",
            "[14] rec loss: 0.0133 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6933 49.388% \n",
            "[15] rec loss: 0.0134 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6933 49.375% \n",
            "[16] rec loss: 0.0132 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6933 49.712% \n",
            "[17] rec loss: 0.0133 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.712% \n",
            "[18] rec loss: 0.0133 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.263% \n",
            "[19] rec loss: 0.0133 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.069% \n",
            "[20] rec loss: 0.0133 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.212% \n",
            "[21] rec loss: 0.0132 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.938% \n",
            "[22] rec loss: 0.0133 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.169% \n",
            "[23] rec loss: 0.0132 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.675% \n",
            "[24] rec loss: 0.0132 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6933 49.663% \n",
            "[25] rec loss: 0.0132 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.769% \n",
            "[26] rec loss: 0.0131 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.312% \n",
            "[27] rec loss: 0.0130 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.631% \n",
            "[28] rec loss: 0.0132 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6929 50.856% \n",
            "[29] rec loss: 0.0131 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6933 50.188% \n",
            "[30] rec loss: 0.0131 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6931 50.669% \n",
            "[31] rec loss: 0.0130 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.919% \n",
            "[32] rec loss: 0.0131 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.000% \n",
            "[33] rec loss: 0.0130 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.419% \n",
            "[34] rec loss: 0.0130 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.444% \n",
            "[35] rec loss: 0.0130 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.587% \n",
            "[36] rec loss: 0.0129 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6931 50.281% \n",
            "[37] rec loss: 0.0129 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.900% \n",
            "[38] rec loss: 0.0129 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.419% \n",
            "[39] rec loss: 0.0129 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.481% \n",
            "[40] rec loss: 0.0130 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.062% \n",
            "[41] rec loss: 0.0129 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.837% \n",
            "[42] rec loss: 0.0129 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.737% \n",
            "[43] rec loss: 0.0127 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.750% \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[44] rec loss: 0.0128 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.394% \n",
            "[45] rec loss: 0.0128 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.306% \n",
            "[46] rec loss: 0.0128 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.244% \n",
            "[47] rec loss: 0.0127 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6931 50.669% \n",
            "[48] rec loss: 0.0131 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 49.969% \n",
            "[49] rec loss: 0.0136 | sim loss: 0.0000 | anti_disc_loss: 0.0014 || scene disc 0.6932 50.319% \n",
            "training loop done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hp_41_yw0vpK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2ebc07e-2ee3-469c-d5f3-f30d7c910fc6"
      },
      "cell_type": "code",
      "source": [
        "''' BCE dynamics\n",
        "bce=lambda a,p: nn.BCELoss()(torch.tensor(a),torch.tensor(p)).numpy()\n",
        "#p= np.array([[0.5001803,  0.50018024, 0.5001803 ]])\n",
        "p = np.array([[1.0,1.0,1.0]]); a = np.array([[0.0,0.0,0.0]]); loss = bce(p,a) ; print (a,p,loss)\n",
        "p = np.array([[1.0,1.0,1.0]]); a = np.array([[1.0,0.0,0.0]]); loss = bce(p,a) ; print (a,p,loss)\n",
        "p = np.array([[1.0,1.0,0.0]]); a = np.array([[0.5,0.5,0.5]]); loss = bce(p,a) ; print (a,p,loss)\n",
        "p = np.array([[0.5,0.5,0.5]]); a = np.array([[0.5,0.5,0.5]]); loss = bce(p,a) ; print (a,p,loss)\n",
        "p = np.array([[0.6,0.4,0.5]]); a = np.array([[0.5,0.5,0.5]]); loss = bce(p,a) ; print (a,p,loss)\n",
        "\n",
        "#       -1*( y* log(p) + (1-y) log(1-p) )\n",
        "#p=0.5  -1   y* 0.693  + (1-y)* 0.693  == -0.693 (y+1-y) = -0.693\n",
        "\n",
        "#a=0.5  -1* (0.5*log(p)+ 0.5*log(1-p)) = -0.5(log(p)+log(1-p))\n",
        "'''\n",
        "1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "Sg7Ny6D4QlXE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# EVAL"
      ]
    },
    {
      "metadata": {
        "id": "8df9zoymohYU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "f48a1a9f-4995-4ca5-95e0-9fda5fb1acb6"
      },
      "cell_type": "code",
      "source": [
        "eval_ds=  TimePairsDataset(1e7,2)\n",
        "eval_batch_generator=iter(DataLoader(eval_ds,batch_size=3))\n",
        "x,y = next(eval_batch_generator)\n",
        "print (x.shape,y.shape)\n",
        "\n",
        "sent0 = None    # x[0] semantic0   , style0\n",
        "sent1 = x[:,1]  # x[1] semantic0   , style1 \n",
        "sent2 = x[:,2]  # x[2] semantic0orX, style0 (1/2 the time 0 , half X)\n",
        "\n",
        "h_sem = en_sem(sent1)\n",
        "h_sty = en_sty(sent2)\n",
        "p = decoder(torch.cat([h_sem,h_sty],dim=1))\n",
        "  \n",
        "for i in range(len(x)) :\n",
        "  print ('source sent. take semantics: ',eval_ds.untokenize_tokens(sent1[i]))\n",
        "  print ('target sent. take style    : ',eval_ds.untokenize_tokens(sent2[i]))\n",
        "  print ('target sent                : ',eval_ds.untokenize_tokens(p[i]))\n",
        "  print ('\\n')\n",
        "  \n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 3, 20]) torch.Size([3, 1])\n",
            "source sent. take semantics:  Jan 01 1970 00:00:02\n",
            "target sent. take style    :  04/Apr/1970 22:51:53\n",
            "target sent                :  ;MS3EJ&1970 06:00:35\n",
            "\n",
            "\n",
            "source sent. take semantics:  Jan 01 1970 00:00:02\n",
            "target sent. take style    :  01/Jan/1970 00:00:02\n",
            "target sent                :  ;MS3EJ&1970 06:00:35\n",
            "\n",
            "\n",
            "source sent. take semantics:  Jan 01 1970 00:00:02\n",
            "target sent. take style    :  02/Feb/1970 11:32:26\n",
            "target sent                :  ;MS3EJ&1970 06:00:35\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GSPSoEXTgVV_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from here: http://anie.me/On-Torchtext/\n",
        "from torchtext.data import Field\n",
        "import torch\n",
        "\n",
        "class SplitReversibleField(Field):\n",
        "\n",
        "    def __init__(self, untokenize_char='', **kwargs):\n",
        "        super(SplitReversibleField, self).__init__(**kwargs)\n",
        "        self.untokenize_char = untokenize_char\n",
        "        \n",
        "\n",
        "    def reverse(self, batch):\n",
        "\n",
        "        if not self.batch_first:\n",
        "            batch = batch.t()\n",
        "        with torch.cuda.device_of(batch):\n",
        "            batch = batch.tolist()\n",
        "        batch = [[self.vocab.itos[ind] for ind in ex] for ex in batch]  # denumericalize\n",
        "\n",
        "        def trim(s, t):\n",
        "            sentence = []\n",
        "            for w in s:\n",
        "                if w == t:\n",
        "                    break\n",
        "                sentence.append(w)\n",
        "            return sentence\n",
        "\n",
        "        batch = [trim(ex, self.eos_token) for ex in batch]  # trim past frst eos\n",
        "\n",
        "        def filter_special(tok):\n",
        "            return tok not in (self.init_token, self.pad_token)\n",
        "\n",
        "        batch = [filter(filter_special, ex) for ex in batch]\n",
        "        return [self.untokenize_char.join(ex) for ex in batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sQLllWeCXlrK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fx9wR-zaDxTc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MY EncoderRNN self made buggy start of result"
      ]
    },
    {
      "metadata": {
        "id": "vAVE29Z8hgHf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "139999cb-22e4-48ec-c033-df4d34f4cc82"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "%matplotlib inline\n",
        "\n",
        "sample = next(train_iter)\n",
        "device='cpu'\n",
        "'''\n",
        "class MyEncoderRNN(nn.Module):  #see https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        \"\"\" input:  1xbs :  example: 1   x 32\n",
        "            hidden: hxbs :  example: 100 x 32\n",
        "        \"\"\"\n",
        "        print ('EncoderRNN input',input.shape, 'hidden',hidden.shape)\n",
        "        assert input.shape[1]==hidden.shape[1] # batch\n",
        "        assert input.shape[0]==1\n",
        "        assert hidden.shape[0]==self.hidden_size\n",
        "        \n",
        "        \n",
        "        embedded = self.embedding(input).view(1, input.shape[1], -1)\n",
        "        print (embedded.shape)\n",
        "        output, hidden = self.gru(embedded, hidden) #input must be 3 dimension\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self,batch_size):\n",
        "        return torch.zeros( 1, self.hidden_size,batch_size, device=device)  \n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoder_rnn):\n",
        "        \"\"\" wrapper around EncoderRNN running on a full sequence in one fwd pass \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encoder_rnn = encoder_rnn\n",
        "    \n",
        "    def forward(self, input):\n",
        "        print (input.shape)\n",
        "        sen_len, batch_size = input.shape\n",
        "        h= self.encoder_rnn.init_hidden(batch_size)\n",
        "        for i in range(sen_len):\n",
        "          o,h = self.encoder_rnn(input[i:i+1], h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class MyDecoderRNN(nn.Module):\n",
        "    def __init__(self, input_hidden_size, output_vocab_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "      \n",
        "      \n",
        "#decoder= MyDecoderRNN(opt.semantics_dim+opt.style_dim)   \n",
        "\n",
        "\n",
        "en_sem = Encoder(EncoderRNN(len(TEXT.vocab), opt.semantics_dim))\n",
        "en_sty = Encoder(EncoderRNN(len(TEXT.vocab), opt.style_dim))\n",
        "\n",
        "en_sem(sample.sent_0)\n",
        "'''\n",
        "1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    }
  ]
}
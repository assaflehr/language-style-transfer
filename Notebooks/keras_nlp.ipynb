{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_nlp.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/assaflehr/language-style-transfer/blob/master/Notebooks/keras_nlp.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "vR_wGTR6szDb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Keras accuracy/performace limitations\n",
        "\n",
        "### CuDNNLSTM vs LSTM \n",
        "The former trains very fast on GPU, but does not support the attributes: dropout,recurrent_dropout,which are the STOA regulaizers. This is cuda problem, and even native TF does not support it\n",
        "\n",
        "### Tip to self:\n",
        "* manually check loss value on one sample. From doing this, I saw <s> was not given one-hot-value"
      ]
    },
    {
      "metadata": {
        "id": "OrKzdh71sBKm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "UbJwZjyhsDvP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# adaptation of: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
        "# first Dataset class to load bible-data\n",
        "# then copy of the model, but working with words instead of chars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PcPSCDUrx3B8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "d8286a18-0969-48b5-815e-b27cf283eb3e"
      },
      "cell_type": "code",
      "source": [
        "## NLP preprocessing for text\n",
        "# has few parts:\n",
        "# 1. load zip files and then use glob to filter part of them (data/*/*.txt)\n",
        "# 2. parse each row into (x,y) by passing a parser method. it can be simple as lambda line:line:x, or if you use tab delimited lambda line: line.split(',')[4]\n",
        "# 3. tokenize - split by spaces, but also by ., and be smart about it.  ('...' should be one token , \"ai'nt\" one token. then; should be two 'token' and ';')\n",
        "#    you should also build vocabulary, keep X words and throw away rare ones, they will be replaced by <oov> flag.\n",
        "# 4. transform text to sequences for the result. for words there are usually two different types: ['s>','hello', 'world'] -> [0,5,6] but there is also \n",
        "#     a one-hot-econding version where 5 is actaully a vector of size voc-length full of zeros, with 5th index==1.\n",
        "#    The one-hot ecoding is used as output for text-generation and has a HUGE MEMORY requirement.  100K sentences of size 20 words need 2M floats = 8MB\n",
        "#    But for the one-hot-encoding multiply this by vocab-size. for char-encoding it's ~30 , for good vocab of 10K words, we need 80GB(!)\n",
        "#    The simple, and only , way to solve this , is to never keep one-hot-encoding in memory, just use a generator to make it one-hot in runtime\n",
        "\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import csv, json\n",
        "from collections import namedtuple\n",
        "from zipfile import ZipFile\n",
        "from os.path import expanduser, exists\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "TVT = namedtuple('TVT',['train','val','test'])\n",
        "\n",
        "class Dataset:\n",
        "  #Dataset for COPY encoder-decoder\n",
        "  # to David: generator x,y where x is batch,max-words of type integer. (            <s>   hello word)\n",
        "  #                               y is batch,max-words,one-hot-encoding (offset one: hello world <end>)\n",
        "  # tokenizer is currently very bad. replace it\n",
        "  # vocabulary (training-only) , don't use 666 , use large number (10K?)\n",
        "  \n",
        "  # Dataset2 for style, x is sentence + STYLE(one-hot-encoding of integer) , y is one hot encoding \n",
        "  \n",
        "  # Dataset3 for classifier: x is sentence + STYLE(one-hot-encoding of integer),  output: STYLE(one-hot-encoding of integer)\n",
        "  \n",
        "  \n",
        "  \n",
        "  def __init__(self,unique_name,url,extract,cache_dir,pattern=None,validation=0.1,test=0.1):\n",
        "    '''\n",
        "    unique_name will be used for the dataset source(or zip) file. \n",
        "    pattern need to include path inside zip (including zip root)\n",
        "    extract - is it zipped/tarred or not\n",
        "    cache_dir - under which the files be downloaded <cache_dir>/datasets/<unique_name>\n",
        "    pattern - glob will be done to choose only those files ,for example data*.txt. This should incldude both train and test\n",
        "    validation - subset glob pattern to use. If it's a float like 0.1, use it as split of one file\n",
        "    test - see above\n",
        "    '''\n",
        "    if not extract and pattern:\n",
        "      raise ValueError('pattern must be empty if extract=False chooses a subset of the files (data/*.txt). but you downloaded only one file')\n",
        "\n",
        "    if not os.path.exists(cache_dir):\n",
        "       os.makedirs(cache_dir)\n",
        "\n",
        "    fpath=get_file(unique_name, url,extract=True, cache_dir=cache_dir)\n",
        "    print ('fpath',fpath)\n",
        "    files = [fpath] if not pattern else glob.glob(f'{cache_dir}/datasets/{pattern}')\n",
        "\n",
        "    lines = [line.rstrip() for f in files for line in open(f).readlines()] \n",
        "    print (files,'#lines',len(lines),'first 3 lines')\n",
        "    print (lines[0],'\\n',lines[1],'\\n',lines[2])\n",
        "    \n",
        "    if isinstance(validation,float) and isinstance(test,float):\n",
        "      test_count = int(len(lines)*(1-test))\n",
        "      val_count =  int(len(lines)*(1-test-validation))\n",
        "      self.tvt_lines = TVT(lines[:val_count],lines[val_count:test_count],lines[test_count:])\n",
        "    \n",
        "    print ('train:',len(self.tvt_lines.train),'val',len(self.tvt_lines.val),'test',len(self.tvt_lines.test))\n",
        "  \n",
        "  def parse(self,row_parser,skip_first=False):\n",
        "    self.parsed=[]\n",
        "    for i,lines in enumerate(self.tvt_lines):\n",
        "      self.parsed.append([row_parser(line) for line in lines[1 if skip_first else 0:]])\n",
        "    self.parsed= TVT(*self.parsed)\n",
        "    print ('\\nrow_parser train:',self.parsed.train[0],'test:',self.parsed.test[0])\n",
        "    \n",
        "  def fit(self):\n",
        "    \"\"\" the current implementation is quite bad, hello world! will be 2 tokens world! is the second. \n",
        "    \"\"\"\n",
        "    print ('limiting num_words in Tokenizer due to MEMORY BOUNDS')  #num_words =100*1000\n",
        "\n",
        "    \n",
        "    # I use here tokenizer only to count freq. of words, then manually choose most freq. and manually split\n",
        "    # this is bad. There are better ways to do it (probably library/code which do it in one line)\n",
        "    print ('\\nREPLACE ME . BAD TOKENIZER!!!')\n",
        "    self.tokenizer = Tokenizer(num_words=100000, filters='', lower=False, split=' ', char_level=False, oov_token='<oov>')\n",
        "    \n",
        "    print (type(self.parsed.train),self.parsed.train[0][0])\n",
        "    self.tokenizer.fit_on_texts([x for x,y in self.parsed.train])\n",
        "    \n",
        "    print ('\\n word_index',len(self.tokenizer.word_index),'<oov>',self.tokenizer.word_index['<oov>'])\n",
        "    print ('common',list(self.tokenizer.word_index.items())[:15])\n",
        "    print ('uncommon',list(self.tokenizer.word_index.items())[-15:])\n",
        "  \n",
        "    num_words= 2000\n",
        "    print ('keeping only ',num_words,'of',len(self.tokenizer.word_index))\n",
        "           \n",
        "           \n",
        "    word2index = dict(list(self.tokenizer.word_index.items())[:num_words-2])\n",
        "    word2index['<s>']=0  #keras tokenizer keeps 0 unused\n",
        "    word2index['<oov>']=num_words-1\n",
        "    #FOR NOW the start and end are both ZERO. maybe not good???\n",
        "    \n",
        "    num_encoder_tokens = num_decoder_tokens= num_words # len(self.tokenizer.word_index)\n",
        "    self.word2index = word2index\n",
        "    self.index2word = {index:word for (word,index) in self.word2index.items()}\n",
        "    self.MAX_SEQUENCE_LENGTH=8\n",
        "    \n",
        "    verbose=5\n",
        "    result = []\n",
        "    for rows in self.parsed:\n",
        "      input_texts = [x for x,y in rows]\n",
        "      encoder_input_data  = np.zeros( (len(input_texts), self.MAX_SEQUENCE_LENGTH),    dtype='float32')\n",
        "      decoder_input_data  = np.zeros( (len(input_texts), self.MAX_SEQUENCE_LENGTH),    dtype='float32') # shifted by 1\n",
        "      decoder_target_data = np.zeros((len(input_texts),  self.MAX_SEQUENCE_LENGTH, num_decoder_tokens),    dtype='float32')\n",
        "      \n",
        "      #input to decoder   <s> hello world\n",
        "      #target of decoder: hello world <s>\n",
        "      \n",
        "      for i, input_text in enumerate(input_texts):\n",
        "        input_text = input_text.split(' ') #BUG: we need to use tokenizer here!!!!\n",
        "        #pad with end token  hello world <end> <end> <end>\n",
        "        end_token='<s>'\n",
        "        input_text += [end_token for _ in range(self.MAX_SEQUENCE_LENGTH - len(input_text)+1)]\n",
        "        if verbose:\n",
        "          print ('input_text',input_text)\n",
        "          verbose-=1\n",
        "        # out : hello  world  <end>  (MAX_SEQUENCE_LENGTH=2)  <-encoder_input+ decoder_output(but one-hot)\n",
        "        #\n",
        "        # in: : <s>   hello   world  <- decoder-input\n",
        "          \n",
        "        for t, word in enumerate((['<s>']+input_text)[:self.MAX_SEQUENCE_LENGTH]):\n",
        "            one_hot = word2index['<oov>'] if word not in word2index else word2index[word]\n",
        "            decoder_input_data[i, t ] = one_hot\n",
        "            \n",
        "        for t,word in enumerate(input_text[:self.MAX_SEQUENCE_LENGTH]):  #last must be <end>=<s> token\n",
        "            one_hot = word2index['<oov>'] if word not in word2index else word2index[word]\n",
        "            encoder_input_data[i,t]=one_hot\n",
        "            decoder_target_data[i, t, one_hot] = 1. \n",
        "      print (decoder_target_data.sum(),len(input_texts)*self.MAX_SEQUENCE_LENGTH)\n",
        "      assert int(decoder_target_data.sum())==len(input_texts)*self.MAX_SEQUENCE_LENGTH #one-hot-encoding must always include one\n",
        "      \n",
        "      result.append( (encoder_input_data,decoder_input_data,decoder_target_data))\n",
        "\n",
        "    self.result= TVT(*result)\n",
        "\n",
        "  def one_x_as_text(self,x):\n",
        "    \"\"\" 1x20 or 20 input\"\"\"\n",
        "    if len(x.shape)==2: \n",
        "      assert x.shape[0] ==1  #can only work on batch of 1\n",
        "      x= x[0]\n",
        "    return [self.index2word[index] for index in x]\n",
        "\n",
        "  def one_y_as_text(self,y):\n",
        "    \"\"\" 1x20x2000 or 20x2000 input, in case of first will work on y[0]\"\"\"\n",
        "    if len(y.shape)==3: \n",
        "      assert y.shape[0] ==1  #can only work on batch of 1\n",
        "      y=y[0]\n",
        "      \n",
        "    best_token = np.argmax(y,1)\n",
        "    return [self.index2word[index] for index in best_token]\n",
        "\n",
        "  \n",
        "cache_dir='cache' \n",
        "#dataset('quora_dups','http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv',False,cache_dir) \n",
        "#dataset('bible4','https://codeload.github.com/keithecarlson/Zero-Shot-Style-Transfer/zip/master',extract=True,cache_dir=cache_dir\n",
        "#       pattern=('Zero-Shot-Style-Transfer-master/Data/Bibles/ASV/*/*.txt','Zero-Shot-Style-Transfer-master/Data/Bibles/BBE/*/*.txt')\n",
        "\n",
        "dataset = Dataset('bible_csv','https://codeload.github.com/ashual/style-transfer/zip/master',extract=True,cache_dir=cache_dir,pattern='style-transfer-master/datasets/bible-corpus/t_a*.csv')        \n",
        "row_parser= lambda line: (line.split(',')[4],line.split(',')[4]) #map x to x\n",
        "dataset.parse(row_parser,skip_first=True)\n",
        "dataset.fit()        \n",
        "x_train, x_train_d, y_train = dataset.result.train\n",
        "x_val, x_val_d,y_val = dataset.result.val\n",
        "x_test,x_test_d,y_test = dataset.result.test\n",
        "\n",
        "print ('train',x_train.shape,x_train_d.shape,y_train.shape)\n",
        "print('val',x_val.shape,y_val.shape)\n",
        "print ('train in MB x,y',x_train.nbytes/1e6,y_train.nbytes/1e6)\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fpath cache/datasets/bible_csv\n",
            "['cache/datasets/style-transfer-master/datasets/bible-corpus/t_asv.csv'] #lines 31104 first 3 lines\n",
            "id,b,c,v,t \n",
            " 1001001,1,1,1,In the beginning God created the heavens and the earth. \n",
            " 1001002,1,1,2,And the earth was waste and void; and darkness was upon the face of the deep: and the Spirit of God moved upon the face of the waters.\n",
            "train: 24883 val 3110 test 3111\n",
            "\n",
            "row_parser train: ('In the beginning God created the heavens and the earth.', 'In the beginning God created the heavens and the earth.') test: ('\"Much every way: first of all', '\"Much every way: first of all')\n",
            "limiting num_words in Tokenizer due to MEMORY BOUNDS\n",
            "\n",
            "REPLACE ME . BAD TOKENIZER!!!\n",
            "<class 'list'> In the beginning God created the heavens and the earth.\n",
            "\n",
            " word_index 13075 <oov> 13075\n",
            "common [('the', 1), ('of', 2), ('\"And', 3), ('and', 4), ('to', 5), ('unto', 6), ('in', 7), ('shall', 8), ('that', 9), ('he', 10), ('a', 11), ('his', 12), ('Jehovah', 13), ('they', 14), ('I', 15)]\n",
            "uncommon [('marvelled.', 13061), (\"do'\", 13062), ('purple', 13063), ('myrrh:', 13064), ('superscription', 13065), ('robbers;', 13066), ('scripture', 13067), ('bottom.', 13068), ('afar:', 13069), ('Arimathaea', 13070), ('already', 13071), ('dead:', 13072), (\"`mother'\", 13073), ('Joses', 13074), ('<oov>', 13075)]\n",
            "keeping only  2000 of 13075\n",
            "input_text ['In', 'the', 'beginning', 'God', 'created', 'the', 'heavens', 'and', 'the', 'earth.']\n",
            "input_text ['And', 'the', 'earth', 'was', 'waste', 'and', 'void;', 'and', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep:', 'and', 'the', 'Spirit', 'of', 'God', 'moved', 'upon', 'the', 'face', 'of', 'the', 'waters.']\n",
            "input_text ['\"And', 'God', 'said', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
            "input_text ['\"And', 'God', 'saw', 'the', 'light', '<s>', '<s>', '<s>', '<s>']\n",
            "input_text ['\"And', 'God', 'called', 'the', 'light', 'Day', '<s>', '<s>', '<s>']\n",
            "199056.0 199056\n",
            "24872.0 24872\n",
            "24880.0 24880\n",
            "train (24882, 8) (24882, 8) (24882, 8, 2000)\n",
            "val (3109, 8) (3109, 8, 2000)\n",
            "train in MB x,y 0.796224 1592.448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S_v9vKqIcTn6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "8bd73550-c03b-4626-8f83-33267ec264f9"
      },
      "cell_type": "code",
      "source": [
        "#show a sample of x_train\n",
        "for i in range(1150,1152):\n",
        "  print ('\\ntokens  :' , x_train[i])\n",
        "  print ('as words:',[dataset.index2word[index] for index in x_train[i] ])\n",
        "  print ('original:',dataset.parsed.train[i][0].split(' '))\n",
        "  print (dataset.one_x_as_text(x_train[i]))\n",
        "  print (dataset.one_y_as_text(y_train[i]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "tokens  : [  3. 218.  20. 114. 109.   5. 826.   4.]\n",
            "as words: ['\"And', 'Joseph', 'was', 'brought', 'down', 'to', 'Egypt;', 'and']\n",
            "original: ['\"And', 'Joseph', 'was', 'brought', 'down', 'to', 'Egypt;', 'and', 'Potiphar']\n",
            "['\"And', 'Joseph', 'was', 'brought', 'down', 'to', 'Egypt;', 'and']\n",
            "['\"And', 'Joseph', 'was', 'brought', 'down', 'to', 'Egypt;', 'and']\n",
            "\n",
            "tokens  : [  3.  13.  20.  21. 218.   0.   0.   0.]\n",
            "as words: ['\"And', 'Jehovah', 'was', 'with', 'Joseph', '<s>', '<s>', '<s>']\n",
            "original: ['\"And', 'Jehovah', 'was', 'with', 'Joseph']\n",
            "['\"And', 'Jehovah', 'was', 'with', 'Joseph', '<s>', '<s>', '<s>']\n",
            "['\"And', 'Jehovah', 'was', 'with', 'Joseph', '<s>', '<s>', '<s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kqkLdFGl0FMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "5ca046cc-53f9-467e-db2b-b6a5f37fe491"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding,CuDNNLSTM,Bidirectional,Concatenate\n",
        "\n",
        "# size of tokenizer indexes\n",
        "\n",
        "num_decoder_tokens = num_encoder_tokens = len(dataset.word2index) \n",
        "\n",
        "embedding_dim=300\n",
        "latent_dim = 256\n",
        "batch_size=64\n",
        "epochs=30\n",
        "bidi_encoder=True\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "\n",
        "shared_embedding = Embedding(num_encoder_tokens, \n",
        "                     embedding_dim, \n",
        "                     #weights=[word_embedding_matrix], if there is one\n",
        "                     #trainable=False,                            \n",
        "                     #input_length=MAX_SEQUENCE_LENGTH, if there is one\n",
        "                     )\n",
        "x = shared_embedding(encoder_inputs) \n",
        "if (bidi_encoder):\n",
        "  encoder_lstm=Bidirectional(CuDNNLSTM(latent_dim, return_state=True),merge_mode='concat')\n",
        "  x, forward_h, forward_c, backward_h, backward_c = encoder_lstm(x) #output,h1,c1,h2,c2\n",
        "  state_h = Concatenate()([forward_h, backward_h])\n",
        "  state_c = Concatenate()([forward_c, backward_c])\n",
        "else:  \n",
        "  encoder_lstm=CuDNNLSTM(latent_dim, return_state=True)\n",
        "  x, state_h, state_c = encoder_lstm(x)\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "decoder_latent_dim = latent_dim*2 if bidi_encoder else latent_dim #bi-di pass merge of h1+h2, c1+c2\n",
        "decoder_lstm = CuDNNLSTM(decoder_latent_dim, return_sequences=True,return_state=True) #returned state used in inference\n",
        "decoder_outputs, _, _  = decoder_lstm(shared_embedding(decoder_inputs), initial_state=encoder_states)\n",
        "decoder_dense  = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile & run training\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')  #30peocs loss: 0.2836 - val_loss: 0.4428\n",
        "print (model.summary())\n",
        "\n",
        "# now for the INFER models (re-arrangement of the prev one)\n",
        "# Remember that the training model varaibles were:\n",
        "#                                        decoder_outputs\n",
        "#encoder   --->    encoder_states  -->   decoder_lstm  \n",
        "#shared_embedding                        shared_embeddings  \n",
        "#encoder_inputs                          decdoer_inputs                  \n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_states_inputs = [Input(shape=(decoder_latent_dim,)), Input(shape=(decoder_latent_dim,))]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(shared_embedding(decoder_inputs), initial_state=decoder_states_inputs)\n",
        "\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_18 (InputLayer)           (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_17 (InputLayer)           (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, None, 300)    600000      input_17[0][0]                   \n",
            "                                                                 input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_4 (Bidirectional) [(None, 512), (None, 1142784     embedding_7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 512)          0           bidirectional_4[0][1]            \n",
            "                                                                 bidirectional_4[0][3]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 512)          0           bidirectional_4[0][2]            \n",
            "                                                                 bidirectional_4[0][4]            \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_12 (CuDNNLSTM)       [(None, None, 512),  1667072     embedding_7[1][0]                \n",
            "                                                                 concatenate_3[0][0]              \n",
            "                                                                 concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, None, 2000)   1026000     cu_dnnlstm_12[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 4,435,856\n",
            "Trainable params: 4,435,856\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e-YdH8dbJnGw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "0f8c7895-b93e-48d5-b6d2-3590e4cc0104"
      },
      "cell_type": "code",
      "source": [
        "'''#BIDI LSTM\n",
        "\n",
        "\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "\n",
        "shared_embedding = Embedding(num_encoder_tokens, \n",
        "                     embedding_dim, \n",
        "                     #weights=[word_embedding_matrix], if there is one\n",
        "                     #trainable=False,                            \n",
        "                     #input_length=MAX_SEQUENCE_LENGTH, if there is one\n",
        "                     )\n",
        "x = shared_embedding(encoder_inputs) \n",
        "encoder_lstm=Bidirectional(CuDNNLSTM(latent_dim, return_state=True),merge_mode='concat')\n",
        "\n",
        "#x, state_h, state_c = encoder_lstm(x)\n",
        "x, forward_h, forward_c, backward_h, backward_c = encoder_lstm(x) \n",
        "# encoder_states = [state_h, state_c]\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "#decoder_lstm = CuDNNLSTM(latent_dim, return_sequences=True,return_state=True) #returned state used in inference\n",
        "decoder_lstm = CuDNNLSTM(2*latent_dim, return_sequences=True,return_state=True) #returned state used in inference\n",
        "\n",
        "decoder_outputs, _, _  = decoder_lstm(shared_embedding(decoder_inputs), initial_state=encoder_states)\n",
        "decoder_dense  = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile & run training\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')  #30peocs loss: 0.2836 - val_loss: 0.4428\n",
        "print (model.summary())\n",
        "\n",
        "# now for the INFER models (re-arrangement of the prev one)\n",
        "# Remember that the training model varaibles were:\n",
        "#                                        decoder_outputs\n",
        "#encoder   --->    encoder_states  -->   decoder_lstm  \n",
        "#shared_embedding                        shared_embeddings  \n",
        "#encoder_inputs                          decdoer_inputs                  \n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_states_inputs = [Input(shape=(latent_dim,)), Input(shape=(latent_dim,))]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(shared_embedding(decoder_inputs), initial_state=decoder_states_inputs)\n",
        "\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "\n",
        "  \n",
        "'''"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"#BIDI LSTM\\n\\n\\n\\n# Define an input sequence and process it.\\nencoder_inputs = Input(shape=(None,))\\n\\nshared_embedding = Embedding(num_encoder_tokens, \\n                     embedding_dim, \\n                     #weights=[word_embedding_matrix], if there is one\\n                     #trainable=False,                            \\n                     #input_length=MAX_SEQUENCE_LENGTH, if there is one\\n                     )\\nx = shared_embedding(encoder_inputs) \\nencoder_lstm=Bidirectional(CuDNNLSTM(latent_dim, return_state=True),merge_mode='concat')\\n\\n#x, state_h, state_c = encoder_lstm(x)\\nx, forward_h, forward_c, backward_h, backward_c = encoder_lstm(x) \\n# encoder_states = [state_h, state_c]\\nstate_h = Concatenate()([forward_h, backward_h])\\nstate_c = Concatenate()([forward_c, backward_c])\\nencoder_states = [state_h, state_c]\\n\\n\\n# Set up the decoder, using `encoder_states` as initial state.\\ndecoder_inputs = Input(shape=(None,))\\n\\n#decoder_lstm = CuDNNLSTM(latent_dim, return_sequences=True,return_state=True) #returned state used in inference\\ndecoder_lstm = CuDNNLSTM(2*latent_dim, return_sequences=True,return_state=True) #returned state used in inference\\n\\ndecoder_outputs, _, _  = decoder_lstm(shared_embedding(decoder_inputs), initial_state=encoder_states)\\ndecoder_dense  = Dense(num_decoder_tokens, activation='softmax')\\ndecoder_outputs = decoder_dense(decoder_outputs)\\n\\n\\n# Define the model that will turn\\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\\n\\n# Compile & run training\\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')  #30peocs loss: 0.2836 - val_loss: 0.4428\\nprint (model.summary())\\n\\n# now for the INFER models (re-arrangement of the prev one)\\n# Remember that the training model varaibles were:\\n#                                        decoder_outputs\\n#encoder   --->    encoder_states  -->   decoder_lstm  \\n#shared_embedding                        shared_embeddings  \\n#encoder_inputs                          decdoer_inputs                  \\nencoder_model = Model(encoder_inputs, encoder_states)\\n\\ndecoder_states_inputs = [Input(shape=(latent_dim,)), Input(shape=(latent_dim,))]\\n\\ndecoder_outputs, state_h, state_c = decoder_lstm(shared_embedding(decoder_inputs), initial_state=decoder_states_inputs)\\n\\ndecoder_states = [state_h, state_c]\\ndecoder_outputs = decoder_dense(decoder_outputs)\\n\\ndecoder_model = Model(\\n    [decoder_inputs] + decoder_states_inputs,\\n    [decoder_outputs] + decoder_states)\\n\\n\\n  \\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "C2cufjsW_9Zw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "_ZC94QpK1bcw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "max_decoder_seq_length=dataset.MAX_SEQUENCE_LENGTH\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq,verbose=False):\n",
        "    assert input_seq.shape == (1, max_decoder_seq_length )\n",
        "    if verbose: print ('input_seq',input_seq.shape)\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    if verbose: print ('encoder result states_value','h',states_value[0].shape,states_value[0].mean(),'c',states_value[1].shape,states_value[1].mean())\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    #                      batch,word-number value is token (0 /122)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = dataset.word2index['<s>']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = []\n",
        "    while not stop_condition:\n",
        "        #start with encoder-state then change to self state\n",
        "        output_tokens, h, c = decoder_model.predict( [target_seq] + states_value) \n",
        "        if verbose: print ('output_tokens',output_tokens.shape,output_tokens.mean(),'h',h.shape,'c',c.shape)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens)# [0, -1, :])\n",
        "        if verbose: print ('sampled_token_index',sampled_token_index.shape,output_tokens.max(),sampled_token_index)\n",
        "        decoded_sentence.append(sampled_token_index)\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_token_index == dataset.word2index['<s>'] or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "          stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    res= np.array(decoded_sentence) #[dataset.index2word[index] for index in decoded_sentence]\n",
        "    return res\n",
        "\n",
        "\n",
        "def show_sample(data_type='val',teacher_forcing=False,range_start=1000,range_count=1):\n",
        "  \"\"\" \n",
        "    teacher_forcing : should we simulate training and feed the input. default False, as for test\n",
        "  \"\"\"\n",
        "  for i in range(range_start,range_start+range_count):\n",
        "      data={'train': dataset.result.train,'val':dataset.result.val , 'test':dataset.result.test}\n",
        "      one_x= data[data_type][0][i:i+1]\n",
        "      one_y= data[data_type][1][i:i+1]\n",
        "      if one_x.shape[0]==0:\n",
        "        print ('one_x',one_x.shape,'your range_start',range_start,' is more than size of data ',data[data_type][0].shape)\n",
        "      print ('\\ngold x: ',dataset.one_x_as_text(one_x))\n",
        "      print ('gold y: ',dataset.one_y_as_text(one_y))\n",
        "      if teacher_forcing:\n",
        "        p = model.predict([one_x,one_x])\n",
        "        print ('actual y:',dataset.one_y_as_text(p))   \n",
        "      else:\n",
        "        p = decode_sequence(one_x,verbose=False)\n",
        "        print ('actual y:',dataset.one_x_as_text(p))   \n",
        "\n",
        "     \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_0G8_Dr2bMM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "outputId": "bb4af463-3046-4cc1-da3d-3e9c6a86eb82"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "      self.losses = {'loss':[],'val_loss':[]}\n",
        "      \n",
        "    #def on_train_begin(self, logs={}):\n",
        "    #  pass  \n",
        "    \n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "      for loss in ['loss','val_loss']:\n",
        "        self.losses[loss].append(logs.get(loss))\n",
        "        \n",
        "        \n",
        "loss_history = LossHistory()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 24882 samples, validate on 3109 samples\n",
            "Epoch 1/5\n",
            "24882/24882 [==============================] - 17s 685us/step - loss: 3.1016 - val_loss: 2.3068\n",
            "Epoch 2/5\n",
            "24882/24882 [==============================] - 14s 559us/step - loss: 1.8910 - val_loss: 1.5566\n",
            "Epoch 3/5\n",
            "10688/24882 [===========>..................] - ETA: 7s - loss: 1.3845"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "24882/24882 [==============================] - 14s 560us/step - loss: 1.2799 - val_loss: 1.1548\n",
            "Epoch 4/5\n",
            "24882/24882 [==============================] - 14s 560us/step - loss: 0.9069 - val_loss: 0.8812\n",
            "Epoch 5/5\n",
            "24882/24882 [==============================] - 14s 562us/step - loss: 0.6368 - val_loss: 0.7031\n",
            "\n",
            "gold x:  ['<s>', '\"And', 'he', 'took', 'bread', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
            "gold y:  ['\"And', 'he', 'took', 'bread', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
            "actual y: ['\"And', 'he', 'took', 'counsel', 'of', 'God', 'shall', '<oov>', 'away', 'all', 'the']\n",
            "Train on 24882 samples, validate on 3109 samples\n",
            "Epoch 1/5\n",
            "  320/24882 [..............................] - ETA: 13s - loss: 0.4817"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "24882/24882 [==============================] - 14s 561us/step - loss: 0.4399 - val_loss: 0.5641\n",
            "Epoch 2/5\n",
            "24882/24882 [==============================] - 14s 557us/step - loss: 0.2978 - val_loss: 0.4836\n",
            "Epoch 3/5\n",
            "23488/24882 [===========================>..] - ETA: 0s - loss: 0.1952"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "24882/24882 [==============================] - 14s 559us/step - loss: 0.1945 - val_loss: 0.4024\n",
            "Epoch 4/5\n",
            "24882/24882 [==============================] - 14s 559us/step - loss: 0.1273 - val_loss: 0.3661\n",
            "Epoch 5/5\n",
            "24882/24882 [==============================] - 14s 561us/step - loss: 0.0846 - val_loss: 0.3223\n",
            "\n",
            "gold x:  ['<s>', '\"And', 'he', 'took', 'bread', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
            "gold y:  ['\"And', 'he', 'took', 'bread', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
            "actual y: ['\"And', 'he', 'took', 'bread', 'and', '<oov>', 'peoples', 'among', 'you', 'and', '<oov>']\n",
            "Train on 24882 samples, validate on 3109 samples\n",
            "Epoch 1/5\n",
            " 1600/24882 [>.............................] - ETA: 12s - loss: 0.0593"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "24882/24882 [==============================] - 14s 561us/step - loss: 0.0546 - val_loss: 0.2959\n",
            "Epoch 2/5\n",
            "24882/24882 [==============================] - 14s 562us/step - loss: 0.0391 - val_loss: 0.2817\n",
            "Epoch 3/5\n",
            "23872/24882 [===========================>..] - ETA: 0s - loss: 0.0273"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "24882/24882 [==============================] - 14s 562us/step - loss: 0.0276 - val_loss: 0.2720\n",
            "Epoch 4/5\n",
            "24882/24882 [==============================] - 14s 564us/step - loss: 0.0226 - val_loss: 0.2709\n",
            "Epoch 5/5\n",
            "24882/24882 [==============================] - 14s 560us/step - loss: 0.0208 - val_loss: 0.2688\n",
            "\n",
            "gold x:  ['<s>', '\"And', 'he', 'took', 'bread', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
            "gold y:  ['\"And', 'he', 'took', 'bread', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
            "actual y: ['\"And', 'he', 'took', 'bread', 'in', 'them', 'statutes', 'and', 'there', 'by', 'the']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Sm05AY862pem",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "8b6cfe28-0e1b-4ad1-d00f-16d536370d3f"
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "epochs = 10 \n",
        "for i in range(5):\n",
        "  print ('lr',K.eval(model.optimizer.lr))\n",
        "  #test model with teadcher-forcing\n",
        "  model.fit([x_train, x_train], y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=([x_val, x_val], y_val),callbacks=[loss_history,\n",
        "                                                               ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, mode='auto', min_delta=0.0001, cooldown=3)])\n",
        "  show_sample('val')      \n",
        "  #show_sample('val',False) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lr 0.001\n",
            "Train on 24882 samples, validate on 3109 samples\n",
            "Epoch 1/10\n",
            "24882/24882 [==============================] - 14s 567us/step - loss: 8.8329e-04 - val_loss: 0.1996\n",
            "Epoch 2/10\n",
            "24882/24882 [==============================] - 14s 559us/step - loss: 0.0031 - val_loss: 0.2590\n",
            "Epoch 3/10\n",
            " 8384/24882 [=========>....................] - ETA: 8s - loss: 0.0228"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "24882/24882 [==============================] - 14s 561us/step - loss: 0.0208 - val_loss: 0.2501\n",
            "Epoch 4/10\n",
            " 6720/24882 [=======>......................] - ETA: 9s - loss: 0.0095"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R_OBmX182djj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "e8cb4bcb-5edb-4c9a-9fab-5a66e470efb6"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(loss_history.losses['loss'][3:])\n",
        "plt.plot(loss_history.losses['val_loss'][3:])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lr 0.00020000001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3Xl8VPW9//HXmX0mM9kzAQIEZBEM\nUERBEQVUFLcu3rqgUtt6W2q1t1rrba2t1S5Qe1u9vbXtbbXa/urlKmq51qqVaquoGEFRQDYRlH3J\nvkwmk9nO749JwpZgtsnMJO/no3mQmTmZ8/kKzXvO93wXwzRNExEREckYllQXICIiIt2j8BYREckw\nCm8REZEMo/AWERHJMApvERGRDKPwFhERyTAKbxHhu9/9Lg888MAJj1m+fDlf+MIXuvy8iCSPwltE\nRCTDKLxFMszevXs5++yzeeihh5g/fz7z589n3bp1LFq0iHPOOYfvfOc77cf+7W9/47LLLuOiiy7i\n+uuvZ/fu3QDU1tZyww03cN5557Fo0SIaGxvbf2b79u0sXLiQ+fPn88lPfpL33nuvy7XV1dVxyy23\nMH/+fC655BIefPDB9tf+8z//s73e66+/nkOHDp3weRHpnC3VBYhI99XW1lJUVMSKFSv4+te/zje+\n8Q3+/Oc/YxgGs2fP5qtf/So2m4277rqLP//5z5SWlvLII4/w/e9/nz/+8Y889NBD5OXl8cgjj7B3\n714+9alPMW7cOOLxODfffDNf+tKXuPLKK1m7di033XQTL7/8cpfquv/++8nJyWHFihXU1dVx+eWX\nM23aNHJycnjhhRd49tlnsdvtPProo5SXl1NWVtbh85/5zGeS/F9QJLPpylskA0WjUS666CIAxo8f\nz+TJk8nPzycvL4+ioiIqKipYtWoVZ5xxBqWlpQBceeWVrF69mmg0yttvv83FF18MwPDhw5kxYwYA\nH374IdXV1VxxxRUAnHbaaeTn5/Puu+92qa6VK1dy7bXXApCbm8sFF1zAqlWryM7Opqamhr/+9a/U\n19fzuc99js985jOdPi8iJ6bwFslAVqsVl8sFgMViwePxHPVaLBajtraW7Ozs9ud9Ph+maVJbW0t9\nfT0+n6/9tbbjGhoaCIVCXHzxxVx00UVcdNFFVFdXU1dX16W6ampqjjpndnY21dXVFBcX88ADD/DC\nCy8wd+5cFi1axIEDBzp9XkROTOEtMkAVFBQcFbr19fVYLBby8vLIzs4+6j53TU0NAH6/n6ysLF54\n4YX2r9dff50LLrigS+csLCw86px1dXUUFhYCcOaZZ/Lggw+yatUqhg4dys9//vMTPi8inVN4iwxQ\ns2bN4u2332bPnj0APP7448yaNQubzcbUqVN56aWXANi9ezdr164FoKSkhCFDhvDCCy8AiVC/7bbb\nCAaDXTrn3LlzWbZsWfvPvvjii8ydO5fXX3+dH/zgB8TjcTweDxMmTMAwjE6fF5ET04A1kQFqyJAh\n/PjHP+amm24iEokwfPhwfvSjHwHwla98hW984xucd955jBkzhgsvvBAAwzC4//77ueeee/jFL36B\nxWLhi1/84lHd8idy6623cs8993DRRRdhsVhYtGgRU6ZMoaWlheeee4758+fjcDjIz89nyZIl+P3+\nDp8XkRMztJ+3iIhIZlG3uYiISIZReIuIiGQYhbeIiEiGUXiLiIhkGIW3iIhIhsmYqWKVlY0ff1A3\n5OV5qK3t2tzVTDHQ2qT2pDe1J72pPemtq+0pKvJ1+PygvfK22aypLqHPDbQ2qT3pTe1Jb2pPeutt\newZteIuIiGQqhbeIiEiGUXiLiIhkGIW3iIhIhlF4i4iIZBiFt4iISIZReIuIiGQYhXcvvfLKP7p0\n3H/9133s378vydWIiMhgoPDuhQMH9vPSSyu6dOwtt3yTYcNKklyRiIgMBhmzPGo6uv/+n7JlyybO\nOWc6F154MQcO7OcXv/gNP/nJD6msrKC5uZkbbljErFnn8LWvLeK2277Fyy//g6amALt372Lfvr18\n/evfZObMWaluioiIZJABE95P/HM7b22t6NKxJibRmInNamBgdHrc9Al+rjpvbKevX3PN51i+/AlG\njx7D7t07+c1vfk9tbQ0zZpzJxRdfxr59e7nrrjuYNeuco36uouIQP//5L3nzzTf4y1/+rPAWEZFu\nGTDh3R3hSJxAcwSv247T3jfr5U6cWAaAz5fNli2beOaZ5RiGhYaG+uOOnTJlKgB+v59AINAn5xcR\nkcFjwIT3VeeNPeFV8pHWvl/Br/9vI5edNYoLp4/ok/Pb7XYAXnzxBRoaGvj1r39PQ0MDX/rS5447\n1mo9/IHBNM0+Ob+IiAweg3LAmseVCNpgKNKr97FYLMRisaOeq6urY+jQYVgsFlau/CeRSO/OISIi\ncqzBGd7ORIdDsCXaq/cpLR3N++9vpanpcNf33Lnn8cYbr3HLLV/F7Xbj9/v5wx8e6tV5REREjmSY\nGdJvW1nZ2GfvVVXXzLd+W85Zk4bwpctO6bP3TbWiIl+f/ndKNbUnvak96U3tSW9dbU9Rka/D5wfn\nlber9co71LsrbxERkVQYlOHtctowjN7f8xYREUmFQRnetaFaXOPX0RhtSHUpIiIi3TYow/ujht2Q\nc5CAXWuNi4hI5hmU4Z3t8AIQMYIprkRERKT7Bml4ZwMQs4SIxuIprkZERKR7BmV45zgTQ+8NRwvN\nvZzr3RVXXPFJgkFd5YuISN8YlOHtsrqwYMWwt2i6mIiIZJwBs7Z5dxiGgdPIImZv6dUqazfccB1L\nltzHkCFDOHjwAN/5zjcpKvLT3NxMKBTiG9/4d045ZVIfVi4iIjKAwnv59md5t+K9Lh/fYjaBPcZv\nt/0Sx0cd7yx2qn8y/zL2sk7fY/bsc1m16lU++9mreO21lcyefS5jxoxj9uy5rF37FkuX/j8WL/5Z\nt9siIiJyIoOy2xzAwMAwIG72fMBaIrxfA+D111dy9tlzWLnyH3z1q//Kf//3A9TXH78dqIiISG8N\nmCvvfxl72Qmvko/127XLeK9+LbO8n+Izp0/t0TlPOmkM1dWVHDp0kMbGRl577RUKC/3cddeP2Lp1\nM7/61S969L4iIiInMmivvHPdOQDUtfRulbWZM8/mwQd/wznnzKG+vo6SkuEArFz5MtGoBsOJiEjf\nG7ThXeDOBaAx3LtdaubMOZeXXlrB3Lnnc9FFl7Js2VK+8Y2bKSubRHV1Nc8990xflCsiItJuwHSb\nd1ehNxHegUjvwnvixDJWrlzd/njp0qfavz/77DkAXHrpp3p1DhERkSMN2ivvIdn5AATjTSmuRERE\npHsGb3jnFAAQMrXymYiIZJZBG96F3mxM0yCCwltERDLLoA1vi2HBiDqJWppTXYqIiEi3DNrwBrDG\nXJjWEKZpproUERGRLhvU4W0zPWCJE4yo61xERDLHoA5vJx4AqoJaxlRERDLHoA5vlzULgMqmuhRX\nIiIi0nWDOryzrF4AqhTeIiKSQZK6wtqSJUtYv349hmFw5513MmXKlPbXli5dyjPPPIPFYmHSpEl8\n97vfTWYpHfLavRCH2lDv1jcXERHpT0m78l6zZg27du1i2bJlLF68mMWLF7e/FggEePjhh1m6dCmP\nPfYYO3bsYN26dckqpVM5Dh8AdQpvERHJIEkL7/LycubNmwfAmDFjqK+vJxAIAGC327Hb7QSDQaLR\nKM3NzeTk5CSrlE7luRLnbOzl+uYiIiL9KWnd5lVVVZSVlbU/zs/Pp7KyEq/Xi9Pp5Oabb2bevHk4\nnU4uvfRSRo8efcL3y8vzYLNZ+7TG0uJiqEsskVpU5OvT906VgdKONmpPelN70pvak956055+21Xs\nyIVQAoEAv/vd73jhhRfwer18/vOfZ+vWrUyYMKHTn6+t7du52EVFPsxIHDNiJ0AjlZWZf/VdVOQb\nEO1oo/akN7Unvak96a2r7eks4JPWbe73+6mqqmp/XFFRQVFREQA7duxgxIgR5Ofn43A4OP3009m4\ncWOySumUx2XDjDgJmdpZTEREMkfSwnvWrFmsWLECgE2bNuH3+/F6E1OzSkpK2LFjB6FQCICNGzcy\natSoZJXSKY/LjhlxEjMihGPhfj+/iIhITySt23zatGmUlZWxYMECDMPg7rvvZvny5fh8Pi644AL+\n9V//leuvvx6r1cqpp57K6aefnqxSOuVx2jDDTgDqWxop8hT0ew0iIiLdldR73rfffvtRj4+8p71g\nwQIWLFiQzNN/LI8z0W0OUB9uUHiLiEhGGNQrrFksBnbTDUBDeOAMhBARkYFtUIc3gIPE+ub1LVqo\nRUREMsOgD2+3JRHeuvIWEZFMMejD29O6s5iWSBURkUwx6MPbZ09MgK8NaU9vERHJDIM+vL0uF2bU\npnveIiKSMQZ9eGe1LtTSGAmkuhQREZEuGfTh3TbXuzkWJBqPprocERGRj6Xwdh1eqKUxrKtvERFJ\nfwpv1xFLpIZ131tERNKfwttlh9Yr74YWzfUWEZH0p/A+an1zhbeIiKS/QR/eWUd0mzdoupiIiGSA\nQR/ebXt6g668RUQkMyi8j+g2b9CANRERyQCDPrwddgtW0w6mhXoNWBMRkQww6MPbMAw8LjtG1KWd\nxUREJCMM+vCGw9PFGsKNxM14qssRERE5IYU3ifvesRYHcTNOUySY6nJEREROSOFNYrpYvG2VNU0X\nExGRNKfw5tglUnXfW0RE0pvCm6PnemuhFhERSXcKb7REqoiIZBaFN0cvkap73iIiku4U3oDbZcMM\nuwGoCdWmuBoREZETU3iT6DYnZseOQ+EtIiJpT+ENZLnsALgMH9WhGkzTTHFFIiIinVN4k5gqBuCI\ne2mJhWmKaqEWERFJXwpvDoe3NZYFQE2zus5FRCR9KbxpvecNGBEPANW67y0iImlM4c3hK+94yAVA\ndagmleWIiIickMIbsFosOB1Wos2J8NaIcxERSWe2VBeQLrJcNlqaEqPMq3XPW0RE0piuvFt5nDaC\nQQOX1aUrbxERSWsK71Yel51QS4x8V57meouISFpTeLfyOG2YQK4jR3O9RUQkrSm8W7WNOPfZcgDN\n9RYRkfSl8G7VFt4eSzagud4iIpK+FN6t2hZqceEFNNdbRETSl8K7VdvmJI54Irw14lxERNKVwrtV\nW7e5JZpY31xzvUVEJF0pvFu1hXekxaq53iIiktYU3q3a7nk3h6MUuDXXW0RE0pfCu1WWO3HPO9Ac\nJd+Vp7neIiKSthTerbI9DgAag2EKXHmA5nqLiEh6Uni38rrtGAY0NB0Ob831FhGRdKTwbmWxGPjc\ndhqCEfLd+YDmeouISHpSeB/Bl+U46spbI85FRCQdKbyPkO1x0NwSJbt1fXPN9RYRkXSk8D5CTlZi\n0Fo0rLneIiKSvhTeR/C1jjhvCEY011tERNJWUsN7yZIlXH311SxYsIANGzYc9dqBAwe45ppruOKK\nK/j+97+fzDK6LDsrMde7MRjWXG8REUlbSQvvNWvWsGvXLpYtW8bixYtZvHjxUa/fe++93HDDDTz1\n1FNYrVb279+frFK6rG2ud32T5nqLiEj6Slp4l5eXM2/ePADGjBlDfX09gUAAgHg8ztq1aznvvPMA\nuPvuuxk2bFiySumy7Ky2hVoimustIiJpy5asN66qqqKsrKz9cX5+PpWVlXi9XmpqasjKyuInP/kJ\nmzZt4vTTT+eb3/zmCd8vL8+DzWbt0xqLinxHPR7ZHAUgEoeJxSWwHVqsweOOS2eZVGtXqD3pTe1J\nb2pPeutNe5IW3sc6cuCXaZocOnSI66+/npKSEhYtWsQrr7zC3LlzO/352tq+vfdcVOSjsrLxqOfi\n4UR4H6oOYAvnArC7+sBxx6WrjtqUydSe9Kb2pDe1J711tT2dBXzSus39fj9VVVXtjysqKigqKgIg\nLy+PYcOGMXLkSKxWKzNnzuSDDz5IVild1jZgraEpMWANNNdbRETST9LCe9asWaxYsQKATZs24ff7\n8Xq9ANhsNkaMGMHOnTvbXx89enSySukyu82K22mloSmCx+bGZXVqrreIiKSdpHWbT5s2jbKyMhYs\nWIBhGNx9990sX74cn8/HBRdcwJ133skdd9yBaZqMHz++ffBaqmV7HDQEwxiGQb7r8FxvwzBSXZqI\niAiQ5Hvet99++1GPJ0yY0P59aWkpjz32WDJP3yO+LAcVdfXE4yYF7jz2Nx2kKRrEa89KdWkiIiKA\nVlg7To7HgWlCIBQh35XYXUxzvUVEJJ0ovI/ha53rrX29RUQkXSm8j5HtaV0i9ajw1r7eIiKSPhTe\nx2jbWaw+GCbfrX29RUQk/Si8j9G2s1hjU4SC1nvemustIiLpROF9jLb1zRuCYTw2N1l2DweaDqa4\nKhERkcMU3sfIPmLAmmEYlGaPoDpUS2M4kOLKREREEhTex2jbFrShKQzAqOyRAOxs2J2ymkRERI6k\n8D6G22nFZjVoCB4T3vUKbxERSQ8K72MYhkF2loOGpggAo7JHALCzYU8qyxIREWmn8O5A2/rmpmmS\nZffgdxeys2EPcTOe6tJEREQU3h3JznIQicYJhWMAjMoZSSgWoiJYmeLKREREFN4dah+0dsx974/U\ndS4iImlA4d0BX1bbEqnH3Peu35WymkRERNoovDuQ03rlXd86XazEOxSbxaZBayIikhYU3h1o21ms\nsbXb3GaxMcJbwv6mg4Rj4VSWJiIiovDuyJGrrLUZlTOCuBlnd+O+VJUlIiICKLw71N5tHjwc3qO1\n0pqIiKQJhXcH2rvNj7zy1kprIiKSJhTeHfC57Rgc3W2e78rDZ/dq0JqIiKScwrsDFouB12OnIRhp\nf84wDEbljKC2pY66lvoUViciIoOdwrsTifXNjx5ZfniHMV19i4hI6ii8O5HtcRBsiRKNHV7PXPe9\nRUQkHSi8O9HRdLHS7OEYGBpxLiIiKaXw7kTb+uaNR9z3dtvcFGf52d24VzuMiYhIyii8O5Hdur55\n/XH3vUfQEgtzoOlQKsoSERFReHemfWexTgetqetcRERSQ+HdiWPXN29zeNCaRpyLiEhqKLw7kZN1\n9M5ibYZlFeOw2HXlLSIiKaPw7oTP07qn9zFX3laLlZHZwznQdIhQNJSK0kREZJBTeHcip4OpYm1K\ns0dgYmqHMRERSYluh3c4HObAgQPJqCWt2G1W3E7rUUukttGgNRERSSVbVw763e9+h8fj4YorruCz\nn/0sWVlZzJo1i1tvvTXZ9aWUz3P8Eqlw5PagGrQmIiL9r0tX3i+//DILFy7khRde4Nxzz+XJJ5/k\nnXfeSXZtKZed5aAxGCFumkc9n+vMIcfh0zKpIiKSEl0Kb5vNhmEYvPrqq8ybNw+AeHzgrzCW7XEQ\nN02amo/uOjcMg1HZI6kPN1AbqktRdSIiMlh1Kbx9Ph+LFi1ix44dnHrqqbz88ssYhpHs2lKuo/XN\n25RmjwBgl7rORUSkn3Xpnvd9993HG2+8wbRp0wBwOp389Kc/TWph6SC7dbpYQzBCyTGvHbk96FT/\n5H6uTEREBrMuXXnX1NSQl5dHfn4+TzzxBM8++yzNzc3Jri3lTnTlPVI7jImISIp0Kby/853vYLfb\n2bx5M08++STz58/nxz/+cbJrS7n29c2Dx4e32+ZiSJafXdphTERE+lmXwtswDKZMmcKLL77Idddd\nx5w5czCPGYE9EJ3oyhsS973D2mFMRET6WZfCOxgMsmHDBlasWMHs2bMJh8M0NDQku7aUy+5kc5I2\nWqxFRERSoUvhfcMNN3DXXXdx9dVXk5+fzwMPPMBll12W7NpSrn3AWtPxq6yBdhgTEZHU6NJo80su\nuYRLLrmEuro66uvrue222wbFVDG304bdZqEu0NLh69phTEREUqFL4b127Vq+/e1v09TURDweJy8v\nj5/97GdMnjywp0gZhkG+z0l1Q8e7h1ktVkb4hvNh/U5C0RZcNmc/VygiIoNRl7rN77//fn7zm99Q\nXl7O6tWruf/++7n33nuTXVtaKMxx0RiM0BKJdfj6qPYdxvb2c2UiIjJYdSm8LRYL48ePb398yimn\nYLVak1ZUOinIcQFQ08nV96icxH1vrbQmIiL9pcvhvWLFCgKBAIFAgOeff37whHd2Iryr6jsJ79Zl\nUnXfW0RE+kuX7nn/4Ac/4Ec/+hF33XUXhmHwiU98gh/+8IfJri0tFOa4AajuJLzznLlkO3zaHlRE\nRPrNCcP72muvbR9VbpomY8eOBSAQCHDHHXewdOnS5FeYYm3d5p0NWmvbYWxD1SbqWurJdeb0Z3ki\nIjIInTC8b7311v6qI221dZt3duUNiZXWNlRtYmf9bm1SIiIiSXfC8J4xY0Z/1ZG2cn0OLIbR6T1v\nOPK+t3YYExGR5OvSgLWeWrJkCVdffTULFixgw4YNHR5z33338bnPfS6ZZfSK1WIhP7vzud6QuPLW\nDmMiItJfkhbea9asYdeuXSxbtozFixezePHi447Zvn07b731VrJK6DMF2S7qGluIxjrePcxtc1Gc\n5We3dhgTEZF+kLTwLi8vZ968eQCMGTOG+vp6AoHAUcfce++9fOMb30hWCX2mIMeFSedzvSHRdd4S\nC/Nh/a7+K0xERAalLk0V64mqqirKysraH+fn51NZWYnX6wVg+fLlzJgxg5KSki69X16eB5utb+eW\nFxX5unTciKHZsPEgUcPS6c9cZM7mzQNvs/Lg68wcN6Uvy+yWrrYpU6g96U3tSW9qT3rrTXuSFt7H\nOnL/77q6OpYvX84f/vAHDh3q2l7YtbXBPq2nqMhHZWVjl4712BIdFDt21zAs19XhMYUUc1LOKN7Z\n/x7vfvg+w33D+qzWrupOmzKB2pPe1J70pvakt662p7OAT1q3ud/vp6qqqv1xRUUFRUVFALz55pvU\n1NRw3XXX8bWvfY1NmzaxZMmSZJXSa+1zvU8w4twwDOaXngvA33e93C91iYjI4JS08J41axYrVqwA\nYNOmTfj9/vYu84suuojnn3+eJ554gl/96leUlZVx5513JquUXutKeAOUFUxguHcY71RsoCJY2R+l\niYjIIJS08J42bRplZWUsWLCAH//4x9x9990sX76cF198MVmnTJp8nwuDzldZa2MYBvNHnYeJyYu7\nXumX2kREZPBJ6j3v22+//ajHEyZMOO6Y4cOH8+ijjyazjF6z2yzkeB0nXKilzdSiSfg9haw++A6X\njL6APFduP1QoIiKDSVIXaRlICnJc1Da2EI+bJzzOYli4cOS5xMwY/9j9aj9VJyIig4nCu4sKsl3E\n4iZ1gZaPPXb6kFPJc+by+v7VNIYDH3u8iIhIdyi8u6hta9CudJ3bLDbmlc4hEo/w8p7Xk12aiIgM\nMgrvLvq4rUGPddbQGfjsXlbufYPmaHMySxMRkUFG4d1FbVuDduXKG8BhtXPeiHMIxUKs2r8mmaWJ\niMggo/DuosIuzvU+0lklM7AZVt7Yv+aoFeZERER6Q+HdRW1X3l3tNgfw2rOY6p/MoWAlO+p3Jqky\nEREZbBTeXeR0WPG67V3uNm8za9gZALy+b3UyyhIRkUFI4d0NBTkuahpC3eoCH5d7En53Ie9WbqAp\n0rebq4iIyOCk8O6GwhwXkWichmCkyz9jGAZnDZtBNB5lzcF3klidiIgMFgrvbmi/793NrvMzh56O\nVQPXRESkjyi8u6FtrndVfffmbfscXqYUlbG/6SAfNexORmkiIjKIKLy7obAHI87bnN06cG3Vfg1c\nExGR3lF4d0NX9/XuyPi8MRS48ll7aL1WXBMRkV5ReHdDYU73Vlk7ksWwMGvYDCLxCG8dXNfXpYmI\nyCCi8O4Gj8uO22ntUbc5JAauWQwLr+9/UwPXRESkxxTe3VSQ7aa6vntzvdvkOLOZXHgK+wIH2N24\nNwnViYjIYKDw7qbCHBehcIxgS7RHP9+24tqKnf/U1beIiPSIwrub2ncXq+tZ1/nE/HGMzR3N+qpN\nvFOxoS9LExGRQULh3U3d3df7WBbDwnUTrsRusfPEtqdpDAf6sjwRERkEFN7d1JOtQY/l9xTyyZPm\nE4g08eS2v/RVaSIiMkgovLupt1febc4dcTajs0tZW7Ge9ZUb+6I0EREZJBTe3dR+z7sXV96Q6D5f\nOPFKbBYbj7//f9pxTEREukzh3U0+jx2HzdKrbvM2Q7L8XDrqAhrCjfz5g7/2QXUiIjIYKLy7yTAM\nCnJc3d6cpDPnj5zNSN9wVh9cy8aqLX3yniIiMrApvHvAn+umKRSlMRju9XtZLVYWTrwSq2Hl4U1L\nWVfxXh9UKCIiA5nCuweGFWUBsL+qqU/er8Q7lBsmXQemyUMbH+W5j14kbsb75L1FRGTgUXj3QElh\nIrz39VF4A0wtmsTtp3+NAlcez3/0Ig9vXEoo2tJn7y8iIgOHwrsHSgq9QN+GNySuwL91+tcZl3sS\n6yrf4/53fkN1c02fnkNERDKfwrsHhhR4MID9lX0b3gBeRxb/NvXLzC6Zyb7AAZas+QUv7nqFSCzS\n5+cSEZHMpPDuAafdSlGuu8+vvNtYLVauPvlyrptwJRbD4Okdz/Oj1T9n7aF12sxEREQU3j01rDCL\nQHOEhqbejzjvzFnDpvODmd/mvBHnUNfSwCOb/pf71v6aLTXbqG6uoTnarIFtIiKDkC3VBWSqkqIs\n1m2vYl9VE9lZjqSdx2P38Nlxn2R2yVk8veN51lW+x6/W/b79dQMDl82Fx+bG7yvAZ/VR4Moj35VH\ngSuf0TmlOKz2pNUnIiL9T+HdQ8MKD08Xm1ial/TzFXkK+PLkz7GjbifvVmwgGG0mGG2mOdpMczRE\nINzE1srtmBzdrZ7nzOXysZcyzT8FwzCSXmc6i5txLIY6m0Qk8ym8eygZ08W6YkzuKMbkjurwtbx8\nNx/s20tNqJbq5lp2N+7jjf2reWTTUlbufYMrx3+KEb6Sfq03Xazat5pl257m7JIzuWT0PLz2rFSX\nJCLSYwrvHhpa4MEwYH9l+uzHbbPaKHQXUOgugDyYyXTOHXE2/7f9OTZUbeKnb/2Ss4ZN58LScylw\n5Z/wSjwQbiIUCyXeK8OZpslLu1cSM2Os3LuKNQff4eJR5zNn+FnYLP3zf4F9gQPk5Lv65VwiMvAp\nvHvIbrPibx1xbppm2nZJ+z2fMxIhAAAgAElEQVSFfGXK59lSs42nPvgrq/avYdX+NXjtWYzMHk6p\nbzil2SNwWh3satjLrsa97G7YQ3WoFoDpxdO4YvwnM/pKdUf9TiqaqzjVP4WTskfy/M5/sHz7s7y6\nr5zLx1zCJ4omJfXvb1P1+/xm/cPkvZfDhSPOZeawGdj76UODiAxM+g3SC8MKs3j3gyoamsLkeJ2p\nLueEJuaP587pt/LmwbfZUr2N3Y172Vz9Ppur3z/uWK89i1MKTqaxpZG3Dr3D1pptXHXyZzi1aHLa\nfkg5kfIDbwFwzrAzOTl/LDOGnsYLH/2Dlfve4KGNj/LJk+Zz0ajzk3b+tvXqG1oCLNv2NH/f9QoX\njz6fM4ecjtViTdp5RWTgUnj3QklRIrz3VTWlfXhDYv74rGFnMGvYGUCia7ztSrslFm6/Es935WEY\nBrF4jH/ueY3nPvo7D2/8Hz5RNImrx3+GHGd2t88dN+P8afMyPmrYzfzS8zhjyLQOg8s0TTbXvM+7\nFe/xWft83HT/XEcKRUO8U7GBAlc+4/JOAhIfTq4Y/ynOKTmTB9b9nmc//Dtjcka3v96XTNNkU/VW\nvPYs7rv4Lpa9+xyv7ivnf7f+mb/vfJnrJl7J+LwxfX5eERnYrPfcc889qS6iK4J9sIPXkbKynL1+\nz/qmMGvfr6R0iI8xw3L6qLKe626bHFYHfk8h4/JOYkL+OIZmFeOxu9uvri2GhTG5ozjVP4W9jQfY\nUrONNw68Rb4zl2HeoV0+j2maPLntGcoPvEUw2sx7VZt5t+I9cp05FHuKMAyDuBnn3cr3+NPmx/nH\nnlfZG9jPqj1vMz53DLnOnv+3XXPwHdZVvsf5I845LiS9jixKs0ew+uBattRs44whp+Gw9u20v72B\n/fxjz6tM9U/m/LFnUeoaxcyhpxONx9hWt4MNVZuYMWQaLlv6f/g7Vl/8fyidqD3pbbC2Jyur498N\nCu9eME145d19FGS7mDq2sI8q67lk/eP22rM4Y+hpZDt8bK55n7UV66kMVnNy/hjslo+fQ/7S7pWs\n2PVPhmUN4eunLiIWj7G19gPWVqxja+0HhKIh/mfrU7y2r5zGcIDT/J/gjCGnsbF6C2sPrWdc3knk\nuXJ7VPsT256mrqWBz51yFW6b+7jX8125WAwLG6o2cbDpEKcXT+3TWwNv7H+LbXU7mF96LmP9pQSD\nYVw2F5MKJ+C2uVlXuZGDwYo+P29/GKy/TDOF2pPeehve6jbvhSH5HiyG0e/TxVLBYliYPXwmE/LH\n8cfNj/HWoXf4sP4jPn/KNZ1OXQN4++C7PL3jeXKdOdz0iRvIc+Vy3cQrOH/kOTzz4QrWV27kw/pd\nWAwLM4dO54LSuRR7igAo9Q/hgTf/yK/WPcRXp9zQ7W7tg00VfFi/i4n548l3dT4X/8LSuXxQu4ON\n1Vv5557XOH/k7G6d50Q2VW/BYliYmD/+uNdmD5/Je1Wb2VS9ldf3r+ackjP77LwiMrBpxYpesNss\n+PPc7K9sGjRrjvs9hXxz2k1cPOp8akJ1/Oc7/82zH67ocOOUbbXb+dOWJ3BZXe3B3WZIVjGLJl/P\nN0+7mc+OvYwfzPw2Cyde2R7cALNGTudfJy0kGo/x6/UPs7Xmg27V+uaBtwGYOfT0Ex5nMSx8vmwB\nPoeXv+z4G7sa9nTrPJ1pDAfY2bCH0dmleOyeDs/7uVOuwmNzs/yDv1IRrOqT84rIwKfw7qWSwiyC\nLVHqAgOnO+fjWC1WLjtpPrdOu5E8Vy5/2/kPbnv1Ln6y5hcs3fIkr+59g/WVG/ndhj8B8JUp11PS\nyT3yk3JKOW/k7E6vjKcWTWLR5OsxMfnvDX/grYPvdumDUiwe482Db+OxuZlSWPaxx2c7fHzhlGuI\nm3Ee3riU5mjzx/7Mx9lSsw0Tk0mFEzo9JteZw9UnX044HuFPmx8nFo/1+rwiMvApvHvpyGVSB5ux\nuaO5c8atzBs5h5G+4RwKVvDGgbdYtu1pHnzvT4RiIa6feBXj88b26jyTCidy4+QvYGDwx82P8at1\nv+dQsPKEP7OpeiuN4QDTh5yKvYtru0/IH8f80nOpDtXwq3UP93ov9Y1VWxL1F0w84XGnF0/l9OKp\nfNSwmxd3v9Krc4rI4KB73r1UUnR4mdSy0fkprqb/uW1uLh97KZC42j0YrGB34z72NO6j1Dec04ec\n2ifnmVgwnu/OuI0ntj3N5pr3WbL6fuaVzmV+6XkdbrxS3t5lPqNb57lk9AVUh2p569C7/OStX3Dt\nhCuY5p/S7Xpj8Riba7aR58xlaFbxxx5/9fjPsL3uI5776EVOyT+ZkdnDu31OERk8dOXdS4evvNNn\nmdRUsVqslHiHMnPo6Vw1/tOcMfS0Pn3/Ik8BN33iBr486XN4HV5e2PkPfrz6Pl7dW86uhj2EW++7\n17c0srF6CyO8wxjhG9btNnz+lAUsnHAlsXiMhzf+D/+79SnCse7dFvmoYTfN0WYmFU7s0ihyj93D\nwolXEjfj/GHz/xKM9L7bXkQGLl1599KQfA9Wy+AYcZ4ODMNgqn8yE/LH87edL/HPPa+xbNv/JV7D\noNhThNPqJG7GOXPY9B6fY+aw6YzOKeWRTUtZtX8NO+p2csOk6zq9d3+sti7zsoKTu3zeifnjmTdy\nDi/tXskfNv8vX53yRe2CJiId0m+GXrJZW0ecVw2eEefpwGVzcvnYS/n+Gf/OdROuYM7wszgpZxR1\nLQ3satyDy+pkenHvuuyHZPn599O+xpzhszgYrOC+tb/ucDnZjmyq3ordYuPkbt7v//SYizkl/2Q2\nV7/PMzte6EnZIjII6Mq7D5QUZnGgOkhtYwv52do5qj8VeQoo8hze+cw0TapDNVgNK1kdTM/qLrvV\nzlXjP83Y3NH8afPj/PeGP3DNyZ/lrBNc1deEatnfdJBTCk7u9optFsPCF8uu5WdrH+DF3a8wzDuE\nGUOm9bYZIjLA6Mq7DwzmEefpxjAMCt0FPV6RrTPT/FP4t6mLcFtdLN36JM99+PdOe1o2VW8FPn6U\neWc8djc3Tv4CLquLpVuf6rN55yIycCQ1vJcsWcLVV1/NggUL2LBhw1Gvvfnmm1x11VUsWLCA73zn\nO8Tj8WSWklQlRV4A3fce4MbkjuKbp91EgSuf53e+xP9sfbLDedkbqxLhXVbQ+fzuj1Oc5eeLZdcQ\ni8f43Yb/R31LQ4/fS0QGnqSF95o1a9i1axfLli1j8eLFLF68+KjXv//97/PLX/6Sxx9/nKamJl57\n7bVklZJ0bVfeCu+BrzjLz+2n38xIXwlvHnib/3znt/xz96vsathDLB4jHIvwfu12hnj8FLp7N3Vw\nUuFEPj3mYurDDfzy3Qf520cvsbXmA5qjoT5qjYhkqqTd8y4vL2fevHkAjBkzhvr6egKBAF5v4ip1\n+fLl7d/n5+dTW1ubrFKSrjjPjdViqNt8kMh2+Ljl1Bv5f5sfZ0PVJj5q2AWA0+qg2OMnEo9QdoJV\n1bpj3sg5VDZXs2r/ap796O9AYlT90KxiSrNHkO/KJdeZS64zm1xnDrnObNw2d8ZtciIi3ZO08K6q\nqqKs7PCylPn5+VRWVrYHdtufFRUVrFq1iltuueWE75eX58FmO37/594oKvL12XuV+L0cqG6isNCb\n0l+cfdmmdJC+7fHxvaFfo7Kpmi2V29lauZ0tldvZ3bgXgHPHnUFR4fG196Q9t/i/wOebL+eDmp1s\nq/qQbdUfsaNmJ/ubDnZ4vNPmpMCdS4EnL/HlziPH5cPryMLnzEr86cgiy+HBbXNhsfS8Ay59/356\nRu1Jb2rPYf022ryjwT3V1dXceOON3H333eTldb7rE0BtbbBP6ykq8lFZ2dhn71ec62b3wUa2bq+k\nMPf4rSf7Q1+3KdUyoz0OJmadwsSsU7h8VGIzksZwgDyz6Ljae9ceC6McJzFq2ElcOCyxgltFcxV1\noXrqWuqpa2mgrqWOupZ6alvqqQvVs7/xUJfe2Wl14La5cVmduGwunFYHTqsTp9WBw+rAaXXgsjpx\n2py4rS6cNicuq5PigjxCgRhOqxNX63N2iz1jr/oz499b16k96a2r7eks4JMW3n6/n6qqw7skVVRU\nUFR0eMeoQCDAl7/8ZW699VbOPvvsZJXRb0YPzeatrRV8sK8+ZeEtqedzePE5vEk/j9ViZWhW8QmX\nXg3HwokwD9UTiDTRFAnSFAkSjLb92UwoGiIUDdEcDRGINFHZXE3M7PnmKAYGLpsLl9WJ2+bCZXPi\nsDiwW+04LHbsFjs2qw2Hxd7+IcFxxIcEu8WWOMZiw26xYbPYjvgQ4cRusWnhGhGSGN6zZs3igQce\nYMGCBWzatAm/39/eVQ5w77338vnPf57Zs/tu7+RUmlia6DnYsquWmWVDUlyNCDisDvyeIvxHbLPa\nFdF4lHAsTEv7VwuhaAuhWAuhaCjxONaC1WlS09BIKNZCS+vrRx5b11JPKNhC3OzbmSQOix27tfWD\ngGHFZrVjN6w4rE48dhceW+J2gMfmxmVztX8gsFtsiWMtiQ8PbR8a2r7PjjoxTTNjew5kcElaeE+b\nNo2ysjIWLFiAYRjcfffdLF++HJ/Px9lnn83TTz/Nrl27eOqppwC47LLLuPrqq5NVTtKN8HvJctnY\nuitzB96JANhar3g72oP8SF3p9jNNk6gZIxqPEI5FicQjROIRwrHwMR8QEo+jZpRILEo0HiHS+n04\nfvj1cOuHiUg8SiQeJRqPEgq3EI5HiMQimPRulUMDA7u1tVfA4sBlc7V/EHDb3LjtiV6Ftt4Eu8XW\n/iEgy+bGY/eQZffgsXk63DBHpK8k9Z737bffftTjCRMOj8DduHFjMk/d7ywWg5NH5vHOtkoq65op\nUte5CIZhYDcSXeDuJI+wiZtxQtEWmqPNBKPNNEebaY6GiLYGfVvYh2Ph1g8QEcJHfJDAFqexOdj+\nQaElFqaquZpQrKVH9dgttkT4W12tYwJch8cU2JxHjS3IsmeR7fCR3XrbxWf3YrX07QBdGVi0PGof\nmliaCO8tu2oV3iL9zGJY8NjdeOxuCj7+8ON01pMQi8cIxVo/FESaaYm1EI5HicTC7Vf8oVgLwWgz\nwcjh8QRNkWDrbYQQdeGGbu1MZ2C0Xu27cNvd7Vf+niMet/cG2Fzt4wxcNlf74EEZ2BTefWhC633v\nrbtqmf2J7m1FKSLpyWqxkmVJdIfTi8/ksXiMllhL+ziClli4fXxAU6SJhnCAxnAjjeEADeHG9g8B\nh4KV3d6SFhLL7Prsiav5bIePbKcPl9XVelvE2jow0IrL6sTn8OK1J47z2N0aFJgBFN59aFiBh+ws\nB1t21Wrgi4gcxWqx4rF4PnYsQUei8SjN0VD77YBgpPnw7YFIiOZY6KjBgs3REM3xILXBeg4FK7p1\nLothSQz2a50e2Na177K5Dn8QOOLLY08MEHRZnerq70cK7z5kGAYTS/NYvfkQB6qD7cumioj0hs1i\n6/Y0xLbbALF4jIZwIw3hxtbBfjGirff/o/EozbEQgXBT4qo/0kRjOEBTpImWWJimUC2haEuXBwK2\nrRlgM6xgGBgkbgFggNPiwOc8Ovh9Du/h2wE2V2K9AZsLi2HBalgwMHQR1AmFdx9rC+8tu2oV3iKS\nclaLlTxXbo932jNNk0jrlX9j64eA+nAjjS2J74OtAwObW9cNCEZDRM0YZtyE1tg3MamL1rMnsL/b\n5zcwsBgWLBYLlrbvsWAYBk6rk6zWEf6Jryx89iyynT5yHNnttwuybB5MTOJmvPXLxMRMLCyUobMC\nFN597Mj53uefNjzF1YiI9I5hGDisdhxWOznO3i1PGoq2tPcCNLTe329uXSiobdGg5liIeLw1ZGkN\nWjOOxWYQjkSJm3FMMxHEoVgLB5oOEolHe1yTzWI7al2AxG0Ce/t0QKfVgc1iw2pYsRlWrK1jBqyG\ntX0hobZFhfJdeYzwlfTqv1GX6+6XswwiRbluCnNcvL+7lnjcxGJRl4+ICJAYCW9z4vcUdvtnT7Su\nQDgWpikSJNDa7d/+AaG1d6ApEkxcsR/xBdDSOkug7cNDdaiWaC8+CADce/b3+2WVRYV3EkwozeP1\nDQfYUxGgdMjAWkhfRCTdtC2U09NbA0eKxWNHzP+PEI6HW8cHxIiZbX/GiLaOHYi0jx+I4HV48dr7\n53apwjsJJraG95ZdtQpvEZEMYrVYcVusuG2uVJdyQprMlwQTRh6+7y0iItLXFN5JkOdzMrTAw7Y9\ndURjfbspg4iIiMI7SSaU5tESibHzwMDZf1ZERNKDwjtJJrZ3ndekuBIRERloFN5JMqE0DwPd9xYR\nkb6n8E4Sr9vOiGIv2/c1EI7EUl2OiIgMIArvJJpYmkc0FueDffWpLkVERAYQhXcSTRmTWEXozY0H\nU1yJiIgMJArvJDp5ZC7FeW7WbK0g0BxJdTkiIjJAKLyTyGIYzJlaQiQa5w1dfYuISB9ReCfZrMlD\nsFktvPLuPkyza3viioiInIjCO8l8HgfTJxRxsCbI+7vrUl2OiIgMAArvfjD31MT+ri+/uy/FlYiI\nyECg8O4HY0tyKCnK4p1tldQ3hVNdjoiIZDiFdz8wDIO5U0uIxU1e37A/1eWIiEiGU3j3k5llQ3DY\nLaxct594XAPXRESk5xTe/cTjsnHmKcVU1YfY+JE2KxERkZ5TePejtoFrr2jgmoiI9ILCux+NGpLN\nqCE+1u+ooqYhlOpyREQkQym8+9m5p5ZgmvCPd/amuhQREclQCu9+NuOUYvJ8Tl58ay9Vdc2pLkdE\nRDKQwrufOe1Wrpg7hmgszhMvb091OSIikoEU3ilw5inFjCnJ5u33K9m6qzbV5YiISIZReKeAYRhc\nO288AP/70gea9y0iIt2i8E6R0UOzmTV5CHsrA6xcr1XXRESk6xTeKfTZOWNwOqz836sf0hSKpLoc\nERHJEArvFMr1OvnUWaMINEf4y+sfpbocERHJEArvFJt3+gj8uW7+uXYf+6qaUl2OiIhkAIV3itlt\nFq4+fyxx0+ThZzcTaFb3uYiInJjCOw1MHVvIrElD2HmwkZ/8z1qq6rV4i4iIdE7hnQYMw+CLl07k\nohkjOVAdZPGja9l9qDHVZYmISJpSeKcJi2Fw1XljWXD+OBoCYe5d+g6bd2rrUBEROZ7CO81cOH0E\nX/l0GdFYnP98Yj3lGw+muiQREUkzCu80NGNiMbddNRWH3cpDz27md89sojEYTnVZIiKSJhTeaWpC\naR7fu/40ThqWzerNh7jr96tZ+35FqssSEZE0oPBOY0MLsrhz4Wlcde5Ygi0xfv1/G/ntXzbSoKtw\nEZFBzZbqAuTELBaDi84YySfGFvDI81tYs6WCjR/W8ImxhUwZU0DZ6Hy8bnuqyxQRkX6k8M4QQwuy\n+M51p/Hi23t4Yc1uyjcdpHzTQQwDxpTkMGl0PuNLC7CYcXJ9TvK8Duw2KwDhSIxAc4TGYIRAKILT\nZqUoz022x45hGClumYiIdJfCO4NYLAbzZ4zkwukj2FMRYMOOajZ8WM2OffVs31sPrx29PnqWy0Yk\nFicciXf4fk6HlaIcN8V5boYUeBgzLIeTSrLJ9jj6ozmDWk1DiIf+upnSIT4+O+ek9g9aIiJdofDO\nQIZhMLLYx8hiH5e1bmzywd46wnHYc6CB2sYW6gKJL4fNitdjx+e24239CoVjVNQ1U1EbpKIuyN7K\nwFHv789zM2ZYNiP8PmxWA8MwMAwwAKvVwphh2QwrzMr4q/adBxt4Y+NBThqazWkn+7Hb+mcISG1j\nC//x2LtU1Dbz/p46Nu+s5cZPlzGsMKtfzi8imU/hPQB43XZOHVdEUZGPysrurcxmmiYNTWH2VATY\nsb+BHfvq+XB/A+WbDlG+6VCnP5ef7WTySQVMGl3AKaPycDtP/E8pbpocqglS09BCNBYnGjOJxeNE\nY3EMw6Ag20VRrpscrwNLkj8U1AVaWL7yQ1a9dwCz9TnvSx8wa/IQ5kwtYUi+J6nnbgvui88cSTAU\nZeW6/fzw/73FtfPGc86UoRn/oUhEkk/hPcgZhkGO10mO18mkkwqARNAerA5yoLoJ0wSTRMibJjSH\no2zdVcumj2pYuW4/K9ftx2ox8Oe5KcpNfPlz3RTmumhuibLzYCO7DzayqyJASzj2sfXYbRaKct0U\n5rgoLsjCZoDP48DrtuPz2Ble5KUgx9WjtoYjMVa8tYfny3fREokxvMjLp2aN4sMDDby+4QAr1uxh\nxZo9TBiZy1mThnLq+EKyXH03GLC2McTPHnuXQzVBLj5zJFfMGYNhGJSNyuePf9vKH/+2lU0f1fD5\ni07G04fnFZGBxzBN0/z4w3pmyZIlrF+/HsMwuPPOO5kyZUr7a2+88Qb3338/VquV2bNnc/PNN5/w\nvbp7RflxenKVmu76s03xuMmHBxp4b0c1m3fWcKA6SLAl2uGxhgHDCrIYWeyjON+N3WbBZrFgsxpY\nrRZicZOq+mYq60JU1jVTVddMU6jj94LEVf+44bmMG57D+OG5DCnwYLMe3+XdEo6xr6qJvZUB9lYG\neHdbJdUNLfg8dv5l9kmcM2UYFkviKjcSjfPuB5W88u4+tu6uA8BqMZg4Ko/pJ/s5dXxRr0b1NwTD\n3P/EenYfbOTC6SO4+ryxR11hV9U38+BfN7N9bz1Oh5VTxxZy+gQ/k0/KT9v74QPt/0NqT3obrO0p\nKvJ1+HzSwnvNmjU8/PDD/O53v2PHjh3ceeedLFu2rP31Sy65hIcffpji4mIWLlzID3/4Q8aOHdvp\n+ym8P16q29QUilDVGsCVdc047FZKh/gY4ffitHcvgIKhKHa3nV176wgEIzQGw9Q3hdl5sJEP9tbR\nGDx661Sb1cDlsOFyWHE5rIQjcSrrmjGPOeaC00dw6cxReFyddzodqg3y9tYK3t5aya7WDWIshsEI\nv5fCHBcFOS4KshN/5nqduJ1WPE4bbqet/b55Y3Piv0XiQ0kzb246xL6qJuadNpxr5o3rsGs8Fo/z\nwurdrFy3n6r6EAAuh5WpYwuZdFI+uV4nPo8DnycxdqGjDyz9KdX/3vqa2pPeBmt7OgvvpHWbl5eX\nM2/ePADGjBlDfX09gUAAr9fLnj17yMnJYejQoQDMmTOH8vLyE4a3pL8sl52sIXZKh3T8j607PC4b\nRYVe7B18tjRNk4M1QT7YW88He+uoaWghFI4RCkdpicSoaWjBajU4eWQuJUVehhdlMbzIS0lRFi7H\nx/+TL87zcOnMUVw6cxQVdc2s3VrB2+9Xsrcy0B7mnbFaDKwWg3D0+BH+l80azeVnj+r0nrbVYuHS\nmaO45MxSdh5s5O2tFby1tYI3Nx/izc3Hjz9wO61YDAOLxTjiT44YYNj6Z+tjaP0eWh8fHohI6/Gt\n/2t/ndbv2ytuOw6w261EozHajzQMjmzZsc08tt3tNXX05JFPdfDg6DOd8Mc7PLKjvwOn00bLMb1H\nvR1+0NXxC70e5dDBG3TUnl6+ZUo5nXZaWiIffyD0/i+uh3K9DhacN669Ry+ZkhbeVVVVlJWVtT/O\nz8+nsrISr9dLZWUl+fn5R722Z8+eE75fXp4HWx93H3b2iSaTDbQ2ddYevz+bKROG9Mv5y8b5uf6T\niQ8NdYEWKmtbR+rXNFPbGCIYihIMRdr/jMbiFOa6Kc7PojjfQ3GBh2GFWQz3d/3vxu/PZsaUEkzT\nZPveOrbtrqOhdQZBfVOYhkCYQHOYWNwk3vZlJv40ATNuEjNNwCQeB5PEmIXEZyGTeGIgQ+t4hsRz\nh8c3tD95xOuHj2l7PvFNR8eJDE5up5UbPj0ZXxen2/bm93W/DVjrbe98bW2wjypJGGhdMDDw2pSu\n7clz28hzZ3PysOxu/2xP2pPrsjFjfGG3fy7ZPu7vxzwi2GkP/dbnOvh10PGvCPO41zv9TXLMOTp/\nz47PXVjoparqyGmTXf+d1dGR/fVhprPfrQUFXqqrAx2+9rHv2ZuCkqSwwEtVV9qTwuKdDiuhphZC\nTS0fe2zadpv7/X6qqqraH1dUVFBUVNTha4cOHcLv9yerFBFJgbYu4yN64Em/ztjDstx2gicYC5Fp\ncrxOws0DZx+Egdae3kraiJdZs2axYsUKADZt2oTf78fr9QIwfPhwAoEAe/fuJRqN8vLLLzNr1qxk\nlSIiIjKgJO1j5rRp0ygrK2PBggUYhsHdd9/N8uXL8fl8XHDBBdxzzz1885vfBBIjz0ePHp2sUkRE\nRAaUpPYR3X777Uc9njBhQvv306dPP2rqmIiIiHSN9vMWERHJMApvERGRDKPwFhERyTAKbxERkQyj\n8BYREckwCm8REZEMo/AWERHJMApvERGRDJO0/bxFREQkOXTlLSIikmEU3iIiIhlG4S0iIpJhFN4i\nIiIZRuEtIiKSYRTeIiIiGSap+3mnqyVLlrB+/XoMw+DOO+9kypQpqS6pR7Zt28ZNN93EF77wBRYu\nXMiBAwf41re+RSwWo6ioiJ/97Gc4HI5Ul9kl//Ef/8HatWuJRqN85StfYfLkyRnblubmZu644w6q\nq6tpaWnhpptuYsKECRnbnjahUIjLLruMm266iZkzZ2Zse1avXs0tt9zCuHHjABg/fjxf+tKXMrY9\nbZ555hl+//vfY7PZ+PrXv87JJ5+csW168skneeaZZ9ofb9y4kccee4x77rkHgJNPPpkf/OAHKaqu\n+5qamvj2t79NfX09kUiEm2++maKiot61xxxkVq9ebS5atMg0TdPcvn27edVVV6W4op5pamoyFy5c\naH7ve98zH330UdM0TfOOO+4wn3/+edM0TfO+++4zly5dmsoSu6y8vNz80pe+ZJqmadbU1Jhz5szJ\n2LaYpmk+99xz5oMPPmiapmnu3bvXvPDCCzO6PW3uv/9+81/+5V/MP//5zxndnjfffNP8t3/7t6Oe\ny+T2mGbi/zcXXnih2djYaB46dMj83ve+l/FtarN69WrznnvuMRcuXGiuX7/eNE3TvO2228xXXnkl\nxZV13aOPPmr+/Oc/N6Rnb+4AAAcQSURBVE3TNA8ePGjOnz+/1+0ZdN3m5eXlzJs3D4AxY8ZQX19P\nIBBIcVXd53A4eOihh/D7/e3PrV69mvPPPx+Ac889l/Ly8lSV1y3Tp0/nv/7rvwDIzs6mubk5Y9sC\ncMkll/DlL38ZgAMHDlBcXJzR7QHYsWMH27dvZ+7cuUDm/lvrTKa3p7y8nJkzZ+L1evH7/fzoRz/K\n+Da1+fWvf82Xv/xl9u3b195LmmntycvLo66uDoCGhgZyc3N73Z5BF95VVVXk5eW1P87Pz6eysjKF\nFfWMzWbD5XId9Vxzc3N7t1hBQUHGtMtqteLxeAB46qmnmD17dsa25UgLFizg9ttv584778z49vz0\npz/ljjvuaH+c6e3Zvn07N954I9dccw2rVq3K+Pbs3buXUCjEjTfeyLXXXkt5eXnGtwlgw4YNDB06\nFKvVSnZ2dvvzmdaeSy+9lP3793PBBRewcOFCvvWtb/W6PYPynveRzAG6Omwmtuull17iqaee4pFH\nHuHCCy9sfz4T2wLw+OOPs2XLFv793//9qDZkWnuefvpppk6dyogRIzp8PdPaM2rUKL72ta9x8cUX\ns2fPHq6//npisVj765nWnjZ1dXX86le/Yv/+/Vx//fUZ/W+uzVNPPcXll19+3POZ1p6//OUvDBs2\njIcffpitW7dy88034/P52l/vSXsGXXj7/X6qqqraH1dUVFBUVJTCivqOx+MhFArhcrk4dOjQUV3q\n6e61117jt7/9Lb///e/x+XwZ3ZaNGzdSUFDA0KFDmThxIrFYjKysrIxtzyuvvML/b+/+QppcAziO\nf4fbVJa08M8bBlYGZeAQhRRqedFll4NuiqSLIAohAkOzYRdD0xQbrC6CGoQtNGoX3UR2Y15YwQg0\njSAC8U8WsiL/lWTaxTg7QRxOnc454+n9fe72jsHz4x38eJ53e57JyUkGBgZ48+YNbrfb6PtjWRb7\n9+8HoKSkhIKCAp49e2ZsHkjN3CorK3E6nZSUlODxeMjKyjI6E6QeZwSDQRwOR3rZGTAuz9OnT/H7\n/QCUlZWxvLzMyspK+v1/ksd2y+Z79uzh/v37AIyNjVFUVMS6desyPKp/x+7du9PZ+vv72bt3b4ZH\n9GPm5+e5cOECV65cwev1AuZmAUgkEkSjUSD1mGZpacnoPOFwmDt37nDr1i0OHDjAiRMnjM5z9+5d\nrl27BsDs7CzJZJJAIGBsHgC/38/jx49ZXV3l/fv3xn/nIFVoHo8Ht9uNy+WitLSURCIBmJdn8+bN\nDA8PAzA9PY3H42Hbtm2/lMeWp4p1dXWRSCRwOBycO3eOsrKyTA/pp42OjtLR0cH09DROpxPLsujq\n6qKpqYnl5WWKi4s5f/48Lpcr00P9W319fUQiEbZu3Zq+1t7eTjAYNC4LpP5SdfbsWWZmZvj06RP1\n9fWUl5fT2NhoZJ5vRSIRNm3ahN/vNzbPwsICDQ0NzM3N8fnzZ+rr69m5c6exef7Q29vL7du3ATh+\n/Dg+n8/oTKOjo4TDYa5evQqkfqfQ0tLC6uoqFRUVnDlzJsMj/HGLi4s0NzeTTCZZWVnh5MmTFBYW\n/lIeW5a3iIiIyWy3bC4iImI6lbeIiIhhVN4iIiKGUXmLiIgYRuUtIiJiGJW3iPyyeDxOQ0NDpoch\nYhsqbxEREcPYbntUETvr6enh3r17fPnyhdLSUo4ePcqxY8eora3lxYsXAFy8eBHLshgYGODy5cvk\n5OSQm5tLKBTCsiyGh4dpa2vD5XKxfv16Ojo6gD83P3n16hXFxcVcunQJh8ORybgivy3NvEVsYmRk\nhAcPHhCLxejr6yMvL4+hoSEmJycJBALcvHmT6upqotEoHz9+JBgMEolE6Onpoba2lnA4DMDp06cJ\nhULcuHGDXbt28fDhQyC1A1YoFCIej/Py5UvGxsYyGVfkt6aZt4hNPHnyhImJCerq6gBYWlri7du3\neL1eysvLAaiqquL69euMj4+Tn5/Pxo0bAaiurqa3t5d3794xNzfH9u3bAThy5AiQeubt8/nIzc0F\nUod/zM/P/88JRexD5S1iE263m3379tHS0pK+NjU1RSAQSL9eW1vD4XB8t9z97fW/2lE5Kyvru8+I\nyH9Dy+YiNlFVVcXg4CCLi4sAxGIxZmdn+fDhA8+fPwdSRxfu2LGDLVu2kEwmef36NQCPHj2ioqKC\nDRs24PV6GRkZASAajRKLxTITSMTGNPMWsQmfz8ehQ4c4fPgw2dnZFBUVUVNTg2VZxONx2tvbWVtb\no7u7m5ycHFpbWzl16lT6/O7W1lYAOjs7aWtrw+l0kpeXR2dnJ/39/RlOJ2IvOlVMxMampqY4ePAg\ng4ODmR6KiPwELZuLiIgYRjNvERERw2jmLSIiYhiVt4iIiGFU3iIiIoZReYuIiBhG5S0iImIYlbeI\niIhhvgLJn1dEV1xB5AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5a48d91710>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "I2NGxSQb2dxD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "1bf23958-964c-487d-ec83-92eeee9b2bff"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "s=2\n",
        "e=3\n",
        "print ('score',model.evaluate([x_train[s:e], x_train[s:e]], y_train[s:e],batch_size=batch_size))\n",
        "\n",
        "show_sample('train',True,s) \n",
        "\n",
        "\n",
        "print ('\\n COMPARE TO VAL:\\n')\n",
        "print('score',model.evaluate([x_val[s:e], x_val[s:e]], y_val[s:e],batch_size=batch_size))\n",
        "show_sample('val',True,s) \n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n",
            "score 3.5762791839033525e-08\n",
            "\n",
            "gold x:  ['<s>', '\"And', 'God', 'said', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
            "gold y:  ['\"And', 'God', 'said', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
            "actual y: ['\"And', 'God', 'said', 'unto', 'to', '<oov>', '<oov>', '<oov>', 'were', 'dwell']\n",
            "\n",
            " COMPARE TO VAL:\n",
            "\n",
            "1/1 [==============================] - 0s 6ms/step\n",
            "score 0.0619158074259758\n",
            "\n",
            "gold x:  ['<s>', '\"And', 'after', 'these', 'things', 'he', 'was', '<oov>', 'in', 'another']\n",
            "gold y:  ['\"And', 'after', 'these', 'things', 'he', 'was', '<oov>', 'in', 'another', '<s>']\n",
            "actual y: ['\"And', 'after', 'these', 'things', 'he', 'was', '<oov>', 'in', 'another', 'cause']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TkFXxinr0fNo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "3e57106b-e8b1-4478-9651-004daf18dcdf"
      },
      "cell_type": "code",
      "source": [
        "#not logical. the train on s=2 is bad match. no <s> and yet score is 1.1e-6\n",
        "\n",
        "a= model.predict([x_train[s:e], x_train[s:e]])\n",
        "for i in range(10):\n",
        "  best=np.argmax(a[0,i])\n",
        "  print (i,best,dataset.index2word[best],a[0,i,best],a[0,i,0])\n",
        "\n",
        "from keras.losses import categorical_crossentropy\n",
        "\n",
        "K.eval(K.sum(categorical_crossentropy(K.constant(y_train[s:e]), K.constant(y_train[s:e]))))\n",
        "y_train[s:e]"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 3 \"And 1.0 4.7492294e-17\n",
            "1 49 God 1.0 1.3111372e-19\n",
            "2 16 said 1.0 6.0936e-16\n",
            "3 6 unto 0.82828313 1.3656206e-08\n",
            "4 5 to 0.5564936 1.0769865e-08\n",
            "5 1999 <oov> 0.9999993 1.2189777e-15\n",
            "6 1999 <oov> 0.9999994 3.7352958e-18\n",
            "7 1999 <oov> 0.98059076 5.361564e-16\n",
            "8 40 were 0.52425724 4.6717628e-15\n",
            "9 286 dwell 0.6240085 3.9660632e-15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "metadata": {
        "id": "beDWv50D8MYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "2321676d-5088-43b6-ae83-2b8533c3f53d"
      },
      "cell_type": "code",
      "source": [
        "print (x_train[s:e])\n",
        "y_train[s:e][:,:,:5]\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.  3. 49. 16.  0.  0.  0.  0.  0.  0.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    }
  ]
}